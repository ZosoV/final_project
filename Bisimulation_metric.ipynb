{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy Bisimulation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "ACTION_SPACE = ['up', 'down', 'left', 'right']\n",
    "ACTION2ACTION = { action[0]: action for action in ACTION_SPACE }\n",
    "ACTION2IDX = {'u': 0, 'd': 1, 'l': 2, 'r': 3}\n",
    "REWARD_FUNC = {'G': 10, '.': -1, 'T': 0} # goal, empty, terminal\n",
    "\n",
    "def read_grid_world(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        grid = [line.split() for line in file]\n",
    "    return np.array(grid)\n",
    "\n",
    "def get_neighbors(grid, row, col):\n",
    "    neighbors = {\n",
    "        'up': (row - 1, col),\n",
    "        'down': (row + 1, col),\n",
    "        'left': (row, col - 1),\n",
    "        'right': (row, col + 1)\n",
    "    }\n",
    "    valid_neighbors = {}\n",
    "    for action, (r, c) in neighbors.items():\n",
    "        if 0 <= r < grid.shape[0] and 0 <= c < grid.shape[1] and grid[r, c] != 'x':\n",
    "            valid_neighbors[action] = (r, c)\n",
    "        else:\n",
    "            valid_neighbors[action] = (row, col)  # stay in the same state if out of bounds or wall\n",
    "    return valid_neighbors\n",
    "\n",
    "def calculate_rewards_and_transitions(grid):\n",
    "    states = {}\n",
    "    rewards = {}\n",
    "    transitions = {}\n",
    "    for row in range(grid.shape[0]):\n",
    "        for col in range(grid.shape[1]):\n",
    "            state = (row, col)\n",
    "            if grid[row, col] == 'G':\n",
    "                # NOTE: Terminal state in the goal has a reward of 0\n",
    "                rewards[state] = {action: REWARD_FUNC['T'] for action in ACTION_SPACE}\n",
    "                transitions[state] = {action: {state: 1.0} for action in ACTION_SPACE}\n",
    "            elif grid[row, col] == '.':\n",
    "                rewards[state] = {}\n",
    "                transitions[state] = {}\n",
    "                neighbors = get_neighbors(grid, row, col)\n",
    "                for action, (r, c) in neighbors.items():\n",
    "                    if grid[r, c] == 'G':\n",
    "                        rewards[state][action] = REWARD_FUNC['G']\n",
    "                    else:\n",
    "                        rewards[state][action] = REWARD_FUNC['.']\n",
    "                    transitions[state][action] = {(r, c): 1.0}\n",
    "    return rewards, transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.' '.' 'G']\n",
      " ['.' '.' '.']\n",
      " ['x' 'x' 'x']\n",
      " ['.' '.' 'G']\n",
      " ['.' '.' '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (0, 1): {'up': -1, 'down': -1, 'left': -1, 'right': 10},\n",
       " (0, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 0): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (1, 1): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (1, 2): {'up': 10, 'down': -1, 'left': -1, 'right': -1},\n",
       " (3, 0): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (3, 1): {'up': -1, 'down': -1, 'left': -1, 'right': 10},\n",
       " (3, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (4, 0): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (4, 1): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (4, 2): {'up': 10, 'down': -1, 'left': -1, 'right': -1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'custom_envs/grid_world.txt'\n",
    "grid = read_grid_world(file_path)\n",
    "print(grid)\n",
    "\n",
    "rewards, transitions = calculate_rewards_and_transitions(grid)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.' '.' 'G']\n",
      " ['.' '.' '.']\n",
      " ['x' 'x' 'x']\n",
      " ['.' '.' 'G']\n",
      " ['.' '.' '.']]\n",
      "Optimal Value Function:\n",
      "[[ 8.  10.   0. ]\n",
      " [ 6.2  8.  10. ]\n",
      " [ 0.   0.   0. ]\n",
      " [ 8.  10.   0. ]\n",
      " [ 6.2  8.  10. ]]\n",
      "\n",
      "Optimal Policy:\n",
      "[['r' 'r' ' ']\n",
      " ['u' 'u' 'u']\n",
      " [' ' ' ' ' ']\n",
      " ['r' 'r' ' ']\n",
      " ['u' 'u' 'u']]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(grid, gamma=0.9, epsilon=1e-4):\n",
    "    # Define parameters\n",
    "    rows, cols = grid.shape\n",
    "    gamma = 0.9\n",
    "    # reward_goal = 10\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    # Initialize value function\n",
    "    V = np.zeros((rows, cols))\n",
    "\n",
    "    # Define the transition probabilities and rewards\n",
    "    def get_next_state(r, c, action):\n",
    "        if action == 'up':\n",
    "            return max(r - 1, 0), c\n",
    "        elif action == 'down':\n",
    "            return min(r + 1, rows - 1), c\n",
    "        elif action == 'left':\n",
    "            return r, max(c - 1, 0)\n",
    "        elif action == 'right':\n",
    "            return r, min(c + 1, cols - 1)\n",
    "        \n",
    "    def reward(r, c):\n",
    "        if grid[r][c] == 'G':\n",
    "            return REWARD_FUNC['G']\n",
    "        else:\n",
    "            return REWARD_FUNC['.']\n",
    "\n",
    "    def is_terminal(r, c):\n",
    "        return grid[r][c] == 'G' or grid[r][c] == 'x'\n",
    "\n",
    "    # Value iteration\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if not is_terminal(r, c):\n",
    "                    max_value = float('-inf')\n",
    "                    for action in ACTION_SPACE:\n",
    "                        next_r, next_c = get_next_state(r, c, action)\n",
    "                        if grid[next_r][next_c] != 'x':\n",
    "                            value = reward(next_r, next_c) + gamma * V[next_r][next_c]\n",
    "                            if value > max_value:\n",
    "                                max_value = value\n",
    "                    new_V[r][c] = max_value\n",
    "                    delta = max(delta, abs(new_V[r][c] - V[r][c]))\n",
    "        V = new_V\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "    # Extract policy\n",
    "    policy = np.full((rows, cols), ' ')\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if not is_terminal(r, c):\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                for action in ACTION_SPACE:\n",
    "                    next_r, next_c = get_next_state(r, c, action)\n",
    "                    if grid[next_r][next_c] != 'x':\n",
    "                        value = reward(next_r, next_c) + gamma * V[next_r][next_c]\n",
    "                        if value > max_value:\n",
    "                            max_value = value\n",
    "                            best_action = action\n",
    "                policy[r][c] = best_action[0]  # Just take the first letter of the action\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# Define the grid world environment\n",
    "file_path = 'custom_envs/grid_world.txt'\n",
    "grid = read_grid_world(file_path)\n",
    "print(grid)\n",
    "\n",
    "# Run value iteration\n",
    "V, policy = value_iteration(grid)\n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': {(0, 0): 1.0},\n",
       "  'down': {(1, 0): 1.0},\n",
       "  'left': {(0, 0): 1.0},\n",
       "  'right': {(0, 1): 1.0}},\n",
       " (0, 1): {'up': {(0, 1): 1.0},\n",
       "  'down': {(1, 1): 1.0},\n",
       "  'left': {(0, 0): 1.0},\n",
       "  'right': {(0, 2): 1.0}},\n",
       " (0, 2): {'up': {(0, 2): 1.0},\n",
       "  'down': {(0, 2): 1.0},\n",
       "  'left': {(0, 2): 1.0},\n",
       "  'right': {(0, 2): 1.0}},\n",
       " (1, 0): {'up': {(0, 0): 1.0},\n",
       "  'down': {(1, 0): 1.0},\n",
       "  'left': {(1, 0): 1.0},\n",
       "  'right': {(1, 1): 1.0}},\n",
       " (1, 1): {'up': {(0, 1): 1.0},\n",
       "  'down': {(1, 1): 1.0},\n",
       "  'left': {(1, 0): 1.0},\n",
       "  'right': {(1, 2): 1.0}},\n",
       " (1, 2): {'up': {(0, 2): 1.0},\n",
       "  'down': {(1, 2): 1.0},\n",
       "  'left': {(1, 1): 1.0},\n",
       "  'right': {(1, 2): 1.0}},\n",
       " (3, 0): {'up': {(3, 0): 1.0},\n",
       "  'down': {(4, 0): 1.0},\n",
       "  'left': {(3, 0): 1.0},\n",
       "  'right': {(3, 1): 1.0}},\n",
       " (3, 1): {'up': {(3, 1): 1.0},\n",
       "  'down': {(4, 1): 1.0},\n",
       "  'left': {(3, 0): 1.0},\n",
       "  'right': {(3, 2): 1.0}},\n",
       " (3, 2): {'up': {(3, 2): 1.0},\n",
       "  'down': {(3, 2): 1.0},\n",
       "  'left': {(3, 2): 1.0},\n",
       "  'right': {(3, 2): 1.0}},\n",
       " (4, 0): {'up': {(3, 0): 1.0},\n",
       "  'down': {(4, 0): 1.0},\n",
       "  'left': {(4, 0): 1.0},\n",
       "  'right': {(4, 1): 1.0}},\n",
       " (4, 1): {'up': {(3, 1): 1.0},\n",
       "  'down': {(4, 1): 1.0},\n",
       "  'left': {(4, 0): 1.0},\n",
       "  'right': {(4, 2): 1.0}},\n",
       " (4, 2): {'up': {(3, 2): 1.0},\n",
       "  'down': {(4, 2): 1.0},\n",
       "  'left': {(4, 1): 1.0},\n",
       "  'right': {(4, 2): 1.0}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_on_policy_bisimulation_metric(states, policy, transition_prob, rewards, gamma, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compute the on-policy bisimulation metric for a given MDP and policy.\n",
    "\n",
    "    Parameters:\n",
    "    - states: List of states each state is a tuple (x,y).\n",
    "    - policy: Function π(a | s). It is a matrix of size similar to the grid\n",
    "    - transition_prob: Function P(s' | s, a).\n",
    "    - rewards: Function R(s, a).\n",
    "    - gamma: Discount factor.\n",
    "    - epsilon: Convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "    - d: On-policy bisimulation metric.\n",
    "    \"\"\"\n",
    "    n_states = len(states)\n",
    "    d = np.zeros((n_states, n_states))\n",
    "\n",
    "    def distance_update(d):\n",
    "        new_d = np.zeros_like(d)\n",
    "        for i, s in enumerate(states):\n",
    "            for j, t in enumerate(states):\n",
    "                if i != j:\n",
    "                    action_s = ACTION2ACTION[policy[s]]\n",
    "                    action_t = ACTION2ACTION[policy[t]]\n",
    "                    reward_diff = abs(rewards[s][action_s] - rewards[t][action_t])\n",
    "                    transition_diff = gamma * np.sum([\n",
    "                        transition_prob(s_next, s, policy(s)) * transition_prob(t_next, t, policy(t)) * d[states.index(s_next)][states.index(t_next)]\n",
    "                        for s_next in states for t_next in states\n",
    "                    ])\n",
    "                    new_d[i][j] = reward_diff + transition_diff\n",
    "        return new_d\n",
    "\n",
    "    while True:\n",
    "        new_d = distance_update(d)\n",
    "        if np.max(np.abs(new_d - d)) < epsilon:\n",
    "            break\n",
    "        d = new_d\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
