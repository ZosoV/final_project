{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mico Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the encoder used in MICo\n",
    "# NOTE: We are going to call\n",
    "# state_dim : to the original dimension from the environment\n",
    "# encoding_dim : to the dimension of the latent space\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, state_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, encoding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for Deep Q-Learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, encoding_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(encoding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Learning agent with MICo integration\n",
    "class DQN_MICo_Agent:\n",
    "    def __init__(self, state_dim, action_dim, encoding_dim, gamma=0.99, lr=0.001, mico_alpha=0.1, mico_beta=0.1):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.mico_alpha = mico_alpha\n",
    "        self.mico_beta = mico_beta\n",
    "        \n",
    "        # Define the encoder, Q-Network, target encoder and target network\n",
    "        self.encoder = Encoder(state_dim, encoding_dim)\n",
    "        self.target_encoder = Encoder(state_dim, encoding_dim)\n",
    "        self.q_network = DQN(encoding_dim, action_dim)\n",
    "        self.target_network = DQN(encoding_dim, action_dim)\n",
    "\n",
    "        # Define the optimizer and loss function\n",
    "        self.optimizer = optim.Adam(list(self.q_network.parameters()) + list(self.encoder.parameters()), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # Load the weights of the Q-Network and encoder into the target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_encoder.load_state_dict(self.encoder.state_dict())\n",
    "    \n",
    "    def select_action(self, state_encoding, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            state_encoding = torch.FloatTensor(state_encoding).unsqueeze(0)\n",
    "            q_values = self.q_network(state_encoding)\n",
    "            return q_values.argmax().item()\n",
    "        \n",
    "    def representation_distance_U(self, encoding_x, encoding_y):\n",
    "        # NOTE: encoding_x and encoding_y can be encoding of the current state or next states        \n",
    "\n",
    "        # Calculate the square norms of the encodings\n",
    "        square_norm_encoding_x = torch.norm(encoding_x , dim=1)**2\n",
    "        square_norm_encoding_y = torch.norm(encoding_y , dim=1)**2\n",
    "\n",
    "        # Calculate the angle between the encodings\n",
    "        dot_product = torch.sum(encoding_x * encoding_y, dim=1)\n",
    "        cosine_similarity = dot_product / (torch.sqrt(square_norm_encoding_x) * torch.sqrt(square_norm_encoding_y))\n",
    "        angle = torch.acos(cosine_similarity)\n",
    "\n",
    "        return ((square_norm_encoding_x + square_norm_encoding_y)/2) + self.beta * angle\n",
    "  \n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        if len(replay_buffer.buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # TODO: Check if the randoms are a proper way to sample \n",
    "        # or I should use random from pytorch\n",
    "\n",
    "        # Gather a sample from the replay buffer\n",
    "        # NOTE: We need to sample two pairs of transitions with the same action\n",
    "        # TODO: ASK if they need to be the same action or just the same state\n",
    "        # TODO: ASK if I need to calculate the TD loss for both transitions\n",
    "        batch_x = random.sample(replay_buffer.buffer, batch_size)\n",
    "        state_x, action_x, reward_x, next_state_x, done_x = zip(*batch_x)\n",
    "        \n",
    "        state_x = torch.FloatTensor(state_x)\n",
    "        action_x = torch.LongTensor(action_x).unsqueeze(1)\n",
    "        reward_x = torch.FloatTensor(reward_x).unsqueeze(1)\n",
    "        next_state_x = torch.FloatTensor(next_state_x)\n",
    "        done_x = torch.FloatTensor(done_x).unsqueeze(1)\n",
    "        \n",
    "        batch_y = random.sample(replay_buffer.buffer, batch_size)\n",
    "        state_y, action_y, reward_y, next_state_y, done_y = zip(*batch_y)\n",
    "\n",
    "        state_y = torch.FloatTensor(state_y)\n",
    "        action_y = torch.LongTensor(action_y).unsqueeze(1)\n",
    "        reward_y = torch.FloatTensor(reward_y).unsqueeze(1)\n",
    "        next_state_y = torch.FloatTensor(next_state_y)\n",
    "        done_y = torch.FloatTensor(done_y).unsqueeze(1)\n",
    "        \n",
    "        # Calculating the MICO Loss\n",
    "        state_x_encoding = self.encoder(state_x)\n",
    "        state_y_encoding = self.encoder(state_y)\n",
    "\n",
    "        next_state_encoding_x =  self.target_encoder(state_x)\n",
    "        next_state_encoding_y =  self.target_encoder(state_y)\n",
    "\n",
    "        target_distance_U = self.representation_distance_U(next_state_encoding_x, next_state_encoding_y)\n",
    "        learning_target = torch.abs(reward_x - reward_y) + self.gamma * target_distance_U\n",
    "        current_distance_U = self.representation_distance_U(state_x_encoding, state_y_encoding)\n",
    "\n",
    "        mico_loss = (learning_target - current_distance_U).pow(2).mean()\n",
    "        \n",
    "\n",
    "        # Calculating the TD-Loss\n",
    "        # TODO: I'm gonna calculate just the loss of the encoding_x\n",
    "        # but probabilly I should calculate the loss of both encodings\n",
    "        q_values = self.q_network(state_x_encoding).gather(1, action_x)\n",
    "\n",
    "        # TODO: The next state of this should be using the target encoder or the encoder?\n",
    "        next_q_values = self.target_network(next_state_encoding_x).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = reward_x + (1 - done_x) * self.gamma * next_q_values\n",
    "        \n",
    "        q_loss = self.criterion(q_values, target_q_values)\n",
    "        \n",
    "        loss = (1 - self.alpha)* q_loss + self.alpha * mico_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "encoding_dim = 32\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "mico_alpha = 0.1\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "capacity = 1000\n",
    "\n",
    "# Initialize agent and replay buffer\n",
    "agent = DQN_MICo_Agent(state_dim, action_dim, encoding_dim, gamma, lr, mico_alpha)\n",
    "replay_buffer = ReplayBuffer(capacity)\n",
    "\n",
    "# Training loop (dummy example, replace with actual environment interaction)\n",
    "for episode in range(100):\n",
    "    state = np.random.rand(state_dim)\n",
    "    for t in range(100):\n",
    "        action = agent.select_action(state, epsilon)\n",
    "        next_state = np.random.rand(state_dim)\n",
    "        reward = random.random()\n",
    "        done = random.random() < 0.1\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        agent.train(replay_buffer, batch_size)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    agent.update_target_network()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
