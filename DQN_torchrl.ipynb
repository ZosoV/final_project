{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/safijari/dqn-tutorial/reports/Deep-Q-Networks-DQN-With-the-Cartpole-Environment--Vmlldzo4MDc2MQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7de47001d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "from torch.optim import Adam\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import CSVLogger, VideoRecorder\n",
    "from torchrl.modules import QValueActor\n",
    "from torchrl.data import CompositeSpec\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import (\n",
    "    CatFrames,\n",
    "    DoubleToFloat,\n",
    "    EndOfLifeTransform,\n",
    "    GrayScale,\n",
    "    GymEnv,\n",
    "    NoopResetEnv,\n",
    "    Resize,\n",
    "    RewardSum,\n",
    "    SignTransform,\n",
    "    StepCounter,\n",
    "    ToTensorImage,\n",
    "    TransformedEnv,\n",
    "    VecNorm,\n",
    ")\n",
    "\n",
    "def make_env(env_name=\"CartPole-v1\", frame_skip = 4, \n",
    "             device=\"cpu\", seed = 0, is_test=False):\n",
    "\n",
    "    env = GymEnv(\n",
    "        env_name,\n",
    "        frame_skip=frame_skip,\n",
    "        from_pixels=True,\n",
    "        pixels_only=False,\n",
    "        device=device,\n",
    "    )\n",
    "    env = TransformedEnv(env)\n",
    "    # env.append_transform(NoopResetEnv(noops=30, random=True)) # NOTE: Cartpole with no noops will fall into reset in the begining\n",
    "                                                                # I could use an small noop reset to avoid this, but I think is not necesary\n",
    "                                                                # in this case. Analyze this later\n",
    "    if not is_test:\n",
    "        # env.append_transform(EndOfLifeTransform()) # NOTE: Check my environment is not based on lives (so not important)\n",
    "        env.append_transform(SignTransform(in_keys=[\"reward\"])) #NOTE: cartpole has no negative rewards\n",
    "    env.append_transform(ToTensorImage()) \n",
    "    env.append_transform(GrayScale())\n",
    "    env.append_transform(Resize(84, 84))\n",
    "    env.append_transform(CatFrames(N=4, dim=-3))\n",
    "    env.append_transform(RewardSum())\n",
    "    env.append_transform(StepCounter()) # NOTE: Cartpole-v1 has a max of 500 steps\n",
    "    env.append_transform(DoubleToFloat())\n",
    "    env.append_transform(VecNorm(in_keys=[\"pixels\"]))\n",
    "    env.set_seed(seed)\n",
    "\n",
    "    # NOTE: a rollout will be take a trajectory of frames and group the frames by N=4 sequentially\n",
    "    # so that the output will be 7x4x84x84 because with a rollout of 10 steps we will have 7 groups of\n",
    "    # 4 frames each\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_name=\"CartPole-v1\", \n",
    "               frame_skip = 4, \n",
    "               device=\"cpu\", #\n",
    "               seed = 0,  \n",
    "               is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['done', 'terminated', 'truncated']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([4]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs.utils import RandomPolicy\n",
    "    \n",
    "policy = RandomPolicy(env.action_spec)\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=policy,\n",
    "    frames_per_batch=4,\n",
    "    total_frames=16000,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1,\n",
    "    init_random_frames=1000,\n",
    ")\n",
    "\n",
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(torch.nn.Module):\n",
    "    \"\"\"The convolutional network used to compute the agent's Q-values.\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 num_outputs,\n",
    "                 num_cells_cnn, \n",
    "                 kernel_sizes, \n",
    "                 strides, \n",
    "                 num_cells_mlp,\n",
    "                 activation_class,\n",
    "                 use_batch_norm=False):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        self.activation_class = activation_class()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "      \n",
    "        # Input shape example: (10, 4, 84, 84)\n",
    "        _, channels, width, height = input_shape\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Xavier (Glorot) uniform initialization\n",
    "        self.initializer = torch.nn.init.xavier_uniform_\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.batch_norm_layers = torch.nn.ModuleList()\n",
    "        in_channels = channels\n",
    "        for out_channels, kernel_size, stride in zip(num_cells_cnn, kernel_sizes, strides):\n",
    "            conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            if self.use_batch_norm:\n",
    "                batch_norm_layer = torch.nn.BatchNorm2d(out_channels)\n",
    "                self.batch_norm_layers.append(batch_norm_layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride  + 1\n",
    "        \n",
    "        # Compute the output shape of the conv layers\n",
    "        width_output = width\n",
    "        height_output = height\n",
    "        for kernel_size, stride in zip(kernel_sizes, strides):\n",
    "            width_output = conv2d_size_out(width_output, kernel_size, stride)\n",
    "            height_output = conv2d_size_out(height_output, kernel_size, stride)\n",
    "\n",
    "        cnn_output = width_output * height_output * num_cells_cnn[-1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        input_size = cnn_output\n",
    "\n",
    "        if len(num_cells_mlp) != 0:\n",
    "            self.fc_layers = torch.nn.ModuleList()\n",
    "            for units in num_cells_mlp:\n",
    "                fc_layer = torch.nn.Linear(input_size, units)\n",
    "                self.fc_layers.append(fc_layer)\n",
    "                input_size = units\n",
    "        else:\n",
    "            self.fc_layers = None\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = torch.nn.Linear(input_size, self.num_outputs)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.conv_layers:\n",
    "            self.initializer(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        if self.fc_layers is not None:\n",
    "            for layer in self.fc_layers:\n",
    "                self.initializer(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.zeros_(layer.bias)\n",
    "        self.initializer(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            torch.nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.float() / 255.0 # Already normalized by VecNorm\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            if self.use_batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "            x = self.activation_class(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        if self.fc_layers is not None:\n",
    "            for fc_layer in self.fc_layers:\n",
    "                x = self.activation_class(fc_layer(x))\n",
    "        q_values = self.output_layer(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICODQNNetwork(torch.nn.Module):\n",
    "    \"\"\"The convolutional network used to compute the agent's Q-values.\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 num_outputs,\n",
    "                 num_cells_cnn, \n",
    "                 kernel_sizes, \n",
    "                 strides, \n",
    "                 num_cells_mlp,\n",
    "                 activation_class,\n",
    "                 use_batch_norm=False):\n",
    "        super(MICODQNNetwork, self).__init__()\n",
    "\n",
    "        self.activation_class = activation_class()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "      \n",
    "        # Input shape example: (10, 4, 84, 84)\n",
    "        _, channels, width, height = input_shape\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Xavier (Glorot) uniform initialization\n",
    "        self.initializer = torch.nn.init.xavier_uniform_\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.batch_norm_layers = torch.nn.ModuleList()\n",
    "        in_channels = channels\n",
    "        for out_channels, kernel_size, stride in zip(num_cells_cnn, kernel_sizes, strides):\n",
    "            conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            if self.use_batch_norm:\n",
    "                batch_norm_layer = torch.nn.BatchNorm2d(out_channels)\n",
    "                self.batch_norm_layers.append(batch_norm_layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride  + 1\n",
    "        \n",
    "        # Compute the output shape of the conv layers\n",
    "        width_output = width\n",
    "        height_output = height\n",
    "        for kernel_size, stride in zip(kernel_sizes, strides):\n",
    "            width_output = conv2d_size_out(width_output, kernel_size, stride)\n",
    "            height_output = conv2d_size_out(height_output, kernel_size, stride)\n",
    "\n",
    "        cnn_output = width_output * height_output * num_cells_cnn[-1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        input_size = cnn_output\n",
    "\n",
    "        if len(num_cells_mlp) != 0:\n",
    "            self.fc_layers = torch.nn.ModuleList()\n",
    "            for units in num_cells_mlp:\n",
    "                fc_layer = torch.nn.Linear(input_size, units)\n",
    "                self.fc_layers.append(fc_layer)\n",
    "                input_size = units\n",
    "        else:\n",
    "            self.fc_layers = None\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = torch.nn.Linear(input_size, self.num_outputs)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.conv_layers:\n",
    "            self.initializer(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        if self.fc_layers is not None:\n",
    "            for layer in self.fc_layers:\n",
    "                self.initializer(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.zeros_(layer.bias)\n",
    "        self.initializer(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            torch.nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.float() / 255.0 # Already normalized by VecNorm\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            if self.use_batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "            x = self.activation_class(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        representation = x\n",
    "\n",
    "        if self.fc_layers is not None:\n",
    "            for fc_layer in self.fc_layers:\n",
    "                x = self.activation_class(fc_layer(x))\n",
    "        q_values = self.output_layer(x)\n",
    "        return q_values, representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the network with a random input\n",
    "tensor = torch.rand(10, 4, 84, 84)\n",
    "num_actions = 2\n",
    "num_cells_cnn = [32, 64, 64]\n",
    "kernel_sizes = [8, 4, 3]\n",
    "strides = [4, 2, 1]\n",
    "num_cells_mlp = [512]\n",
    "activation_class = torch.nn.ReLU\n",
    "\n",
    "network = MICODQNNetwork(\n",
    "    input_shape=tensor.shape,\n",
    "    num_outputs=num_actions,\n",
    "    num_cells_cnn=num_cells_cnn,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    strides=strides,\n",
    "    num_cells_mlp=num_cells_mlp,\n",
    "    activation_class=activation_class,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "q_values, representation = network(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MICODQNNetwork(\n",
       "  (activation_class): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (batch_norm_layers): ModuleList(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1-2): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10, 3136]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape, representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the network with a random input\n",
    "tensor = torch.rand(10, 4, 84, 84)\n",
    "num_actions = 2\n",
    "num_cells_cnn = [64, 64, 32]\n",
    "kernel_sizes = [5, 5, 5]\n",
    "strides = [2, 2, 2]\n",
    "num_cells_mlp = []\n",
    "activation_class = torch.nn.ReLU\n",
    "\n",
    "network = DQNNetwork(\n",
    "    input_shape=tensor.shape,\n",
    "    num_outputs=num_actions,\n",
    "    num_cells_cnn=num_cells_cnn,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    strides=strides,\n",
    "    num_cells_mlp=num_cells_mlp,\n",
    "    activation_class=activation_class,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "q_values = network(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNNetwork(\n",
       "  (activation_class): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(4, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (2): Conv2d(64, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  )\n",
       "  (batch_norm_layers): ModuleList(\n",
       "    (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=1568, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10, 1568]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape, representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (0): LazyConv2d(0, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): SquashDims()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrl.modules import ConvNet\n",
    "\n",
    "cnn = ConvNet(\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    num_cells=[32, 64, 64],\n",
    "    kernel_sizes=[8, 4, 3],\n",
    "    strides=[4, 2, 1])\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the network with a toy input\n",
    "x = torch.randn(1, 4, 84, 84)\n",
    "y = cnn(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28224"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "84 * 84 * 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 2 3 4 5 6 7 8 9 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([7, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([7, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([7, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([7]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([7, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([7, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([7, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([7]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: a rollout will be take a trajectory of frame of 10 steps and will concat them by N = 4\n",
    "# so the output will be 7x4x84x84\n",
    "env.rollout(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedEnv(\n",
       "    env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu),\n",
       "    transform=StepCounter(keys=[]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TransformedEnv(GymEnv(\"CartPole-v1\", from_pixels = True), StepCounter())\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# considering rollout['pixels'].shape equal torch.Size([10, 400, 600, 3]), plot the 10 images\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(axs):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# considering rollout['pixels'].shape equal torch.Size([10, 400, 600, 3]), plot the 10 images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(rollout['pixels'][i])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICOMLPNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 activation_class, \n",
    "                 encoder_out_features,\n",
    "                 mlp_out_features,\n",
    "                 encoder_num_cells = None,\n",
    "                 mlp_num_cells = None):\n",
    "        super(MICOMLPNetwork, self).__init__()\n",
    "\n",
    "        self.activation = activation_class()\n",
    "\n",
    "        if encoder_num_cells is None:\n",
    "            encoder_num_cells = []\n",
    "        layers_sizes = [in_features] + encoder_num_cells + [encoder_out_features]\n",
    "\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.encoder.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "\n",
    "        if mlp_num_cells is None:\n",
    "            mlp_num_cells = []\n",
    "\n",
    "        layers_sizes = [encoder_out_features] + mlp_num_cells + [mlp_out_features.item()]\n",
    "\n",
    "        self.q_net = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.q_net.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.encoder)):\n",
    "            x = self.activation(self.encoder[i](x))\n",
    "\n",
    "        representation = x\n",
    "\n",
    "        for i in range(len(self.q_net)-1):\n",
    "            x = self.activation(self.q_net[i](x))\n",
    "\n",
    "        return self.q_net[-1](x), representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MICOMLPNetwork(\n",
       "  (activation): ReLU()\n",
       "  (encoder): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       "  (q_net): ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_mlp = MICOMLPNetwork(\n",
    "    in_features=4,\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    encoder_out_features=3,\n",
    "    mlp_out_features=env.action_spec.shape[-1],\n",
    "    encoder_num_cells=[64],\n",
    "    mlp_num_cells=[64]\n",
    ")\n",
    "\n",
    "value_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4192e+00,  1.4267e+01,  3.2200e-01, -1.0510e+01],\n",
      "        [ 1.3432e-02, -6.3539e+00,  1.1603e-01, -5.9438e+00],\n",
      "        [ 1.0642e+00,  8.1578e+00,  3.2745e-01,  3.9270e+00],\n",
      "        [-2.6756e+00,  1.4854e+00,  3.5121e-01, -3.7025e+00],\n",
      "        [ 2.2572e+00,  1.9429e+01, -3.0938e-02,  1.3803e-01],\n",
      "        [-2.2600e+00, -1.0534e+01,  3.1909e-01, -5.3538e+00],\n",
      "        [-9.6557e-01,  1.2589e+01,  2.1707e-01, -3.2190e+00],\n",
      "        [ 6.2308e-01,  1.5224e+01, -2.8249e-01, -9.9592e+00],\n",
      "        [-2.1956e+00,  3.5823e+00, -9.9670e-02, -1.1440e+01],\n",
      "        [ 5.3441e-01,  1.0174e+01,  1.7487e-01,  1.1139e+00],\n",
      "        [ 8.5608e-01,  1.8250e+00,  1.9602e-01, -7.3161e+00],\n",
      "        [-7.3933e-01, -8.7312e+00,  1.6818e-01, -1.1422e+01],\n",
      "        [-2.8183e+00,  6.6944e+00,  3.6328e-01,  2.7987e+00],\n",
      "        [-4.3169e+00, -7.6947e-01, -3.1822e-01,  1.0076e+01],\n",
      "        [ 3.5351e+00,  8.2402e-01,  1.6378e-01, -3.6396e+00],\n",
      "        [ 8.6137e-01, -5.4435e-01, -3.6763e-01,  7.6138e+00],\n",
      "        [ 4.5975e-01, -5.8303e+00, -3.1591e-02,  1.9685e+00],\n",
      "        [ 1.8869e+00,  9.3262e+00,  3.4591e-01, -6.4764e-01],\n",
      "        [-8.6966e-01, -4.0350e+00,  4.0924e-01, -1.2574e+01],\n",
      "        [-2.8056e+00,  1.3371e+00,  4.0345e-01, -1.3737e+01],\n",
      "        [ 2.3841e+00,  1.9066e+01, -3.8204e-01,  8.6817e+00],\n",
      "        [-4.6309e+00,  8.9007e+00, -1.2224e-02,  2.4590e+00],\n",
      "        [-4.5669e+00,  3.4566e+00,  2.5700e-01,  1.3261e+01],\n",
      "        [-4.1051e+00, -2.4723e+00, -2.9603e-01, -8.0562e+00],\n",
      "        [ 1.0784e+00,  7.6366e+00,  3.7839e-01, -4.6467e+00],\n",
      "        [-4.3545e+00, -1.9985e+01,  1.7495e-01,  1.5444e+01],\n",
      "        [-1.3096e+00,  9.5245e-01, -3.0265e-01,  6.2501e+00],\n",
      "        [-4.3525e+00,  6.0272e+00,  1.7272e-01, -6.7275e+00],\n",
      "        [ 4.3541e+00,  3.7095e+00,  1.6518e-01,  1.1016e+01],\n",
      "        [ 3.3244e+00,  5.5618e-01, -2.2310e-01,  1.2104e+01],\n",
      "        [-4.3846e+00, -1.4940e+01, -1.7051e-01, -3.8077e+00],\n",
      "        [ 1.7962e-01, -2.2341e-01,  2.5060e-01,  2.9759e+00],\n",
      "        [ 9.1120e-01,  3.5240e+00,  3.0085e-02,  8.6249e+00],\n",
      "        [ 4.4256e+00, -5.1787e+00,  3.8652e-01, -1.2308e+01],\n",
      "        [-3.0597e+00,  1.0886e+01, -1.5409e-01, -1.1156e+01],\n",
      "        [-1.3853e+00,  5.4047e+00, -3.6615e-01,  5.9513e+00],\n",
      "        [-1.4041e+00, -1.1565e+01, -3.2650e-01, -1.8484e+01],\n",
      "        [ 2.6851e+00, -2.7982e+00,  2.6516e-02,  5.1400e+00],\n",
      "        [-8.2336e-01,  4.4614e-01,  4.7013e-02,  7.4661e+00],\n",
      "        [-4.6865e+00,  6.5802e-01,  9.1625e-02, -6.3296e+00],\n",
      "        [ 2.8847e+00, -5.6448e-02,  1.8426e-01,  1.2373e+01],\n",
      "        [ 4.2582e+00,  1.7744e+01,  1.1118e-01, -6.9971e+00],\n",
      "        [-2.4854e+00, -2.8777e+00, -2.7890e-01, -1.3407e+01],\n",
      "        [-2.1131e+00, -3.2281e+00,  4.1387e-01, -1.2884e+01],\n",
      "        [-2.8938e+00,  2.5394e+00, -5.7824e-02, -1.5784e+01],\n",
      "        [-3.7130e+00, -7.9686e+00, -2.0989e-02, -7.8035e+00],\n",
      "        [ 1.8579e+00, -1.8690e+00, -3.8714e-01, -7.8295e+00],\n",
      "        [-3.7347e+00,  2.6383e+00,  3.5385e-01,  2.3244e+00],\n",
      "        [-3.8846e+00,  4.2230e+00,  3.3968e-01,  9.2421e+00],\n",
      "        [-4.0157e+00,  1.1821e+01, -5.4167e-02,  9.8685e-02],\n",
      "        [ 3.5493e+00,  1.8667e+00,  4.1496e-01, -1.4830e+01],\n",
      "        [-7.9164e-01,  8.8608e+00, -3.7381e-01, -1.3989e+01],\n",
      "        [ 1.4999e+00, -5.9701e+00, -9.1319e-02,  9.9881e+00],\n",
      "        [-4.7652e+00, -3.9002e+00,  2.2833e-01,  5.4924e+00],\n",
      "        [-2.9678e+00, -6.6430e+00,  2.8524e-01,  2.4166e+00],\n",
      "        [-7.8289e-03, -9.7010e+00, -1.9652e-01, -1.7573e+01],\n",
      "        [-2.5749e+00,  1.3751e+00,  2.4212e-01, -1.9914e-01],\n",
      "        [-3.2284e+00, -8.3882e+00, -1.6497e-01,  1.6297e+00],\n",
      "        [-4.1611e+00, -3.4230e+00, -2.0868e-01, -5.9743e+00],\n",
      "        [-4.0836e+00, -9.6935e-01,  3.0511e-01, -1.1302e+01],\n",
      "        [-4.6793e-01,  2.2972e+00, -3.9737e-01, -1.3866e-01],\n",
      "        [-5.1393e-01, -1.1373e+01, -2.4012e-01, -2.3928e+00],\n",
      "        [ 1.3875e-01, -1.4809e+01,  1.8140e-01,  1.9485e+01],\n",
      "        [ 4.4380e-01,  6.6287e+00,  2.8921e-01, -5.1389e+00],\n",
      "        [ 4.2504e+00,  1.6159e+01, -8.4274e-02, -7.2269e+00],\n",
      "        [ 1.7250e+00,  1.5455e+00, -1.8038e-01, -4.7919e+00],\n",
      "        [-2.1912e+00,  8.8129e+00, -1.8300e-02,  1.5739e+01],\n",
      "        [ 3.9807e+00,  1.2717e+01,  5.9738e-02, -1.4968e+00],\n",
      "        [ 3.6126e+00,  1.4965e+01,  2.8987e-01, -6.9416e+00],\n",
      "        [-3.4174e+00, -5.3009e+00, -2.7135e-01, -1.2001e+01],\n",
      "        [ 1.0387e+00,  1.2157e+00,  2.6662e-01,  1.4493e+01],\n",
      "        [-4.4359e+00,  7.5930e+00, -9.7174e-02,  9.1492e+00],\n",
      "        [-3.8186e+00,  3.2798e+00, -3.7555e-01, -8.9185e-02],\n",
      "        [-1.3751e+00,  1.4014e+01, -6.3643e-03,  8.7908e+00],\n",
      "        [ 1.9533e+00, -6.4096e-01,  2.1741e-01, -3.1580e+00],\n",
      "        [ 6.4860e-01, -3.1111e+00,  2.7006e-01,  3.9124e+00],\n",
      "        [-2.3155e+00,  7.3970e+00,  1.5770e-01,  2.1795e+00],\n",
      "        [ 2.7359e+00, -8.8951e-01,  2.1091e-01,  7.1248e-01],\n",
      "        [-1.4690e+00, -9.2653e+00,  1.9377e-01, -6.0854e+00],\n",
      "        [ 2.1828e+00,  8.4470e+00, -8.2916e-02, -7.6474e-01],\n",
      "        [ 5.9320e-01, -3.4009e+00, -2.8956e-01,  2.5179e+00],\n",
      "        [ 3.4832e+00, -2.4066e+00,  2.4404e-01,  9.8730e+00],\n",
      "        [ 5.3593e-01, -6.5045e+00, -3.2322e-02, -5.1086e+00],\n",
      "        [ 2.8749e+00, -6.4949e+00, -2.2624e-01, -1.0715e+00],\n",
      "        [ 5.9302e-01, -1.6585e+01,  3.9525e-02,  9.5974e+00],\n",
      "        [ 1.4200e+00, -1.3362e+01,  4.0783e-01,  9.3981e-01],\n",
      "        [-2.1561e+00, -6.4835e+00, -2.6848e-01,  1.4108e+01],\n",
      "        [ 3.6042e+00,  4.4340e+00, -2.4872e-01,  1.9421e+01],\n",
      "        [ 2.5679e+00,  1.5267e+01, -4.0285e-01, -1.1691e+01],\n",
      "        [ 3.1971e+00, -1.4661e+01, -1.8945e-01,  2.3030e+00],\n",
      "        [ 3.9562e+00, -2.3743e+00, -5.9688e-02,  7.0771e-01],\n",
      "        [ 7.6884e-03, -1.9341e+01, -3.3204e-01, -9.4282e+00],\n",
      "        [ 1.5206e+00, -2.4896e-01, -2.0637e-01, -2.6437e+00],\n",
      "        [ 2.7368e+00,  3.2588e+00,  3.9525e-01,  2.4350e+00],\n",
      "        [-1.9920e+00,  5.7826e+00, -1.2753e-01,  2.0591e-01],\n",
      "        [-3.3233e+00,  1.1800e+01,  2.0032e-01,  3.3648e+00],\n",
      "        [ 3.4693e+00,  9.7497e+00,  2.8775e-01, -4.4075e+00],\n",
      "        [-1.2783e+00, -4.4188e+00,  2.7505e-01, -5.9211e+00],\n",
      "        [ 6.2304e-01,  5.8746e+00,  4.0335e-01,  1.8673e+00],\n",
      "        [-1.5880e+00,  2.0638e+01, -3.1791e-01,  1.3706e-01]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specifications\n",
    "num_observations = 4\n",
    "cart_position_min = -4.8\n",
    "cart_position_max = 4.8\n",
    "cart_velocity_min = -np.inf\n",
    "cart_velocity_max = np.inf\n",
    "pole_angle_min = -0.418\n",
    "pole_angle_max = 0.418\n",
    "pole_angular_velocity_min = -np.inf\n",
    "pole_angular_velocity_max = np.inf\n",
    "\n",
    "def create_batched_random_tensor(n):\n",
    "    # Creating the batched random tensor\n",
    "    cart_position = np.random.uniform(cart_position_min, cart_position_max, size=n)\n",
    "    cart_velocity = np.random.normal(loc=0.0, scale=10.0, size=n)  # Assuming normal distribution with large std deviation\n",
    "    pole_angle = np.random.uniform(pole_angle_min, pole_angle_max, size=n)\n",
    "    pole_angular_velocity = np.random.normal(loc=0.0, scale=10.0, size=n)  # Assuming normal distribution with large std deviation\n",
    "\n",
    "    # Combining into a single tensor of shape (n, 4)\n",
    "    batched_tensor = np.stack((cart_position, cart_velocity, pole_angle, pole_angular_velocity), axis=-1)\n",
    "    \n",
    "    return batched_tensor\n",
    "\n",
    "# Example usage with batch size n = 5\n",
    "n = 100\n",
    "batched_tensor = torch.tensor(create_batched_random_tensor(n), dtype=torch.float32)\n",
    "print(batched_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 2.087986469268799, Loss Representation: 1.243345022201538, Loss Q-Values: 0.8446414470672607\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the network\n",
    "# network = MICOMLPNetwork(in_features=4,\n",
    "#                          activation_class=torch.nn.ReLU, \n",
    "#                          encoder_out_features=8,\n",
    "#                          mlp_out_features=1,\n",
    "#                          encoder_num_cells=[16],\n",
    "#                          mlp_num_cells=[8])\n",
    "\n",
    "network = MICOMLPNetwork(\n",
    "    in_features=4,\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    encoder_out_features=2,\n",
    "    mlp_out_features=env.action_spec.shape[-1],\n",
    "    encoder_num_cells=[128, ],\n",
    "    mlp_num_cells=[64]\n",
    ")\n",
    "\n",
    "# Define dummy target tensors for losses\n",
    "target_representation = torch.randn(100, 2)  # Assuming the representation has 8 features\n",
    "target_q_values = torch.randn(100, 2)  # Assuming the Q-values have 1 feature\n",
    "\n",
    "# Define loss functions\n",
    "criterion_representation = nn.MSELoss()\n",
    "criterion_q_values = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Forward pass\n",
    "q_values, representation = network(batched_tensor)\n",
    "\n",
    "# Compute the losses\n",
    "loss_representation = criterion_representation(representation, target_representation)\n",
    "loss_q_values = criterion_q_values(q_values, target_q_values)\n",
    "\n",
    "# Sum the losses\n",
    "total_loss = loss_representation + loss_q_values\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Print losses\n",
    "print(f\"Total Loss: {total_loss.item()}, Loss Representation: {loss_representation.item()}, Loss Q-Values: {loss_q_values.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the gradients of the total_loss\n",
    "# for name, param in network.named_parameters():\n",
    "#     print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_example = torch.tensor(batched_tensor, dtype=torch.float32)\n",
    "# q_values, representation = value_mlp(toy_example)\n",
    "# print(q_values)\n",
    "# print(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=MICOMLPNetwork(\n",
       "      (activation): ReLU()\n",
       "      (encoder): ModuleList(\n",
       "        (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "      )\n",
       "      (q_net): ModuleList(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['action_value', 'representation'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net = Mod(value_mlp, \n",
    "                in_keys=[\"observation\"], \n",
    "                out_keys=[\"action_value\", \"representation\"])\n",
    "value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QValueActor(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MICOMLPNetwork(\n",
       "            (activation): ReLU()\n",
       "            (encoder): ModuleList(\n",
       "              (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "            )\n",
       "            (q_net): ModuleList(\n",
       "              (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "            )\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['action_value', 'representation'])\n",
       "      (1): QValueModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy = Seq(value_net, \n",
    "#              QValueModule(spec=env.action_spec))\n",
    "# policy\n",
    "\n",
    "policy = QValueActor(\n",
    "    module=value_net,\n",
    "    spec=CompositeSpec(action= env.specs[\"input_spec\", \"full_action_spec\", \"action\"]),\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): QValueActor(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=MICOMLPNetwork(\n",
       "                  (activation): ReLU()\n",
       "                  (encoder): ModuleList(\n",
       "                    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "                  )\n",
       "                  (q_net): ModuleList(\n",
       "                    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "                  )\n",
       "                ),\n",
       "                device=cpu,\n",
       "                in_keys=['observation'],\n",
       "                out_keys=['action_value', 'representation'])\n",
       "            (1): QValueModule()\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])\n",
       "      (1): EGreedyModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action_value', 'chosen_action_value', 'action'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the exploration step (e-greedy policy)\n",
    "exploration_module = EGreedyModule(\n",
    "    env.action_spec, \n",
    "    annealing_num_steps=100_000, \n",
    "    eps_init=0.1,\n",
    ")\n",
    "policy_explore = Seq(policy, \n",
    "                     exploration_module)\n",
    "policy_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to collect the data (experiences)\n",
    "init_rand_steps = 5000 # warm-up steps\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "replay_capacity = 100_000\n",
    "\n",
    "# NOTE: collector will gather rollouts continously\n",
    "# If the current trajectory ends, it will start a new one\n",
    "# NOTE: the rollout gotten from the collector is a dictionary\n",
    "# that defines the sate and next state as a tensor with a batch dimension in the begining\n",
    "# for example a rollout of 10 steps will have a tensor of observation of 10 in the batch dimension\n",
    "# and the next will also have 10 which are all the tensors of the next state\n",
    "# Practically, next is as you will shift the tensor of observation by one step\n",
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=frames_per_batch,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=init_rand_steps,\n",
    "# )\n",
    "# rb = ReplayBuffer(storage=LazyTensorStorage(replay_capacity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the recording and logging\n",
    "path = \"./training_loop\"\n",
    "logger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\n",
    "video_recorder = VideoRecorder(logger, tag=\"video\")\n",
    "record_env = TransformedEnv(\n",
    "    GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1063, 0.0000],\n",
      "        [0.1425, 0.0952, 0.0061],\n",
      "        [0.0912, 0.0871, 0.0460],\n",
      "        [0.0490, 0.0752, 0.0896],\n",
      "        [0.0580, 0.0906, 0.1324],\n",
      "        [0.0857, 0.0904, 0.1998],\n",
      "        [0.1152, 0.0904, 0.2713],\n",
      "        [0.1495, 0.0832, 0.3370],\n",
      "        [0.1832, 0.0750, 0.4068],\n",
      "        [0.2148, 0.0679, 0.4824]])\n"
     ]
    }
   ],
   "source": [
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=10,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=10000,\n",
    "# )\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=policy_explore,\n",
    "    frames_per_batch=10,\n",
    "    total_frames=100,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1\n",
    ")\n",
    "# NOTE: IMPORTANTISIMO en las primeras iteraciones no se usa la policy, entonces representation se configura\n",
    "# a zero, por lo que el primer batch de datos no tiene representation\n",
    "# Tengo que hacer el warm-up de otra manera (ojo con esto)\n",
    "\n",
    "for data in collector:\n",
    "    print(data['representation'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1063, 0.0000],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.0912, 0.0871, 0.0460],\n",
       "        [0.0490, 0.0752, 0.0896],\n",
       "        [0.0580, 0.0906, 0.1324],\n",
       "        [0.0857, 0.0904, 0.1998],\n",
       "        [0.1152, 0.0904, 0.2713],\n",
       "        [0.1495, 0.0832, 0.3370],\n",
       "        [0.1832, 0.0750, 0.4068],\n",
       "        [0.2148, 0.0679, 0.4824]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyTensorStorage(\n",
       "        data=<empty>, \n",
       "        shape=None, \n",
       "        len=0, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=None, slice_len=2, end_key=('next', 'done'), traj_key=('collector', 'traj_ids'), truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=0, full_storage=False), \n",
       "    batch_size=10, \n",
       "    collate_fn=<function _collate_id at 0x7f14b8373b00>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "\n",
    "size = 100\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(size),\n",
    "    sampler=SliceSampler(traj_key=(\"collector\",\"traj_ids\"), slice_len=2),\n",
    "    batch_size=10,\n",
    ")\n",
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['collector','traj_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rb.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [8],\n",
       "        [9],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['step_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0313,  0.0413,  0.0107,  0.0229],\n",
       "        [ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.1472,  1.6044, -0.1500, -2.3857],\n",
       "        [ 0.1793,  1.8005, -0.1977, -2.7204],\n",
       "        [ 0.0455,  0.6262, -0.0053, -0.8464],\n",
       "        [ 0.0580,  0.8214, -0.0222, -1.1407],\n",
       "        [ 0.0745,  1.0168, -0.0451, -1.4403],\n",
       "        [ 0.0948,  1.2125, -0.0739, -1.7467],\n",
       "        [ 0.0455,  0.6262, -0.0053, -0.8464],\n",
       "        [ 0.0580,  0.8214, -0.0222, -1.1407]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.0369,  0.4312,  0.0058, -0.5555],\n",
       "        [ 0.1793,  1.8005, -0.1977, -2.7204],\n",
       "        [ 0.2153,  1.9964, -0.2521, -3.0663],\n",
       "        [ 0.0580,  0.8214, -0.0222, -1.1407],\n",
       "        [ 0.0745,  1.0168, -0.0451, -1.4403],\n",
       "        [ 0.0948,  1.2125, -0.0739, -1.7467],\n",
       "        [ 0.1190,  1.4084, -0.1088, -2.0614],\n",
       "        [ 0.0580,  0.8214, -0.0222, -1.1407],\n",
       "        [ 0.0745,  1.0168, -0.0451, -1.4403]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"next\", \"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1063, 0.0000],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.1832, 0.0750, 0.4068],\n",
       "        [0.2148, 0.0679, 0.4824],\n",
       "        [0.0490, 0.0752, 0.0896],\n",
       "        [0.0580, 0.0906, 0.1324],\n",
       "        [0.0857, 0.0904, 0.1998],\n",
       "        [0.1152, 0.0904, 0.2713],\n",
       "        [0.0490, 0.0752, 0.0896],\n",
       "        [0.0580, 0.0906, 0.1324]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "sample['representation']\n",
    "\n",
    "first_states = sample[0::2] # even rows\n",
    "second_states = sample[1::2] # odd rows (or next states)\n",
    "\n",
    "print(first_states)\n",
    "print(second_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1063, 0.0000],\n",
       "        [0.1832, 0.0750, 0.4068],\n",
       "        [0.0490, 0.0752, 0.0896],\n",
       "        [0.0857, 0.0904, 0.1998],\n",
       "        [0.0490, 0.0752, 0.0896]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1425, 0.0952, 0.0061],\n",
       "        [0.2148, 0.0679, 0.4824],\n",
       "        [0.0580, 0.0906, 0.1324],\n",
       "        [0.1152, 0.0904, 0.2713],\n",
       "        [0.0580, 0.0906, 0.1324]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n",
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "repeated_a = torch.Tensor.repeat(a, (2, 3))\n",
    "\n",
    "print(repeated_a)\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "tiled_a = torch.tile(a, (2, 3))\n",
    "print(tiled_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1425, 0.0952, 0.0061],\n",
       "        [0.2148, 0.0679, 0.4824],\n",
       "        [0.0580, 0.0906, 0.1324],\n",
       "        [0.1152, 0.0904, 0.2713],\n",
       "        [0.0580, 0.0906, 0.1324]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061]],\n",
       "\n",
       "        [[0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824]],\n",
       "\n",
       "        [[0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324]],\n",
       "\n",
       "        [[0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713]],\n",
       "\n",
       "        [[0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation'].shape # batch, rep_dim\n",
    "\n",
    "repeated_rep = torch.tile(second_states['representation'], (1,1,5)).view(5,5,3)\n",
    "repeated_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squarify(x):\n",
    "    # Squarify will take the input and adds a new dimension between the batch and the representation\n",
    "    # so that the representation is repeated along the new dimension\n",
    "    # To visualize thing of x as a matrix of batch_size x representation_dim\n",
    "    # and squarify will place that matrix in a lateral way and repeat it along the new dimension j\n",
    "\n",
    "    # NOTE: after squarify if you pick a i-th row all the elements (j-th index) in that row will be the same\n",
    "    batch_size = x.shape[0]\n",
    "    if len(x.shape) > 1:\n",
    "        representation_dim = x.shape[-1]\n",
    "        return x.tile((batch_size,)).view(batch_size, batch_size, representation_dim)\n",
    "    return x.tile((batch_size,)).view(batch_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarify(second_states['next','reward']).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061],\n",
       "         [0.1425, 0.0952, 0.0061]],\n",
       "\n",
       "        [[0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824],\n",
       "         [0.2148, 0.0679, 0.4824]],\n",
       "\n",
       "        [[0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324]],\n",
       "\n",
       "        [[0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713],\n",
       "         [0.1152, 0.0904, 0.2713]],\n",
       "\n",
       "        [[0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324],\n",
       "         [0.0580, 0.0906, 0.1324]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarify(second_states['representation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0400, 0.2769, 0.1315, 0.1758, 0.1315, 0.2262, 0.2479, 0.1562, 0.1631,\n",
       "        0.1562, 0.1056, 0.2010, 0.0330, 0.0904, 0.0330, 0.1417, 0.1961, 0.0589,\n",
       "        0.0849, 0.0589, 0.1056, 0.2010, 0.0330, 0.0904, 0.0330])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representation_distances(first_representations, second_representations,\n",
    "                             distance_fn, beta=0.1,\n",
    "                             return_distance_components=False):\n",
    "  \"\"\"Compute distances between representations.\n",
    "     In the paper, it corresponds to the calculation of the U term\n",
    "     for each pair of representations in the batch (all-vs-all).\n",
    "\n",
    "  This will compute the distances between two representations.\n",
    "\n",
    "  Args:\n",
    "    first_representations: first set of representations to use.\n",
    "    second_representations: second set of representations to use.\n",
    "    distance_fn: function to use for computing representation distances.\n",
    "    beta: float, weight given to cosine distance between representations.\n",
    "    return_distance_components: bool, whether to return the components used for\n",
    "      computing the distance.\n",
    "\n",
    "  Returns:\n",
    "    The distances between representations, combining the average of the norm of\n",
    "    the representations and the distance given by distance_fn.\n",
    "  \"\"\"\n",
    "  batch_size = first_representations.shape[0]\n",
    "  representation_dim = first_representations.shape[-1]\n",
    "\n",
    "  # Squarify the representations and reshape them to make a pair-waise comparison with vmap\n",
    "  first_squared_reps = squarify(first_representations)\n",
    "  first_squared_reps = torch.reshape(first_squared_reps,\n",
    "                                   [batch_size**2, representation_dim])\n",
    "  \n",
    "  # Squarify the representations and reshape them to make a pair-waise comparison with vmap\n",
    "  # However, we now need to permute (transpose) the dimension 0, 1 to alternate the values\n",
    "  # so that we have the pair-wise comparisons of all-vs-all\n",
    "  second_squared_reps = squarify(second_representations)\n",
    "  second_squared_reps = torch.permute(second_squared_reps, dims=(1, 0, 2))\n",
    "  second_squared_reps = torch.reshape(second_squared_reps,\n",
    "                                    [batch_size**2, representation_dim])\n",
    "  \n",
    "  # vmap will calculate the pairwise distance_fn along the dimension specified\n",
    "  # in in_axes. In this case, will take the dim 0 of the first_squared_reps and\n",
    "  # the dim 0 of the second_squared_reps and apply the distance\n",
    "  # It vertorize the process of calculating the distance between all the pairs\n",
    "\n",
    "  # NOTE: base distance corresponds to the second term in the U calculation in the paper\n",
    "  # It calculates the angle between the representations in the paper\n",
    "  # Check what function is using\n",
    "  base_distances = torch.vmap(distance_fn, in_dims=(0, 0))(first_squared_reps,\n",
    "                                                         second_squared_reps)\n",
    "  base_distances = base_distances\n",
    "  print(base_distances.shape)\n",
    "  # Sum along the second dimension and normalize the distance\n",
    "  # NOTE: this is practically the first term of U in the paper\n",
    "  norm_average = 0.5 * (torch.sum(torch.square(first_squared_reps), -1) +\n",
    "                        torch.sum(torch.square(second_squared_reps), -1))\n",
    "  \n",
    "  print(norm_average.shape)\n",
    "  if return_distance_components:\n",
    "    return norm_average + beta * base_distances, norm_average, base_distances\n",
    "  return norm_average + beta * base_distances\n",
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def _sqrt(x):\n",
    "  # zeros like instead of zeros\n",
    "  # It is because vmap works with a weird way of broadcasting\n",
    "  # and a weird structure based on tensors\n",
    "  tol = torch.zeros_like(x)\n",
    "  return torch.sqrt(torch.maximum(x, tol))\n",
    "\n",
    "\n",
    "def cosine_distance(x, y):\n",
    "  # NOTE: the cosine similarity is not calculate directly for \n",
    "  # instabilities observed when using `jnp.arccos`, but I'm using torch\n",
    "  # so I don't know if I will need to do this\n",
    "  numerator = torch.sum(x * y)\n",
    "  denominator = torch.sqrt(torch.sum(x**2)) * torch.sqrt(torch.sum(y**2))\n",
    "  cos_similarity = numerator / (denominator + EPSILON)\n",
    "\n",
    "  # cos_similarity = cos(theta)\n",
    "\n",
    "  # NOTE: From, the Pythagorean trigometric identity\n",
    "  # sin^2(theta) + cos^2(theta) = 1\n",
    "  # you can get sin(theta) = sqrt(1 - cos^2(theta))\n",
    "  # and the arctan2(sin(theta), cos(theta)) = theta\n",
    "  return torch.arctan2(_sqrt(1. - cos_similarity**2), cos_similarity)\n",
    "\n",
    "distances = representation_distances(first_states['representation'], \n",
    "                                     second_states['representation'], \n",
    "                                     cosine_distance)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: check in the main code if the output of this requires grad and the \n",
    "# other output must require grad\n",
    "# @torch.no_grad()\n",
    "def target_distances(representations, rewards, distance_fn, cumulative_gamma):\n",
    "  \"\"\"Target distance using the metric operator. This is the T in the paper :D\"\"\"\n",
    "  next_state_similarities = representation_distances(\n",
    "      representations, representations, distance_fn)\n",
    "  squared_rews = squarify(rewards).squeeze(-1)\n",
    "  squared_rews_transp = squared_rews.T\n",
    "  squared_rews = squared_rews.reshape((squared_rews.shape[0]**2))\n",
    "  squared_rews_transp = squared_rews_transp.reshape(\n",
    "      (squared_rews_transp.shape[0]**2))\n",
    "  reward_diffs = torch.abs(squared_rews - squared_rews_transp)\n",
    "  return reward_diffs + cumulative_gamma * next_state_similarities\n",
    "\n",
    "t_distances = target_distances(first_states['representation'], first_states['next','reward'], cosine_distance, cumulative_gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_distances.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1717, 0.1063, 0.0000],\n",
       "         [0.1832, 0.0750, 0.4068],\n",
       "         [0.0490, 0.0752, 0.0896],\n",
       "         [0.0857, 0.0904, 0.1998],\n",
       "         [0.0490, 0.0752, 0.0896]]),\n",
       " tensor([[0.0836, 0.2269],\n",
       "         [0.0057, 0.2379],\n",
       "         [0.0791, 0.2383],\n",
       "         [0.0543, 0.2416],\n",
       "         [0.0791, 0.2383]]),\n",
       " tensor([[0.2269],\n",
       "         [0.2379],\n",
       "         [0.2383],\n",
       "         [0.2416],\n",
       "         [0.2383]]),\n",
       " tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Checar la distancia euclidean entre la representacion target y la representacion con\n",
    "# la politica actual\n",
    "\n",
    "# NOTE: en el repositorio de MICO, la distancia target es calculada con\n",
    "# la target network (que es una copia de la politica actual) osea mis representaciones guardadas\n",
    "# la distancia online por otra parte es calculada con una representacion con red actual\n",
    "# y una representacion target\n",
    "\n",
    "\n",
    "collector.policy(first_states['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make the components\n",
    "# Policy\n",
    "\n",
    "from dqn_mico_er.utils_cartpole import make_dqn_model, make_env\n",
    "from dqn_mico_er.custom_modules import MICODQNLoss\n",
    "\n",
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.data.replay_buffers.samplers import RandomSampler, PrioritizedSampler, PrioritizedSliceSampler\n",
    "from torchrl.objectives import DQNLoss, HardUpdate\n",
    "\n",
    "# load condig_cartpole.yaml\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with open(\"dqn_mico_er/config_cartpole.yaml\") as f:\n",
    "    cfg = OmegaConf.create(yaml.safe_load(f))\n",
    "\n",
    "model = make_dqn_model(\"CartPole-v1\", cfg.policy)\n",
    "\n",
    "greedy_module = EGreedyModule(\n",
    "    annealing_num_steps=cfg.collector.annealing_frames,\n",
    "    eps_init=cfg.collector.eps_start,\n",
    "    eps_end=cfg.collector.eps_end,\n",
    "    spec=model.spec,\n",
    ")\n",
    "model_explore = TensorDictSequential(\n",
    "    model,\n",
    "    greedy_module,\n",
    ") #.to(device)\n",
    "\n",
    "# Create the collector\n",
    "# NOTE: init_random_frames: Number of frames \n",
    "# for which the policy is ignored before it is called.\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=make_env(cfg.env.env_name, \"cpu\", cfg.env.seed),\n",
    "    policy=model_explore,\n",
    "    frames_per_batch=cfg.collector.frames_per_batch,\n",
    "    total_frames=cfg.collector.total_frames,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1,\n",
    "    init_random_frames=cfg.collector.init_random_frames,\n",
    ")\n",
    "\n",
    "# Create the replay buffer\n",
    "if cfg.buffer.prioritized_replay:\n",
    "    print(\"Using Prioritized Replay Buffer\")\n",
    "    sampler = PrioritizedSliceSampler(\n",
    "        max_capacity=cfg.buffer.buffer_size, \n",
    "        alpha=cfg.buffer.alpha, \n",
    "        beta=cfg.buffer.beta, \n",
    "        traj_key=(\"collector\",\"traj_ids\"), \n",
    "        slice_len=2)\n",
    "else:\n",
    "    sampler = SliceSampler(\n",
    "        traj_key=(\"collector\",\"traj_ids\"), \n",
    "        slice_len=2)\n",
    "    \n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    pin_memory=False,\n",
    "    prefetch=10,\n",
    "    storage=LazyTensorStorage(\n",
    "        max_size=cfg.buffer.buffer_size,\n",
    "        device=\"cpu\",\n",
    "    ),\n",
    "    batch_size=cfg.buffer.batch_size,\n",
    "    sampler = sampler\n",
    ")\n",
    "\n",
    "# Create the loss module\n",
    "loss_module = MICODQNLoss(\n",
    "    value_network=model,\n",
    "    loss_function=\"l2\", \n",
    "    delay_value=True, # delay_value=True means we will use a target network\n",
    "    mico_gamma=cfg.loss.mico_gamma,\n",
    "    mico_beta=cfg.loss.mico_beta,\n",
    "    mico_weight=cfg.loss.mico_weight,\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(gamma=cfg.loss.gamma) # only to change the gamma value\n",
    "loss_module = loss_module #.to(device)\n",
    "target_net_updater = HardUpdate(\n",
    "    loss_module, value_network_update_interval=cfg.loss.hard_update_freq\n",
    ")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(loss_module.parameters(), lr=cfg.optim.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of the model parameters per layer\n",
      "tensor(0.4998)\n",
      "tensor(0.4991)\n",
      "tensor(0.0913)\n",
      "tensor(0.0879)\n",
      "tensor(0.1083)\n",
      "tensor(-0.0046)\n",
      "tensor(0.5714)\n",
      "tensor(0.5667)\n",
      "tensor(0.1090)\n",
      "tensor(0.0080)\n",
      "Maximum of the model parameters per layer\n",
      "tensor(0.4998, grad_fn=<MaxBackward1>)\n",
      "tensor(0.4991, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0913, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0879, grad_fn=<MaxBackward1>)\n",
      "tensor(0.1083, grad_fn=<MaxBackward1>)\n",
      "tensor(-0.0046, grad_fn=<MaxBackward1>)\n",
      "tensor(0.5714, grad_fn=<MaxBackward1>)\n",
      "tensor(0.5667, grad_fn=<MaxBackward1>)\n",
      "tensor(0.1090, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0080, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.4998, grad_fn=<MaxBackward1>),\n",
       " tensor(0.4991, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0913, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0879, grad_fn=<MaxBackward1>),\n",
       " tensor(0.1083, grad_fn=<MaxBackward1>),\n",
       " tensor(-0.0046, grad_fn=<MaxBackward1>),\n",
       " tensor(0.5714, grad_fn=<MaxBackward1>),\n",
       " tensor(0.5667, grad_fn=<MaxBackward1>),\n",
       " tensor(0.1090, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0080, grad_fn=<MaxBackward1>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the maximum of the model parameters\n",
    "def print_maximum_weights(model):\n",
    "    print(\"Maximum of the model parameters per layer\")#\n",
    "    weights = []\n",
    "    for p in model.parameters():\n",
    "        print(torch.max(p))\n",
    "        weights.append(torch.max(p))\n",
    "    return weights\n",
    "\n",
    "def print_maximum_grads(model):\n",
    "    print(\"Maximum of the model gradients per layer\")#\n",
    "    max_grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            print(torch.max(p.grad))\n",
    "            max_grads.append(torch.max(p.grad))\n",
    "    return max_grads\n",
    "\n",
    "def print_target_value_weights(loss_module):\n",
    "    with loss_module.target_value_network_params.to_module(loss_module.value_network):\n",
    "        return print_maximum_weights(loss_module.value_network)    \n",
    "\n",
    "def print_value_weights(loss_module):\n",
    "    with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "        return print_maximum_weights(loss_module.value_network)\n",
    "        # print_maximum_grads(loss_module.value_network)    \n",
    "\n",
    "def print_value_grads(loss_module):\n",
    "    with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "        # print_maximum_weights(loss_module.value_network)\n",
    "        return print_maximum_grads(loss_module.value_network)   \n",
    "\n",
    "# Print the maximum of the model parameters\n",
    "print_target_value_weights(loss_module)\n",
    "\n",
    "print_value_weights(loss_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchviz/dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"340pt\" height=\"468pt\"\n",
       " viewBox=\"0.00 0.00 340.00 468.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 464)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-464 336,-464 336,4 -4,4\"/>\n",
       "<!-- 139726566625584 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139726566625584</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"193.5,-31 139.5,-31 139.5,0 193.5,0 193.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 139726561649728 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139726561649728</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223,-86 110,-86 110,-67 223,-67 223,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MseLossBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649728&#45;&gt;139726566625584 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>139726561649728&#45;&gt;139726566625584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-66.79C166.5,-60.07 166.5,-50.4 166.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-41.19 166.5,-31.19 163,-41.19 170,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139726561649920 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139726561649920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-141 116,-141 116,-122 217,-122 217,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649920&#45;&gt;139726561649728 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139726561649920&#45;&gt;139726561649728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-121.75C166.5,-114.8 166.5,-104.85 166.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-96.09 166.5,-86.09 163,-96.09 170,-96.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649680 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139726561649680</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649680&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139726561649680&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.14,-176.98C87.8,-168.46 116.75,-155.23 138.24,-145.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.88,-148.51 147.52,-141.17 136.97,-142.14 139.88,-148.51\"/>\n",
       "</g>\n",
       "<!-- 139726566625200 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139726566625200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"83,-262 18,-262 18,-232 83,-232 83,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">fc2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 139726566625200&#45;&gt;139726561649680 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139726566625200&#45;&gt;139726561649680</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n",
       "</g>\n",
       "<!-- 139726561649824 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139726561649824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-196 119,-196 119,-177 214,-177 214,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649824&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139726561649824&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-176.75C166.5,-169.8 166.5,-159.85 166.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-151.09 166.5,-141.09 163,-151.09 170,-151.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649632 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139726561649632</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-256.5 112,-256.5 112,-237.5 213,-237.5 213,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649632&#45;&gt;139726561649824 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139726561649632&#45;&gt;139726561649824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.09,-237.37C163.65,-229.25 164.5,-216.81 165.21,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.72,-206.38 165.91,-196.17 161.73,-205.91 168.72,-206.38\"/>\n",
       "</g>\n",
       "<!-- 139726561649296 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139726561649296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"118,-322.5 17,-322.5 17,-303.5 118,-303.5 118,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649296&#45;&gt;139726561649632 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139726561649296&#45;&gt;139726561649632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.31,-303.37C95.89,-292.87 122.43,-275 141.12,-262.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.25,-265.19 149.59,-256.7 139.34,-259.38 143.25,-265.19\"/>\n",
       "</g>\n",
       "<!-- 139726566625008 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139726566625008</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"100,-394 35,-394 35,-364 100,-364 100,-394\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 139726566625008&#45;&gt;139726561649296 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139726566625008&#45;&gt;139726561649296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.5,-363.8C67.5,-354.7 67.5,-342.79 67.5,-332.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71,-332.84 67.5,-322.84 64,-332.84 71,-332.84\"/>\n",
       "</g>\n",
       "<!-- 139726561649344 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139726561649344</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-322.5 136,-322.5 136,-303.5 213,-303.5 213,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649344&#45;&gt;139726561649632 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139726561649344&#45;&gt;139726561649632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.88,-303.37C171.14,-294.07 168.31,-278.98 166.04,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.45,-266.09 164.17,-256.91 162.57,-267.38 169.45,-266.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649248 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139726561649248</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"225,-388.5 124,-388.5 124,-369.5 225,-369.5 225,-388.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649248&#45;&gt;139726561649344 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139726561649248&#45;&gt;139726561649344</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.5,-369.37C174.5,-360.16 174.5,-345.29 174.5,-333.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178,-332.91 174.5,-322.91 171,-332.91 178,-332.91\"/>\n",
       "</g>\n",
       "<!-- 139726566624912 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139726566624912</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"213,-460 136,-460 136,-430 213,-430 213,-460\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (5, 10)</text>\n",
       "</g>\n",
       "<!-- 139726566624912&#45;&gt;139726561649248 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139726566624912&#45;&gt;139726561649248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.5,-429.8C174.5,-420.7 174.5,-408.79 174.5,-398.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178,-398.84 174.5,-388.84 171,-398.84 178,-398.84\"/>\n",
       "</g>\n",
       "<!-- 139726561649968 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>139726561649968</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"313,-196 236,-196 236,-177 313,-177 313,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649968&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139726561649968&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.15,-176.98C239.93,-168.54 213.3,-155.47 193.35,-145.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.69,-142.43 184.17,-141.17 191.6,-148.72 194.69,-142.43\"/>\n",
       "</g>\n",
       "<!-- 139726561649200 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>139726561649200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"332,-256.5 231,-256.5 231,-237.5 332,-237.5 332,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649200&#45;&gt;139726561649968 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139726561649200&#45;&gt;139726561649968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M280.47,-237.37C279.5,-229.25 278.01,-216.81 276.76,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.2,-205.68 275.54,-196.17 273.25,-206.51 280.2,-205.68\"/>\n",
       "</g>\n",
       "<!-- 139726566625104 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>139726566625104</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"320,-328 243,-328 243,-298 320,-298 320,-328\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">fc2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (2, 5)</text>\n",
       "</g>\n",
       "<!-- 139726566625104&#45;&gt;139726561649200 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>139726566625104&#45;&gt;139726561649200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.5,-297.8C281.5,-288.7 281.5,-276.79 281.5,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"285,-266.84 281.5,-256.84 278,-266.84 285,-266.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f14a0127d50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define a simple neural network with two outputs\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)  # Two outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the network and some input data\n",
    "net = SimpleNet()\n",
    "x = torch.randn(1, 10)\n",
    "target = torch.randn(1, 2)  # Dummy target for the loss function\n",
    "\n",
    "# Perform the forward pass\n",
    "outputs = net(x)\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(outputs, target)\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Visualize the computational graph using torchviz\n",
    "make_dot(loss, params=dict(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "tensor(0.0024)\n",
      "tensor(0.0056)\n",
      "tensor(0.0090)\n",
      "tensor(0.0161)\n",
      "tensor(0.0317)\n",
      "tensor(0.0813)\n",
      "tensor(0.0212)\n",
      "tensor(0.1943)\n",
      "tensor(0.)\n",
      "tensor(-1.0666)\n",
      "Maximum of the model gradients per layer\n",
      "tensor(0.0024)\n",
      "tensor(0.0056)\n",
      "tensor(0.0090)\n",
      "tensor(0.0161)\n",
      "tensor(0.0317)\n",
      "tensor(0.0813)\n",
      "tensor(0.0212)\n",
      "tensor(0.1943)\n",
      "tensor(0.)\n",
      "tensor(-1.0666)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main loop\n",
    "collected_frames = 0\n",
    "total_episodes = 0\n",
    "start_time = time.time()\n",
    "num_updates = cfg.loss.num_updates\n",
    "batch_size = cfg.buffer.batch_size\n",
    "test_interval = cfg.logger.test_interval\n",
    "num_test_episodes = cfg.logger.num_test_episodes\n",
    "frames_per_batch = cfg.collector.frames_per_batch\n",
    "# pbar = tqdm.tqdm(total=cfg.collector.total_frames)\n",
    "init_random_frames = cfg.collector.init_random_frames\n",
    "sampling_start = time.time()\n",
    "q_losses = torch.zeros(num_updates) #, device=device)\n",
    "\n",
    "for i, data in enumerate(collector):\n",
    "\n",
    "        # NOTE: This reshape must be for frame data (maybe)\n",
    "        data = data.reshape(-1)\n",
    "        current_frames = data.numel()\n",
    "        replay_buffer.extend(data)\n",
    "        collected_frames += current_frames\n",
    "        greedy_module.step(current_frames)\n",
    "\n",
    "        # Get the number of episodes\n",
    "        total_episodes += data[\"next\", \"done\"].sum()\n",
    "\n",
    "        # Get and log training rewards and episode lengths\n",
    "        # Collect the episode rewards and lengths in average over the\n",
    "        # transitions in the current data batch\n",
    "        episode_rewards = data[\"next\", \"episode_reward\"][data[\"next\", \"done\"]]\n",
    "\n",
    "\n",
    "        # Warmup phase (due to the continue statement)\n",
    "        # Additionally This help us to keep a track of the collected_frames\n",
    "        # after the init_random_frames\n",
    "        if collected_frames < init_random_frames:\n",
    "            continue\n",
    "\n",
    "        # optimization steps\n",
    "        training_start = time.time()\n",
    "        for j in range(num_updates):\n",
    "            sampled_tensordict = replay_buffer.sample(batch_size)\n",
    "            # TODO: check if the sample is already in the device\n",
    "            sampled_tensordict = sampled_tensordict #.to(device)\n",
    "\n",
    "            # Also the loss module will use the current and target model to get the q-values\n",
    "            loss_td = loss_module(sampled_tensordict)\n",
    "            q_loss = loss_td[\"loss\"]\n",
    "\n",
    "            # with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "            #     dot = make_dot(q_loss, params=dict(loss_module.value_network.named_parameters()), show_attrs=True, show_saved=True)\n",
    "            #     dot.render(\"computational_graph_with_loss\", format=\"png\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the priorities\n",
    "            if cfg.buffer.prioritized_replay:\n",
    "                replay_buffer.update_priority(index=sampled_tensordict['index'], priority = sampled_tensordict['td_error'])\n",
    "\n",
    "            # NOTE: This is only one step (after n-updated steps defined before)\n",
    "            # the target will update\n",
    "            target_net_updater.step()\n",
    "            q_losses[j].copy_(q_loss.detach())\n",
    "        training_time = time.time() - training_start\n",
    "\n",
    "        # Get and log evaluation rewards and eval time\n",
    "        # NOTE: As I'm using only the model and not the model_explore that will deterministic I think\n",
    "        # with torch.no_grad(): #, set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "\n",
    "        #     # NOTE: Check how we are using the frames here because it seems that I am dividing \n",
    "        #     # 10 for 50000\n",
    "        #     prev_test_frame = ((i - 1) * frames_per_batch) // test_interval\n",
    "        #     cur_test_frame = (i * frames_per_batch) // test_interval\n",
    "        #     final = current_frames >= collector.total_frames\n",
    "\n",
    "        #     # compara prev_test_frame < cur_test_frame is the same as current_frames % test_interval == 0\n",
    "        #     if (i >= 1 and (prev_test_frame < cur_test_frame)) or final:\n",
    "        #         model.eval()\n",
    "        #         eval_start = time.time()\n",
    "        #         test_rewards = eval_model(model, test_env, num_test_episodes)\n",
    "        #         eval_time = time.time() - eval_start\n",
    "        #         model.train()\n",
    "        #         log_info.update(\n",
    "        #             {\n",
    "        #                 \"eval/reward\": test_rewards,\n",
    "        #                 \"eval/eval_time\": eval_time,\n",
    "        #             }\n",
    "        #         )\n",
    "\n",
    "        # Log all the information\n",
    "\n",
    "        # update weights of the inference policy\n",
    "        # NOTE: Updates the policy weights if the policy of the data \n",
    "        # collector and the trained policy live on different devices.\n",
    "        collector.update_policy_weights_()\n",
    "        sampling_start = time.time()\n",
    "\n",
    "collector.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = DQNLoss(value_network=policy, \n",
    "               action_space=env.action_spec, \n",
    "               delay_value=True) # delay_value=True means we will use a target network\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "\n",
    "# eps: will be used to update the target network as \n",
    "# \\theta_t = \\theta_{t-1} * \\epsilon + \\theta_t * (1-\\epsilon)\n",
    "# where eps = 1 is hard update\n",
    "updater = SoftUpdate(loss, eps=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictParams(params=TensorDict(\n",
       "    fields={\n",
       "        module: TensorDict(\n",
       "            fields={\n",
       "                0: TensorDict(\n",
       "                    fields={\n",
       "                        module: TensorDict(\n",
       "                            fields={\n",
       "                                activation: TensorDict(\n",
       "                                    fields={\n",
       "                                    },\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                encoder: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([3, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                q_net: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False)},\n",
       "                            batch_size=torch.Size([]),\n",
       "                            device=None,\n",
       "                            is_shared=False)},\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False),\n",
       "                1: TensorDict(\n",
       "                    fields={\n",
       "                    },\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=None,\n",
       "            is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.value_network_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictParams(params=TensorDict(\n",
       "    fields={\n",
       "        module: TensorDict(\n",
       "            fields={\n",
       "                0: TensorDict(\n",
       "                    fields={\n",
       "                        module: TensorDict(\n",
       "                            fields={\n",
       "                                activation: TensorDict(\n",
       "                                    fields={\n",
       "                                    },\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                encoder: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([3, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                q_net: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False)},\n",
       "                            batch_size=torch.Size([]),\n",
       "                            device=None,\n",
       "                            is_shared=False)},\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False),\n",
       "                1: TensorDict(\n",
       "                    fields={\n",
       "                    },\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=None,\n",
       "            is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.target_value_network_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode are grouped tensor([3, 3, 4, 4, 2, 2, 1, 1], dtype=torch.int32)\n",
      "steps are successive tensor([0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import LazyMemmapStorage\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(size),\n",
    "    sampler=SliceSampler(traj_key=\"episode\", num_slices=4),\n",
    "    batch_size=8,\n",
    ")\n",
    "episode = torch.zeros(10, dtype=torch.int)\n",
    "episode[:3] = 1\n",
    "episode[3:5] = 2\n",
    "episode[5:7] = 3\n",
    "episode[7:] = 4\n",
    "steps = torch.cat([torch.arange(3), torch.arange(2), torch.arange(2), torch.arange(3)])\n",
    "obs = torch.randn((3, 4, 5)).expand(10, 3, 4, 5)\n",
    "data = TensorDict(\n",
    "    {\n",
    "        \"episode\": episode,\n",
    "        \"obs\": obs,\n",
    "        \"act\": torch.randn((20,)).expand(10, 20),\n",
    "        \"other\": torch.randn((20, 50)).expand(10, 20, 50),\n",
    "        \"steps\": steps,\n",
    "    },\n",
    "    [10],\n",
    ")\n",
    "rb.extend(data)\n",
    "sample = rb.sample()\n",
    "print(\"episode are grouped\", sample[\"episode\"])\n",
    "print(\"steps are successive\", sample[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 3, 3, 4, 4, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyMemmapStorage(\n",
       "        data=TensorDict(\n",
       "            fields={\n",
       "                act: MemoryMappedTensor(shape=torch.Size([10, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                episode: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                index: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                obs: MemoryMappedTensor(shape=torch.Size([10, 3, 4, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                other: MemoryMappedTensor(shape=torch.Size([10, 20, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                steps: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False), \n",
       "        shape=torch.Size([10]), \n",
       "        len=10, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=4, slice_len=None, end_key=('next', 'done'), traj_key=episode, truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=10, full_storage=False), \n",
       "    batch_size=8, \n",
       "    collate_fn=<function _collate_id at 0x7f7435503c40>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_07_23-17_34_50'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "date_str = current_date.strftime(\"%Y_%m_%d-%H_%M_%S\")  # Includes date and time\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/data/replay_buffers/replay_buffers.py:585: UserWarning: Got conflicting batch_sizes in constructor (32) and `sample` (128). Refer to the ReplayBuffer documentation for a proper usage of the batch-size arguments. The batch-size provided to the sample method will prevail.\n",
      "  warnings.warn(\n",
      "2024-07-23 17:34:53,433 [torchrl][INFO] solved after 0 steps, 0 episodes and in 2.57519268989563s.\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in replay buffer\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:][\"next\", \"step_count\"].max() # From all the next steps get the max step count\n",
    "    if len(rb) > init_rand_steps: # wam-up steps\n",
    "        # Optim loop (we do several optim steps\n",
    "        # per batch collected for efficiency)\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128) # sample a batch of 128 (repetition is allowed)\n",
    "            # print(sample)\n",
    "            break\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update exploration factor\n",
    "            # NOTE: Why I am updating the exploration factor here? \n",
    "            # I'm considering practically that I did 100 (or n) iteractions in the environment time optim_steps\n",
    "            exploration_module.step(data.numel()) # data.numel() returns the number of elements in the data\n",
    "            # Update target params each optimisation step\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum() # sum the number of done episodes\n",
    "    \n",
    "    if max_length > 200:\n",
    "        break\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([128, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([128]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_env.rollout(max_steps=1000, policy=policy)\n",
    "video_recorder.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118398, 676190, 786456, 171936, 887739, 919409, 711872, 442081, 189061, 117840]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate and print 10 random seeds\n",
    "random_seeds = [random.randint(0, 1000000) for _ in range(10)]\n",
    "print(random_seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
