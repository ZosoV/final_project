{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48e401c030>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "from torch.optim import Adam\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import CSVLogger, VideoRecorder\n",
    "from torchrl.modules import QValueActor\n",
    "from torchrl.data import CompositeSpec\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "795726461"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the environment\n",
    "env = TransformedEnv(GymEnv(\"CartPole-v1\"), StepCounter())\n",
    "env.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICOMLPNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 activation_class, \n",
    "                 encoder_out_features,\n",
    "                 mlp_out_features,\n",
    "                 encoder_num_cells = None,\n",
    "                 mlp_num_cells = None):\n",
    "        super(MICOMLPNetwork, self).__init__()\n",
    "\n",
    "        self.activation = activation_class()\n",
    "\n",
    "        if encoder_num_cells is None:\n",
    "            encoder_num_cells = []\n",
    "        layers_sizes = [in_features] + encoder_num_cells + [encoder_out_features]\n",
    "\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.encoder.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "\n",
    "        if mlp_num_cells is None:\n",
    "            mlp_num_cells = []\n",
    "\n",
    "        layers_sizes = [encoder_out_features] + mlp_num_cells + [mlp_out_features.item()]\n",
    "\n",
    "        self.q_net = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.q_net.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.encoder)):\n",
    "            x = self.activation(self.encoder[i](x))\n",
    "\n",
    "        representation = x\n",
    "\n",
    "        for i in range(len(self.q_net)-1):\n",
    "            x = self.activation(self.q_net[i](x))\n",
    "\n",
    "        return self.q_net[-1](x), representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MICOMLPNetwork(\n",
       "  (activation): ReLU()\n",
       "  (encoder): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       "  (q_net): ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_mlp = MICOMLPNetwork(\n",
    "    in_features=4,\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    encoder_out_features=3,\n",
    "    mlp_out_features=env.action_spec.shape[-1],\n",
    "    encoder_num_cells=[64],\n",
    "    mlp_num_cells=[64]\n",
    ")\n",
    "\n",
    "value_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0332,  0.2120]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4680, 0.0689, 0.4621]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "q_values, representation = value_mlp(torch.randn(1, 4))\n",
    "print(q_values)\n",
    "print(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=MICOMLPNetwork(\n",
       "      (activation): ReLU()\n",
       "      (encoder): ModuleList(\n",
       "        (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "      )\n",
       "      (q_net): ModuleList(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['action_value', 'representation'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net = Mod(value_mlp, \n",
    "                in_keys=[\"observation\"], \n",
    "                out_keys=[\"action_value\", \"representation\"])\n",
    "value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QValueActor(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MICOMLPNetwork(\n",
       "            (activation): ReLU()\n",
       "            (encoder): ModuleList(\n",
       "              (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "            )\n",
       "            (q_net): ModuleList(\n",
       "              (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "            )\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['action_value', 'representation'])\n",
       "      (1): QValueModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy = Seq(value_net, \n",
    "#              QValueModule(spec=env.action_spec))\n",
    "# policy\n",
    "\n",
    "policy = QValueActor(\n",
    "    module=value_net,\n",
    "    spec=CompositeSpec(action= env.specs[\"input_spec\", \"full_action_spec\", \"action\"]),\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): QValueActor(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=MICOMLPNetwork(\n",
       "                  (activation): ReLU()\n",
       "                  (encoder): ModuleList(\n",
       "                    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "                  )\n",
       "                  (q_net): ModuleList(\n",
       "                    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "                  )\n",
       "                ),\n",
       "                device=cpu,\n",
       "                in_keys=['observation'],\n",
       "                out_keys=['action_value', 'representation'])\n",
       "            (1): QValueModule()\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])\n",
       "      (1): EGreedyModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action_value', 'chosen_action_value', 'action'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the exploration step (e-greedy policy)\n",
    "exploration_module = EGreedyModule(\n",
    "    env.action_spec, \n",
    "    annealing_num_steps=100_000, \n",
    "    eps_init=0.1,\n",
    ")\n",
    "policy_explore = Seq(policy, \n",
    "                     exploration_module)\n",
    "policy_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to collect the data (experiences)\n",
    "init_rand_steps = 5000 # warm-up steps\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "replay_capacity = 100_000\n",
    "\n",
    "# NOTE: collector will gather rollouts continously\n",
    "# If the current trajectory ends, it will start a new one\n",
    "# NOTE: the rollout gotten from the collector is a dictionary\n",
    "# that defines the sate and next state as a tensor with a batch dimension in the begining\n",
    "# for example a rollout of 10 steps will have a tensor of observation of 10 in the batch dimension\n",
    "# and the next will also have 10 which are all the tensors of the next state\n",
    "# Practically, next is as you will shift the tensor of observation by one step\n",
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=frames_per_batch,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=init_rand_steps,\n",
    "# )\n",
    "# rb = ReplayBuffer(storage=LazyTensorStorage(replay_capacity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the recording and logging\n",
    "path = \"./training_loop\"\n",
    "logger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\n",
    "video_recorder = VideoRecorder(logger, tag=\"video\")\n",
    "record_env = TransformedEnv(\n",
    "    GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1063, 0.0000],\n",
      "        [0.1425, 0.0952, 0.0061],\n",
      "        [0.0912, 0.0871, 0.0460],\n",
      "        [0.0490, 0.0752, 0.0896],\n",
      "        [0.0580, 0.0906, 0.1324],\n",
      "        [0.0857, 0.0904, 0.1998],\n",
      "        [0.1152, 0.0904, 0.2713],\n",
      "        [0.1495, 0.0832, 0.3370],\n",
      "        [0.1411, 0.0853, 0.3108],\n",
      "        [0.1740, 0.0766, 0.3788]])\n"
     ]
    }
   ],
   "source": [
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=10,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=10000,\n",
    "# )\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=policy_explore,\n",
    "    frames_per_batch=10,\n",
    "    total_frames=100,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1\n",
    ")\n",
    "# NOTE: IMPORTANTISIMO en las primeras iteraciones no se usa la policy, entonces representation se configura\n",
    "# a zero, por lo que el primer batch de datos no tiene representation\n",
    "# Tengo que hacer el warm-up de otra manera (ojo con esto)\n",
    "\n",
    "for data in collector:\n",
    "    print(data['representation'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1063, 0.0000],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.0912, 0.0871, 0.0460],\n",
       "        [0.0490, 0.0752, 0.0896],\n",
       "        [0.0580, 0.0906, 0.1324],\n",
       "        [0.0857, 0.0904, 0.1998],\n",
       "        [0.1152, 0.0904, 0.2713],\n",
       "        [0.1495, 0.0832, 0.3370],\n",
       "        [0.1411, 0.0853, 0.3108],\n",
       "        [0.1740, 0.0766, 0.3788]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyTensorStorage(\n",
       "        data=<empty>, \n",
       "        shape=None, \n",
       "        len=0, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=None, slice_len=2, end_key=('next', 'done'), traj_key=('collector', 'traj_ids'), truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=0, full_storage=False), \n",
       "    batch_size=10, \n",
       "    collate_fn=<function _collate_id at 0x7f47f936bb00>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "\n",
    "size = 100\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(size),\n",
    "    sampler=SliceSampler(traj_key=(\"collector\",\"traj_ids\"), slice_len=2),\n",
    "    batch_size=10,\n",
    ")\n",
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['collector','traj_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rb.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8],\n",
       "        [0],\n",
       "        [1],\n",
       "        [7],\n",
       "        [8],\n",
       "        [0],\n",
       "        [1],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['step_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1190,  1.4084, -0.1088, -2.0614],\n",
       "        [ 0.1472,  1.2145, -0.1500, -1.8043],\n",
       "        [ 0.0313,  0.0413,  0.0107,  0.0229],\n",
       "        [ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.1190,  1.4084, -0.1088, -2.0614],\n",
       "        [ 0.1472,  1.2145, -0.1500, -1.8043],\n",
       "        [ 0.0313,  0.0413,  0.0107,  0.0229],\n",
       "        [ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.0745,  1.0168, -0.0451, -1.4403],\n",
       "        [ 0.0948,  1.2125, -0.0739, -1.7467]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1472,  1.2145, -0.1500, -1.8043],\n",
       "        [ 0.1715,  1.4110, -0.1861, -2.1396],\n",
       "        [ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.0369,  0.4312,  0.0058, -0.5555],\n",
       "        [ 0.1472,  1.2145, -0.1500, -1.8043],\n",
       "        [ 0.1715,  1.4110, -0.1861, -2.1396],\n",
       "        [ 0.0322,  0.2362,  0.0111, -0.2663],\n",
       "        [ 0.0369,  0.4312,  0.0058, -0.5555],\n",
       "        [ 0.0948,  1.2125, -0.0739, -1.7467],\n",
       "        [ 0.1190,  1.4084, -0.1088, -2.0614]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"next\", \"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1495, 0.0832, 0.3370],\n",
       "        [0.1411, 0.0853, 0.3108],\n",
       "        [0.1717, 0.1063, 0.0000],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.1495, 0.0832, 0.3370],\n",
       "        [0.1411, 0.0853, 0.3108],\n",
       "        [0.1717, 0.1063, 0.0000],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.0857, 0.0904, 0.1998],\n",
       "        [0.1152, 0.0904, 0.2713]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "sample['representation']\n",
    "\n",
    "first_states = sample[0::2] # even rows\n",
    "second_states = sample[1::2] # odd rows (or next states)\n",
    "\n",
    "print(first_states)\n",
    "print(second_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1495, 0.0832, 0.3370],\n",
       "        [0.1717, 0.1063, 0.0000],\n",
       "        [0.1495, 0.0832, 0.3370],\n",
       "        [0.1717, 0.1063, 0.0000],\n",
       "        [0.0857, 0.0904, 0.1998]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1411, 0.0853, 0.3108],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.1411, 0.0853, 0.3108],\n",
       "        [0.1425, 0.0952, 0.0061],\n",
       "        [0.1152, 0.0904, 0.2713]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1495, 0.0832, 0.3370],\n",
       "         [0.1717, 0.1063, 0.0000],\n",
       "         [0.1495, 0.0832, 0.3370],\n",
       "         [0.1717, 0.1063, 0.0000],\n",
       "         [0.0857, 0.0904, 0.1998]]),\n",
       " tensor([[0.0215, 0.2401],\n",
       "         [0.0836, 0.2269],\n",
       "         [0.0215, 0.2401],\n",
       "         [0.0836, 0.2269],\n",
       "         [0.0543, 0.2416]]),\n",
       " tensor([[0.2401],\n",
       "         [0.2269],\n",
       "         [0.2401],\n",
       "         [0.2269],\n",
       "         [0.2416]]),\n",
       " tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Checar la distancia euclidean entre la representacion target y la representacion con\n",
    "# la politica actual\n",
    "\n",
    "# NOTE: en el repositorio de MICO, la distancia target es calculada con\n",
    "# la target network (que es una copia de la politica actual) osea mis representaciones guardadas\n",
    "# la distancia online por otra parte es calculada con una representacion con red actual\n",
    "# y una representacion target\n",
    "\n",
    "\n",
    "collector.policy(first_states['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sample tensor of shape [10, 3]\n",
    "tensor = torch.randn(10, 3)\n",
    "\n",
    "# Separate odd and even rows\n",
    "even_rows = tensor[0::2]\n",
    "odd_rows = tensor[1::2]\n",
    "\n",
    "print(\"Even rows:\\n\", even_rows)\n",
    "print(\"Odd rows:\\n\", odd_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = DQNLoss(value_network=policy, \n",
    "               action_space=env.action_spec, \n",
    "               delay_value=True) # delay_value=True means we will use a target network\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "\n",
    "# eps: will be used to update the target network as \n",
    "# \\theta_t = \\theta_{t-1} * \\epsilon + \\theta_t * (1-\\epsilon)\n",
    "# where eps = 1 is hard update\n",
    "updater = SoftUpdate(loss, eps=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode are grouped tensor([3, 3, 4, 4, 2, 2, 1, 1], dtype=torch.int32)\n",
      "steps are successive tensor([0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import LazyMemmapStorage\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(size),\n",
    "    sampler=SliceSampler(traj_key=\"episode\", num_slices=4),\n",
    "    batch_size=8,\n",
    ")\n",
    "episode = torch.zeros(10, dtype=torch.int)\n",
    "episode[:3] = 1\n",
    "episode[3:5] = 2\n",
    "episode[5:7] = 3\n",
    "episode[7:] = 4\n",
    "steps = torch.cat([torch.arange(3), torch.arange(2), torch.arange(2), torch.arange(3)])\n",
    "obs = torch.randn((3, 4, 5)).expand(10, 3, 4, 5)\n",
    "data = TensorDict(\n",
    "    {\n",
    "        \"episode\": episode,\n",
    "        \"obs\": obs,\n",
    "        \"act\": torch.randn((20,)).expand(10, 20),\n",
    "        \"other\": torch.randn((20, 50)).expand(10, 20, 50),\n",
    "        \"steps\": steps,\n",
    "    },\n",
    "    [10],\n",
    ")\n",
    "rb.extend(data)\n",
    "sample = rb.sample()\n",
    "print(\"episode are grouped\", sample[\"episode\"])\n",
    "print(\"steps are successive\", sample[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 3, 3, 4, 4, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyMemmapStorage(\n",
       "        data=TensorDict(\n",
       "            fields={\n",
       "                act: MemoryMappedTensor(shape=torch.Size([10, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                episode: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                index: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                obs: MemoryMappedTensor(shape=torch.Size([10, 3, 4, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                other: MemoryMappedTensor(shape=torch.Size([10, 20, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                steps: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False), \n",
       "        shape=torch.Size([10]), \n",
       "        len=10, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=4, slice_len=None, end_key=('next', 'done'), traj_key=episode, truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=10, full_storage=False), \n",
       "    batch_size=8, \n",
       "    collate_fn=<function _collate_id at 0x7f7435503c40>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_07_23-17_34_50'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "date_str = current_date.strftime(\"%Y_%m_%d-%H_%M_%S\")  # Includes date and time\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/data/replay_buffers/replay_buffers.py:585: UserWarning: Got conflicting batch_sizes in constructor (32) and `sample` (128). Refer to the ReplayBuffer documentation for a proper usage of the batch-size arguments. The batch-size provided to the sample method will prevail.\n",
      "  warnings.warn(\n",
      "2024-07-23 17:34:53,433 [torchrl][INFO] solved after 0 steps, 0 episodes and in 2.57519268989563s.\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in replay buffer\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:][\"next\", \"step_count\"].max() # From all the next steps get the max step count\n",
    "    if len(rb) > init_rand_steps: # wam-up steps\n",
    "        # Optim loop (we do several optim steps\n",
    "        # per batch collected for efficiency)\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128) # sample a batch of 128 (repetition is allowed)\n",
    "            # print(sample)\n",
    "            break\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update exploration factor\n",
    "            # NOTE: Why I am updating the exploration factor here? \n",
    "            # I'm considering practically that I did 100 (or n) iteractions in the environment time optim_steps\n",
    "            exploration_module.step(data.numel()) # data.numel() returns the number of elements in the data\n",
    "            # Update target params each optimisation step\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum() # sum the number of done episodes\n",
    "    \n",
    "    if max_length > 200:\n",
    "        break\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([128, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([128]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_env.rollout(max_steps=1000, policy=policy)\n",
    "video_recorder.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118398, 676190, 786456, 171936, 887739, 919409, 711872, 442081, 189061, 117840]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate and print 10 random seeds\n",
    "random_seeds = [random.randint(0, 1000000) for _ in range(10)]\n",
    "print(random_seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
