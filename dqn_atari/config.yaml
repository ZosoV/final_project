  
device: null

exp_name: DQN_atari

# Environment
# SEEDS: [118398, 676190, 786456, 171936, 887739, 919409, 711872, 442081, 189061, 117840]
env:
  env_name: Asteroids
  seed: 118398

# collector
# Total time steps corresponds to num_iterations * training_steps
# In other words, there are num_iterations cycles of training_steps steps each
collector:
  num_iterations: 200
  training_steps: 250_000
  frames_per_batch: 4 # update_period in Dopamine
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay_period: 250_000 # TODO: decay has to init after warmup
  init_random_frames: 20_004 # min_replay_history in Dopamine
  frame_skip: 4

# policy
policy:
  type: "CNN_MLP"
  cnn_net:
    num_cells: [32, 64, 64] # number of kernel per layer
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  mlp_net:
    num_cells: [512]  # Number of units per layer
  activation: "ReLU"  # Activation function used in the layers
  use_batch_norm: False

# buffer
buffer:
  buffer_size: 1_000_000
  batch_size: 32
  scratch_dir: null
  prioritized_replay: False
  # These are the hyperparameters related with our work
  mico_priority:
    alpha: 0.6 # alpha: priority exponent
    beta: 0.4 # beta: importance sampling exponent
    priority_type: "current_vs_next" # "all_vs_all" or "current_vs_next"
    priority_weight: 1.0 # 1.0 full BPER and 0 PER
    moving_average: null


# logger
logger:
  backend: wandb
  project_name: test_atari
  mode: "disabled" # "online" or "disabled" ("offline" too but not used)
  save_checkpoints: False
  save_distributions: False
  evaluation:
    enable: False
    num_episodes: 5

# Optim
optim:
  lr: 6.25e-5
  max_grad_norm: null
  eps: 1.5e-4 # default: 1e-08 # rainbow 1.5e-4

# loss
loss:
  double_dqn: False
  gamma: 0.99 #
  target_update_period: 8_000 
  mico_loss:
    enable: False
    mico_weight: 0.01 #0.5 # 0.01 for DQN and Rainbow
    mico_beta: 0.1
    mico_gamma: 0.99

