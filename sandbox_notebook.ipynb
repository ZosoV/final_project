{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/safijari/dqn-tutorial/reports/Deep-Q-Networks-DQN-With-the-Cartpole-Environment--Vmlldzo4MDc2MQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"action_value\"] # shape (batch_size, num_actions)\n",
    "# Get the mean q_value from the batch data[\"action_value\"]\n",
    "# data[\"action_value\"].mean()\n",
    "\n",
    "import torch\n",
    "# Create a toy tensor with shape (10, 2)\n",
    "tensor = torch.rand(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4304, 0.3132],\n",
       "        [0.7245, 0.4431],\n",
       "        [0.9938, 0.1911],\n",
       "        [0.6044, 0.1895],\n",
       "        [0.4958, 0.2732],\n",
       "        [0.5166, 0.9291],\n",
       "        [0.2419, 0.7202],\n",
       "        [0.1368, 0.6574],\n",
       "        [0.7252, 0.6230],\n",
       "        [0.1141, 0.6644]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4994)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mean(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4994)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efc1421c130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "from torch.optim import Adam\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import CSVLogger, VideoRecorder\n",
    "from torchrl.modules import QValueActor\n",
    "from torchrl.data import CompositeSpec\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import (\n",
    "    CatFrames,\n",
    "    DoubleToFloat,\n",
    "    EndOfLifeTransform,\n",
    "    GrayScale,\n",
    "    GymEnv,\n",
    "    NoopResetEnv,\n",
    "    Resize,\n",
    "    RewardSum,\n",
    "    SignTransform,\n",
    "    StepCounter,\n",
    "    ToTensorImage,\n",
    "    TransformedEnv,\n",
    "    VecNorm,\n",
    ")\n",
    "\n",
    "def make_env(env_name=\"CartPole-v1\", frame_skip = 4, \n",
    "             device=\"cpu\", seed = 0, is_test=False):\n",
    "\n",
    "    env = GymEnv(\n",
    "        env_name,\n",
    "        frame_skip=frame_skip,\n",
    "        from_pixels=True,\n",
    "        pixels_only=False,\n",
    "        device=device,\n",
    "    )\n",
    "    env = TransformedEnv(env)\n",
    "    # env.append_transform(NoopResetEnv(noops=30, random=True)) # NOTE: Cartpole with no noops will fall into reset in the begining\n",
    "                                                                # I could use an small noop reset to avoid this, but I think is not necesary\n",
    "                                                                # in this case. Analyze this later\n",
    "    if not is_test:\n",
    "        # env.append_transform(EndOfLifeTransform()) # NOTE: Check my environment is not based on lives (so not important)\n",
    "        env.append_transform(SignTransform(in_keys=[\"reward\"])) #NOTE: cartpole has no negative rewards\n",
    "    env.append_transform(ToTensorImage()) \n",
    "    env.append_transform(GrayScale())\n",
    "    env.append_transform(Resize(84, 84))\n",
    "    env.append_transform(CatFrames(N=4, dim=-3))\n",
    "    env.append_transform(RewardSum())\n",
    "    env.append_transform(StepCounter()) # NOTE: Cartpole-v1 has a max of 500 steps\n",
    "    env.append_transform(DoubleToFloat())\n",
    "    env.append_transform(VecNorm(in_keys=[\"pixels\"]))\n",
    "    env.set_seed(seed)\n",
    "\n",
    "    # NOTE: a rollout will be take a trajectory of frames and group the frames by N=4 sequentially\n",
    "    # so that the output will be 7x4x84x84 because with a rollout of 10 steps we will have 7 groups of\n",
    "    # 4 frames each\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_name=\"CartPole-v1\", \n",
    "               frame_skip = 4, \n",
    "               device=\"cpu\", #\n",
    "               seed = 0,  \n",
    "               is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acrobot-v1',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'Blackjack-v1',\n",
       " 'CarRacing-v2',\n",
       " 'CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'GymV21Environment-v0',\n",
       " 'GymV26Environment-v0',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Taxi-v3',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'phys2d/CartPole-v0',\n",
       " 'phys2d/CartPole-v1',\n",
       " 'phys2d/Pendulum-v0',\n",
       " 'tabular/Blackjack-v0',\n",
       " 'tabular/CliffWalking-v0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 1, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 1, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 2, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 6, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 4, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 5, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n",
      "Action: 4, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "dict_keys(['image', 'direction', 'mission'])\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\") #, render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(10):\n",
    "   # Random action\n",
    "   action = np.random.randint(env.action_space.n)\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   print(f\"Action: {action}, Reward: {reward}\")\n",
    "   print(f\"Observation shape: {observation['image'].shape}\")\n",
    "   print(observation.keys())\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(7)\n",
      "Action Space Type: <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "Observation Space: Dict('direction': Discrete(4), 'image': Box(0, 255, (7, 7, 3), uint8), 'mission': MissionSpace(<function EmptyEnv._gen_mission at 0x7f9da0186980>, None))\n",
      "Observation Space Type: <class 'gymnasium.spaces.dict.Dict'>\n",
      "Number of actions: 7\n",
      "Action: 2, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 6, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 6, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 6, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 0, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 4, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 4, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n",
      "Action: 4, Reward: 0\n",
      "Observation shape: (7, 7, 3)\n",
      "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
      "Mission: get to the green goal square\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\") #, render_mode=\"human\")\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Print action space details\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Action Space Type:\", type(env.action_space))\n",
    "\n",
    "# Print observation space details\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Observation Space Type:\", type(env.observation_space))\n",
    "\n",
    "# Print more detailed information if it's a Box or Discrete space\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    print(\"Number of actions:\", env.action_space.n)\n",
    "\n",
    "if isinstance(env.observation_space, gym.spaces.Box):\n",
    "    print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "    print(\"Observation Space High:\", env.observation_space.high)\n",
    "    print(\"Observation Space Low:\", env.observation_space.low)\n",
    "\n",
    "# Step through the environment\n",
    "for _ in range(10):\n",
    "    # Random action\n",
    "    action = np.random.randint(env.action_space.n)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Observation shape: {observation['image'].shape}\")\n",
    "    print(\"Observation keys:\", observation.keys())\n",
    "    print(\"Mission:\", observation['mission'])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "display display: Unable to load font (-*-helvetica-medium-r-normal--12-*-*-*-*-*-iso8859-1) [Resource temporarily unavailable].\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class SimpleGridEnv(gym.Env):\n",
    "    def __init__(self, grid_size=5):\n",
    "        super(SimpleGridEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: left, right, up, down\n",
    "        self.observation_space = spaces.Box(low=0, high=grid_size-1, shape=(2,), dtype=np.int32)\n",
    "        self.state = None\n",
    "        self.goal = np.array([grid_size-1, grid_size-1])  # Goal position\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0])  # Starting position\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   # Left\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 1: # Right\n",
    "            self.state[1] = min(self.grid_size - 1, self.state[1] + 1)\n",
    "        elif action == 2: # Up\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 3: # Down\n",
    "            self.state[0] = min(self.grid_size - 1, self.state[0] + 1)\n",
    "        \n",
    "        done = np.array_equal(self.state, self.goal)\n",
    "        reward = 1 if done else -0.1  # Reward for reaching the goal\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        cell_size = 84 // self.grid_size\n",
    "        img = Image.new('RGB', (84, 84), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                top_left = (j * cell_size, i * cell_size)\n",
    "                bottom_right = ((j + 1) * cell_size, (i + 1) * cell_size)\n",
    "                draw.rectangle([top_left, bottom_right], outline='black')\n",
    "        \n",
    "        # Draw agent\n",
    "        agent_top_left = (self.state[1] * cell_size, self.state[0] * cell_size)\n",
    "        agent_bottom_right = ((self.state[1] + 1) * cell_size, (self.state[0] + 1) * cell_size)\n",
    "        draw.rectangle([agent_top_left, agent_bottom_right], fill='blue')\n",
    "        \n",
    "        # Draw goal\n",
    "        goal_top_left = (self.goal[1] * cell_size, self.goal[0] * cell_size)\n",
    "        goal_bottom_right = ((self.goal[1] + 1) * cell_size, (self.goal[0] + 1) * cell_size)\n",
    "        draw.rectangle([goal_top_left, goal_bottom_right], fill='green')\n",
    "        \n",
    "        if mode == 'human':\n",
    "            img.show()\n",
    "        elif mode == 'rgb_array':\n",
    "            return np.array(img)\n",
    "\n",
    "# Example usage\n",
    "env = SimpleGridEnv(grid_size=5)\n",
    "observation = env.reset()\n",
    "env.render(mode='human')\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(observation.shape)\n",
    "    # env.render(mode='human')\n",
    "    if done:\n",
    "        print(\"Goal reached!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "Action Space Type: <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Observation Space Type: <class 'gymnasium.spaces.box.Box'>\n",
      "Number of actions: 2\n",
      "Observation Space Shape: (4,)\n",
      "Observation Space High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Observation Space Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Print action space details\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Action Space Type:\", type(env.action_space))\n",
    "\n",
    "# Print observation space details\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Observation Space Type:\", type(env.observation_space))\n",
    "\n",
    "# Print more detailed information if it's a Box or Discrete space\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    print(\"Number of actions:\", env.action_space.n)\n",
    "\n",
    "if isinstance(env.observation_space, gym.spaces.Box):\n",
    "    print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "    print(\"Observation Space High:\", env.observation_space.high)\n",
    "    print(\"Observation Space Low:\", env.observation_space.low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFzElEQVR4nO3c0WrjMBRAwWjJh/fPtQ+CA5t1QA44TWDm2aHiInxQ1HbMOecNAG6325/fXgAAn0MUAIgoABBRACCiAEBEAYCIAgARBQBy331wjHHlOgA44aq/O3ZSACCiAEBEAYBs3yk88n/0luO7FrNZ/p2NPbMc7RmzWR5nYy7LO+90nRQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADImHPOrQfHuHotAGzafHWf5qQAQEQBgIgCABEFAHJ/9YNXXXJ8m6MLeLNZHmdjLos985w9c+ydv+jjpABARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEDGnHNuPTjG1WsBYNPmq/s0JwUAIgoARBQAyP3VD171fda3ObprMZvlcTbmstgzz9kzx955p+ukAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQMacc249OMbVawFg0+ar+zQnBQAiCgBEFACIKACQ+6sfvOqS49scXcCbzfI4G3NZ7Jnn7Jlj7/xFHycFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAMuacc+vBMa5eCwCbNl/dpzkpABBRACCiAEDur37wqu+zvs3RXYvZLI+zMZfFnnnuv9n8/MoyPs/P+36UkwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgAZc8659eAYV68FgE2br+7TnBQAiCgAEFEAIKIAQO67D151qQHA53BSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAOQvUgt2+H5btgsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFzUlEQVR4nO3cwWrjMBRA0WjIh/fPNQvDXbgOKAGlCZyzdqh4CF8UtR1zznkDgNvt9u+vFwDA5xAFACIKAEQUAIgoABBRACCiAEBEAYDcVx8cY+xcBwBP2PV3x04KAEQUAIgoAJDlO4Uz/0fvcHXXYjaH82zM5WDPPGbPXHvnna6TAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABlzzrn04Bi71wLAosVX99OcFACIKAAQUQAgogBA7q9+cNclx7e5uoA3m8N5NuZysGces2euvfMXfZwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyJhzzqUHx9i9FgAWLb66n+akAEBEAYCIAgC5v/7RPd9nfZ/fdy27vuv7Nud7KHM5XN3Pmc3Bnrn2zjtdJwUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAy5pxz6cExdq8FgEWLr+6nOSkAEFEAIKIAQEQBgNxf/eCuS45vc3UBbzaH82zM5WDPPGbPXHvnL/o4KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJAx55xLD46xey0ALFp8dT/NSQGAiAIAEQUAcn/1g7u+z/o2V3ctZnM4z8ZcDvbMY79m8/Mny/g8P+/7UU4KAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAZMw559KDY+xeCwCLFl/dT3NSACCiAEBEAYCIAgC5rz6461IDgM/hpABARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIf6BqdvgXBfkuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SimpleGridEnv(gym.Env):\n",
    "    def __init__(self, grid_size=5, render_mode = None):\n",
    "        super(SimpleGridEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: left, right, up, down\n",
    "        self.observation_space = spaces.Box(low=0, high=grid_size-1, shape=(2,), dtype=np.int32)\n",
    "        self.state = None\n",
    "        self.goal = np.array([grid_size-1, grid_size-1])  # Goal position\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0])  # Starting position\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   # Left\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 1: # Right\n",
    "            self.state[1] = min(self.grid_size - 1, self.state[1] + 1)\n",
    "        elif action == 2: # Up\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 3: # Down\n",
    "            self.state[0] = min(self.grid_size - 1, self.state[0] + 1)\n",
    "        \n",
    "        done = np.array_equal(self.state, self.goal)\n",
    "        reward = 1 if done else -0.1  # Reward for reaching the goal\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        cell_size = 84 // self.grid_size\n",
    "        img = Image.new('RGB', (84, 84), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw agent\n",
    "        agent_top_left = (self.state[1] * cell_size, self.state[0] * cell_size)\n",
    "        agent_bottom_right = ((self.state[1] + 1) * cell_size, (self.state[0] + 1) * cell_size)\n",
    "        draw.rectangle([agent_top_left, agent_bottom_right], fill='blue')\n",
    "        \n",
    "        # Draw goal\n",
    "        goal_top_left = (self.goal[1] * cell_size, self.goal[0] * cell_size)\n",
    "        goal_bottom_right = ((self.goal[1] + 1) * cell_size, (self.goal[0] + 1) * cell_size)\n",
    "        draw.rectangle([goal_top_left, goal_bottom_right], fill='green')\n",
    "\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                top_left = (j * cell_size, i * cell_size)\n",
    "                bottom_right = ((j + 1) * cell_size, (i + 1) * cell_size)\n",
    "                draw.rectangle([top_left, bottom_right], outline='black')\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            img.show()\n",
    "        elif self.render_mode == 'rgb_array':\n",
    "            return np.array(img)\n",
    "\n",
    "# Example usage\n",
    "env = SimpleGridEnv(grid_size=5, render_mode='rgb_array')\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    pixels = env.render()\n",
    "\n",
    "    print(pixels.shape)\n",
    "\n",
    "    # Convert numpy array to PIL image and resize to 84x84 pixels\n",
    "    img = Image.fromarray(pixels)\n",
    "\n",
    "    # Display the resized image using matplotlib\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "    # Close the environment\n",
    "    if done:\n",
    "        print(\"Goal reached!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['x', 'x', 'x', 'x', 'x'],\n",
       "       ['x', '.', '.', 'G', 'x'],\n",
       "       ['x', '.', '.', '.', 'x'],\n",
       "       ['x', 'x', '.', 'x', 'x'],\n",
       "       ['x', '.', '.', 'G', 'x'],\n",
       "       ['x', '.', '.', '.', 'x'],\n",
       "       ['x', 'x', 'x', 'x', 'x']], dtype='<U1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"grid.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    print(len(lines[0].split()))\n",
    "    print(len(lines))\n",
    "\n",
    "np.array([line.split() for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SimpleGridEnv-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n",
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:189: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Shape mismatch: the value has shape torch.Size([]) which is incompatible with the spec shape torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/data/tensor_specs.py:579\u001b[0m, in \u001b[0;36mTensorSpec.encode\u001b[0;34m(self, val, ignore_device)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2]' is invalid for input of size 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Create and wrap the environment\u001b[39;00m\n\u001b[1;32m    134\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimpleGridEnv-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, grid_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_envs/grid_envs/grid_world2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m env_torchrl \u001b[38;5;241m=\u001b[39m \u001b[43mGymWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    138\u001b[0m observation \u001b[38;5;241m=\u001b[39m env_torchrl\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/libs/gym.py:543\u001b[0m, in \u001b[0;36m_AsyncMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 543\u001b[0m     instance: GymWrapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;66;03m# before gym 0.22, there was no final_observation\u001b[39;00m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m_is_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:175\u001b[0m, in \u001b[0;36m_EnvPostInit.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m auto_reset \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_reset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    174\u001b[0m auto_reset_replace \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_reset_replace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m instance: EnvBase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# we create the done spec by adding a done/terminated entry if one is missing\u001b[39;00m\n\u001b[1;32m    177\u001b[0m instance\u001b[38;5;241m.\u001b[39m_create_done_specs()\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/libs/gym.py:729\u001b[0m, in \u001b[0;36mGymWrapper.__init__\u001b[0;34m(self, env, categorical_action_encoding, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_gym_backend(libname):\n\u001b[1;32m    728\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m--> 729\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:3021\u001b[0m, in \u001b[0;36m_EnvWrapper.__init__\u001b[0;34m(self, device, batch_size, allow_done_after_reset, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_specs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env)  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   3020\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3021\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/libs/gym.py:1139\u001b[0m, in \u001b[0;36mGymWrapper._init_env\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_env\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2120\u001b[0m, in \u001b[0;36mEnvBase.reset\u001b[0;34m(self, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m-> 2120\u001b[0m tensordict_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;66;03m#        We assume that this is done properly\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;66;03m#        if reset.device != self.device:\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;66;03m#            reset = reset.to(self.device, non_blocking=True)\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict_reset \u001b[38;5;129;01mis\u001b[39;00m tensordict:\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/libs/gym.py:1165\u001b[0m, in \u001b[0;36mGymWrapper._reset\u001b[0;34m(self, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m reset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/gym_like.py:360\u001b[0m, in \u001b[0;36mGymLikeEnv._reset\u001b[0;34m(self, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reset\u001b[39m(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: Optional[TensorDictBase] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorDictBase:\n\u001b[1;32m    358\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_output_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 360\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m TensorDict(\n\u001b[1;32m    363\u001b[0m         source\u001b[38;5;241m=\u001b[39msource,\n\u001b[1;32m    364\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    365\u001b[0m         _run_checks\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidated,\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo_dict_reader \u001b[38;5;129;01mand\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/gym_like.py:267\u001b[0m, in \u001b[0;36mGymLikeEnv.read_obs\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, spec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_spec\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    266\u001b[0m     observations_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 267\u001b[0m     observations_dict[key] \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# we don't check that there is only one spec because obs spec also\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# contains the data spec of the info dict.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/data/tensor_specs.py:581\u001b[0m, in \u001b[0;36mTensorSpec.encode\u001b[0;34m(self, val, ignore_device)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 val \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 581\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    582\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: the value has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the spec shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _CHECK_SPEC_ENCODE:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_is_in(val)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Shape mismatch: the value has shape torch.Size([]) which is incompatible with the spec shape torch.Size([2])."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleGridEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, grid_file=None):\n",
    "        super(SimpleGridEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.goal_positions = []\n",
    "        self.load_grid_from_file(grid_file)\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: left, right, up, down\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size-1, shape=(2,), dtype=np.int32)\n",
    "        self.state = None\n",
    "        \n",
    "        \n",
    "\n",
    "    def load_grid_from_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            self.height = len(lines)\n",
    "            self.width = len(lines[0].split())\n",
    "            self.grid_size = max(self.height, self.width)\n",
    "\n",
    "            if self.height != self.width:\n",
    "                print(\"Warning: The grid is not square. Padding with 'x' to make it square.\")\n",
    "            self.grid = np.array([line.split() for line in lines])\n",
    "\n",
    "        def make_square(grid):\n",
    "            # Get the dimensions of the grid\n",
    "            rows, cols = grid.shape\n",
    "            \n",
    "            # Determine the size of the square matrix\n",
    "            size = max(rows, cols)\n",
    "            \n",
    "            # Calculate padding for rows and columns\n",
    "            row_padding_top = (size - rows) // 2\n",
    "            row_padding_bottom = size - rows - row_padding_top\n",
    "            col_padding_left = (size - cols) // 2\n",
    "            col_padding_right = size - cols - col_padding_left\n",
    "            \n",
    "            # Pad the grid with 'x' to make it square\n",
    "            square_grid = np.pad(grid, \n",
    "                                pad_width=((row_padding_top, row_padding_bottom), (col_padding_left, col_padding_right)),\n",
    "                                mode='constant', \n",
    "                                constant_values='x')\n",
    "            \n",
    "            return square_grid\n",
    "           \n",
    "        self.grid = make_square(self.grid)\n",
    "\n",
    "            # Add walls in the width or height to make the grid square, but keep it centered\n",
    "\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if self.grid[i, j] == 'G':\n",
    "                    self.goal_positions.append((i, j))\n",
    "        \n",
    "\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Find a starting position randomly among the '.' positions\n",
    "        start_positions = np.argwhere(self.grid == '.')\n",
    "        start_idx = np.random.choice(len(start_positions))\n",
    "        self.state = start_positions[start_idx]\n",
    "        if return_info:\n",
    "            return self.state, {}\n",
    "        else:\n",
    "            return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:   # Left\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 1: # Right\n",
    "            self.state[1] = min(self.grid_size - 1, self.state[1] + 1)\n",
    "        elif action == 2: # Up\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 3: # Down\n",
    "            self.state[0] = min(self.grid_size - 1, self.state[0] + 1)\n",
    "        \n",
    "        # Check for wall collisions\n",
    "        if self.grid[self.state[0], self.state[1]] == 'x':\n",
    "            if action == 0:   # Move back to the right\n",
    "                self.state[1] += 1\n",
    "            elif action == 1: # Move back to the left\n",
    "                self.state[1] -= 1\n",
    "            elif action == 2: # Move back down\n",
    "                self.state[0] += 1\n",
    "            elif action == 3: # Move back up\n",
    "                self.state[0] -= 1\n",
    "        \n",
    "        done = tuple(self.state) in self.goal_positions\n",
    "        reward = 1 if done else -0.1  # Reward for reaching the goal\n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        cell_size = 84 // self.grid_size\n",
    "        img = Image.new('RGB', (84, 84), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                top_left = (j * cell_size, i * cell_size)\n",
    "                bottom_right = ((j + 1) * cell_size, (i + 1) * cell_size)\n",
    "                if self.grid[i, j] == 'x':\n",
    "                    draw.rectangle([top_left, bottom_right], fill='#646464ff', outline='#646464ff')\n",
    "                elif self.grid[i, j] == 'G':\n",
    "                    draw.rectangle([top_left, bottom_right], fill='#00FF00', outline='#646464ff')\n",
    "                else:\n",
    "                    draw.rectangle([top_left, bottom_right], fill='#000000', outline='#646464ff')\n",
    "        \n",
    "        # Draw agent\n",
    "        agent_top_left = (self.state[1] * cell_size, self.state[0] * cell_size)\n",
    "        agent_bottom_right = ((self.state[1] + 1) * cell_size, (self.state[0] + 1) * cell_size)\n",
    "        draw.rectangle([agent_top_left, agent_bottom_right], fill='#0000FF', outline='#646464ff')\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            img.show()\n",
    "        elif self.render_mode == 'rgb_array':\n",
    "            return np.array(img)\n",
    "\n",
    "# Example usage\n",
    "grid_file_content = \"\"\"\\\n",
    "x x x x x\n",
    "x . . G x\n",
    "x . . . x\n",
    "x x . x x\n",
    "x . . G x\n",
    "x . . . x\n",
    "x x x x x\n",
    "\"\"\"\n",
    "\n",
    "# with open('grid.txt', 'w') as file:\n",
    "#     file.write(grid_file_content)\n",
    "\n",
    "env = SimpleGridEnv(grid_file='custom_envs/grid_envs/grid_world2.txt', render_mode='rgb_array')\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    print(observation)\n",
    "\n",
    "    pixels = env.render()\n",
    "\n",
    "    print(pixels.shape)\n",
    "\n",
    "    # Convert numpy array to PIL image and resize to 84x84 pixels\n",
    "    img = Image.fromarray(pixels)\n",
    "\n",
    "    # Display the resized image using matplotlib\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "    # Convert to gray scale and plot\n",
    "    # gray_img = img.convert('L')\n",
    "    # plt.imshow(gray_img, cmap='gray')\n",
    "    # plt.axis('off')  # Hide axis\n",
    "    # plt.show()\n",
    "\n",
    "    if done:\n",
    "        print(\"Goal reached!\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, grid_file = None):\n",
    "\n",
    "        # Load from a file\n",
    "        if grid_file is None:\n",
    "            raise ValueError(\"You must provide a grid file. As an example, you can use the grid.txt file.\")\n",
    "        \n",
    "        self._grid = self.load_grid_from_file(grid_file)\n",
    "\n",
    "        self.size = self._grid.shape[0] # if self._grid else size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Set walls and target\n",
    "        self._set_components()\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        # self.observation_space = spaces.Dict(\n",
    "        #     {\n",
    "        #         \"agent\": spaces.Box(0, self.size - 1, shape=(2,), dtype=int),\n",
    "        #         \"target\": spaces.Box(0, self.size - 1, shape=(2,), dtype=int),\n",
    "        #     }\n",
    "        # )\n",
    "        self.observation_space = spaces.Box(low=0, high=self.size-1, shape=(2,), dtype=np.int32)\n",
    "\n",
    "\n",
    "        # We have 4 actions, corresponding to \"down\", \"right\", \"up\" and \"left\".\n",
    "        # Action 0: Down\n",
    "        # Action 1: Right\n",
    "        # Action 2: Up\n",
    "        # Action 3: Left\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def load_grid_from_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            self.height = len(lines)\n",
    "            self.width = len(lines[0].split())\n",
    "            self.grid_size = self.height\n",
    "\n",
    "            if self.height != self.width:\n",
    "                # Raise a error message\n",
    "                raise ValueError(\"Invalid shape of widht and height must be equal, add extra walls to make it square\")\n",
    "\n",
    "            return np.array([line.split() for line in lines])\n",
    "        \n",
    "    def _set_components(self):\n",
    "        self._walls_location = []\n",
    "        self._targets_location = []\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self._grid[i, j] == 'x':\n",
    "                    self._walls_location.append((i, j))\n",
    "                elif self._grid[i, j] == 'G':\n",
    "                    self._targets_location.append((i, j))\n",
    "        self._walls_location = set(self._walls_location)\n",
    "        self._targets_location = set(self._targets_location)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # return {\"agent\": self._agent_location, \"target\": self._targets_location}\n",
    "        return self._agent_location\n",
    "\n",
    "    def _get_info(self):\n",
    "        # return {\n",
    "        #     \"distance\": np.linalg.norm(\n",
    "        #         self._agent_location - self._target_location, ord=1\n",
    "        #     )\n",
    "        # }\n",
    "        return {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # We will sample the agent's location randomly until it does not coincide with the target or walls's location\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        while tuple(self._agent_location) in self._walls_location or tuple(self._agent_location) in self._targets_location:\n",
    "            self._agent_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        # while np.array_equal(self._target_location, self._agent_location) or tuple(self._agent_location) in self._walls_location:\n",
    "        #     self._agent_location = self.np_random.integers(\n",
    "        #         0, self.size, size=2, dtype=int\n",
    "        #     )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # if to make sure we don't walk into a wall\n",
    "        if tuple(self._agent_location + direction) not in self._walls_location:\n",
    "            # We use `np.clip` to make sure we don't leave the grid\n",
    "            self._agent_location = np.clip(\n",
    "                self._agent_location + direction, 0, self.size - 1\n",
    "            )\n",
    "\n",
    "\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        # terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        terminated = tuple(self._agent_location) in self._targets_location\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the walls\n",
    "        for wall_location in self._walls_location:\n",
    "            pygame.draw.rect(\n",
    "                canvas,\n",
    "                (0, 0, 0),\n",
    "                pygame.Rect(\n",
    "                    pix_square_size * np.array(wall_location)[::-1], # I have to switch to display in order (rows, cols)\n",
    "                    (pix_square_size, pix_square_size),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # Second we draw the targets\n",
    "        for target_location in self._targets_location:\n",
    "            pygame.draw.rect(\n",
    "                canvas,\n",
    "                (255, 0, 0),\n",
    "                pygame.Rect(\n",
    "                    pix_square_size * np.array(target_location)[::-1], # I have to switch to display in order (rows, cols)\n",
    "                    (pix_square_size, pix_square_size),\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location[::-1] + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walls = set([(1,2),(3,4)])\n",
    "arr = np.array([1,2])\n",
    "\n",
    "# Check if wall in arr\n",
    "tuple(arr) in walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "walls = set([(1,2),(3,4)])\n",
    "for wall in walls:\n",
    "    print(np.array(wall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment GridWorldEnv-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='GridWorldEnv-v0',\n",
    "    entry_point='__main__:GridWorldEnv',\n",
    "    max_episode_steps=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Action Space Type: <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "Observation Space: Box(0, 6, (2,), int32)\n",
      "Observation Space Type: <class 'gymnasium.spaces.box.Box'>\n",
      "Number of actions: 4\n",
      "Observation Space Shape: (2,)\n",
      "Observation Space High: [6 6]\n",
      "Observation Space Low: [0 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the cartpole environment\n",
    "env = gym.make(\"GridWorldEnv-v0\", grid_file='custom_envs/grid_envs/grid_world2.txt')\n",
    "\n",
    "# Print action space details\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Action Space Type:\", type(env.action_space))\n",
    "\n",
    "# Print observation space details\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Observation Space Type:\", type(env.observation_space))\n",
    "\n",
    "# Print more detailed information if it's a Box or Discrete space\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    print(\"Number of actions:\", env.action_space.n)\n",
    "\n",
    "if isinstance(env.observation_space, gym.spaces.Box):\n",
    "    print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "    print(\"Observation Space High:\", env.observation_space.high)\n",
    "    print(\"Observation Space Low:\", env.observation_space.low)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [4 3]\n",
      "Initial Info: {}\n",
      "Action: 2, Reward: 0\n",
      "Observation: [3 3]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be int32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be int32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Reward: 0\n",
      "Observation: [3 3]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2, Reward: 0\n",
      "Observation: [2 3]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAANmklEQVR4nO3dX4ycdb3H8c84C61iEUmJJRhNEBqamDZpKfFPDSZcWCgm3OlJiCCXGoUGE5OKgiEKpOGigUtiQ7jAGC4wBkNFpAbUtJyCiSDFP9QEDmmUGqmU7LKz+5yLPXwjJ8eT1Zntb/fZ1yvZdNPOzu/7dDrPe57nmWYGXdd1AYAk72o9AADLhygAUEQBgCIKABRRAKCIAgBFFAAoogBAmVrsDQeDwVLOAcASW8z/VXakAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAlKnWA7T2gx/8IBs3bmw9xtiee+65XHvttVm3bl0OHjyY4XDYeqSx7d+/P/v27cuOHTty7733th5nIr70pS/ll7/8ZW666aZcf/31rccZ22g0yuWXX55Tp07lwSSbWg80AS8k+Y/WQzS06qOwcePGbNmypfUYY5uZmUmSDIfDbN68OVNTK/+h3bBhQ5Jk3bp1vXiMkoVtSZLzzz+/F9s0OztbL0AuTrLytyiZaz1AY04fAVBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAMpU6wFae+655zIzM9N6jLH99re/TZKMRqM8/fTTGQ6HjSca3yuvvJIkef3113P48OHG00zG66+/niR5+eWXe7FNo9Eoc3NzSZLnk8y1HWciXmw9QGODruu6Rd1wMFjqWQBYQovZ3a/6I4V169b14lX1aDTKG2+8kcFgkPe9732tx5mI6enpTE9PZ2pqKu9973tbjzMRb7zxRkajUdauXZu1a9e2Hmci/va3vyXpz3Npbm4uf//731uP0cyqP1I4cuRINm/e3HqMsT399NP5xCc+kXPOOSfHjx/vxZNz79692bNnT3bu3Jkf/ehHrceZiF27duUnP/lJ7rzzztx8882txxnbaDTKBz7wgZw8eTKHDh3K1q1bW480tmeffTaXXXZZ6zGWhCOFRRgOh5maWvl/Df8Ygb5s09svRAaDQS+2J+nfNv3jTqYv/+768IJqHN59BEARBQCKKABQRAGAIgoAlJX/VgH4F/3lL8nsbNJ1yec/n5w48c4/X78+efDBZDBIzjgjOe+8NnNCC6LAqvHnPyePPJLcfnty7Nj/f9sPfnDh1wsvTG65Jbn6anFgdRAFeq/rkrvvTh59NHn88X/tZ196KbnhhuSKK5Krrkp27144goC+EgV67Te/SR5+eOHoYHb237+fxx9PnnwyefPN5Jprko9+dFITwvLiQjO99eijyac/nXzrW+MF4W1vvZV885sL93ngwPj3B8uRKNA7Xbfwyv6LX0z++tfJ3/+JE8n11ydPPLGwFvSJKNA7P/tZsmtXcvz40q1x/PjCNYYnnli6NaAFUaBXui757neT0/G5SdPTyR13OFqgX0SB3piZSb7zneQXvzh9az711EIYevDhfZBEFOiR3/9+4ULw6dxBT08n3/hG8sc/nr41YSmJAr3QdcmePe3W37PHaST6QRTohRdeOL2njf63p55Kjh5ttz5MiijQC3fdtTRvP12sEyeSvXvbrQ+TIgoAFFFgxTt2bHlc6P3DH5I//an1FDAeUWDFO3So7fWEtz35ZHL4cOspYDyiAEARBQCKKABQRAGAIgqseJdemnzsY62nSD7+8WTbttZTwHhEgRXvoouSjRtbT5FccknykY+0ngLGIwoAFFGgF3bvTt7//nbrn3tucuON7daHSREFemHLlmT79nbrX3ZZsnlzu/VhUkSBXhgMkrvvbrf+3XcvzAArnSjQGxddtPCBN2eccfrWPPPMhQ/2ufDC07cmLCVRoDfWrk1uvz3ZseP0rfmpTyXf/vbC2tAHokCvDAbJ1752eo4Wzjwzuflmp43oF1Ggd668Mnn44WT9+qVbY/365Ic/THbuXLo1oAVRoHcGg+Sqq5LvfW9p3qZ67rnJ/v0LQXCUQN+IAr312c8mBw4ke/YkU1Pj39/U1MKF7AMHkquvHv/+YDmawFMFlq/t25OtW5N3vzt59NF//8N4duxYOC319a8nw+FkZ4TlxJECvTccJrfckjz0UHLvvcmHPrT4n/3whxd+5qGHFo44BIG+c6TAqrFhQ/LlLyfXXJPMzCz83uc+l5w48c7brV+ffP/7C9+vWZNccMFpHROaEgVWnX/cyf+zz1R2AZnVShRY1ez84Z1cUwCgiAIARRQAKKIAQBEFAIooAFBW/VtS9+/fnw0bNrQeY2yvvPJKkmR6ejp79+7NoAfvtfz5z3+eJHnppZdy5513Np5mMo4dO5YkOXjwYObn5xtPM775+fnM/M//BLz//vvz2GOPNZ5ofK+++mrrEZoadF3XLeqGPdjJAKxmi9ndr/ojBZa/8847L9u3b289xkQcPnw4r732Wusx4J8SBZa9Sy+9NI888kjrMSZi586dOXDgQOsx4J9yoRmAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAMui6rlvUDQeDpZ4F/k/r16/Ptm3bWo8xEUeOHMlrr73WegxWqcXs7kUBYJVYzO5+6jTMsazt3r07559/fusxxvbyyy/nnnvuydq1a3PbbbflXe9a+WcGDx48mB//+Me55JJLcsMNN7QeZyLuu+++/O53v8uuXbty+eWXtx5nbPPz87n11lszMzOTG5Nc0HqgCfivJPtaD9FSt0hJevn161//erF/BcvaoUOHuiTdOeec083OzrYeZyLuuOOOLkl35ZVXth5lYj7zmc90Sbq77rqr9SgT8dZbb3Vnn312l6T7z6TrevB1ZBnsl5bqazFW/stJACZGFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKBMtR6gtdFolNnZ2dZjjG00Gr3j+67rGk4zGfPz80mSrut68Rglqcdlfn6+F9v0jn93SVb+Fi1sx2o26Ba59xgMBks9SxNnnXVWhsNh6zHGNjc3l1OnTiVJzj777MbTTMbMzExmZmYyNTWV97znPa3HmYg333wzo9Eoa9asyZo1a1qPMxEnT55MkpyVZOU/k5K5JKdaD7FEFrO7X/VRAFgtFrO7X/Wnjx588MFcfPHFrccY2/PPP5/rrrsu69aty09/+tNeHP3cf//9ueeee/LJT34y+/btaz3ORHzlK1/Jr371q3z1q1/NF77whdbjjG00GuWKK67IqVOn8sADD2TTpk2tRxrb0aNHc+2117Yeo5lVH4VNmzZly5YtrccY29zcXJJkOBxm69atmZpa+Q/tY489lmThdNi2bdsaTzMZb5/au+CCC3qxTbOzs/UCZNOmTb3YptV+VsS7jwAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBlqvUArb3wwguZm5trPcbYXnzxxSTJ3Nxcnn322QyHw8YTje/VV19Nkpw8eTLPPPNM42km4+TJk0kWtq0P2zQajer5c/To0QwGg8YTje/o0aOtR2hq0HVdt6gb9uDBBljNFrO7d/oIgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoU4u9Ydd1SzkHAMuAIwUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAyn8DI8S7bXAPc/kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Reward: 0\n",
      "Observation: [2 4]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 3, Reward: 0\n",
      "Observation: [2 3]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Reward: 0\n",
      "Observation: [2 4]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 3, Reward: 0\n",
      "Observation: [2 3]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 3, Reward: 0\n",
      "Observation: [2 2]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 3, Reward: 0\n",
      "Observation: [2 1]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAANpklEQVR4nO3db6jedf3H8de1c2yjeeYgRYcUJCkNwpH/iDSGCbnYjbQiyVbJulWgP4dFkZBGhBNvyFC8ZYkYDIxuWGR/FjGDsmlrUepmUAbaGCbk5rZ2Otd1rt+Ni94VlbvmdZ19tut6PODAdeN7zvf9Pdc53+f1/XOd0+n3+/0AQJJlrQcA4NQhCgAUUQCgiAIARRQAKKIAQBEFAIooAFBmh12w0+ks5RwALLFh3qvsSAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACizrQdo7dFHH81FF13UeoyRPfPMM9m0aVPm5uayc+fOzMzMtB5pZA899FC2bduWq666Kvfff3/rccbic5/7XH7xi1/k1ltvzU033dR6nJF1u92sX78+R44cyfYka1sPNAZ7k3y89RANTX0ULrrooqxbt671GCObn59PkszMzOTiiy/O7Ozp/9Sed955SZK5ubmJeI6SwbYkyZo1ayZimxYWFuoFyIVJTv8tSnqtB2jM6SMAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAmW09QGvPPPNM5ufnW48xsueeey5J0u128/TTT2dmZqbxRKN76aWXkiQHDx7MU0891Xia8Th48GCS5MUXX5yIbep2u+n1ekmSZ5P02o4zFs+3HqCxTr/f7w+1YKez1LMAsISG2d1P/ZHC3NzcRLyq7na7OXz4cDqdTs4666zW44zFsWPHcuzYsczOzubMM89sPc5YHD58ON1uNytWrMiKFStajzMWr776apLJ+V3q9Xp57bXXWo/RzNQfKezevTsXX3xx6zFG9vTTT+e9731vVq9enQMHDkzEL+c999yTL3/5y9mwYUO+973vtR5nLDZu3Jgf//jH2bp1a2677bbW44ys2+3m3HPPzaFDh7Jr165ccsklrUca2Z49e3LFFVe0HmNJOFIYwszMTGZnT/9vw79GYFK26R8vRDqdzkRsTzJ52/SvO5lJ+bmbhBdUo3D3EQBFFAAoogBAEQUAiigAUE7/WwVOEYuLyYEDg8cvv5zceON/LnP11cnttw8er1yZTMjbCYAJIgpj8LvfJT//eXLzzUm3+7+X27s3eeCBweMNG5KPfzz5xCeSKb8DDjiFiMIIer3ks59Ndu1KfvvbE/vcH/4w2bEj+e53k5tuSjZuTCb0/YHAacQ1hTdgYSH51a8Gp4gefPDEg/APvV7yne8kN9yQPPZY8qc/jXVMgBMmCm/AAw8kl1+ePPpoMtwfCXl9R48m11+ffPSjg1NMAK2IwgnodpNt2/55sXjcdu9OPvKRf16wBjjZRGFIi4vJvfcmW7YkR44s3Xr27h3cpeSIAWhBFIbU7SZf+9p4Thcdz7594zs1BXAiRGEIr76afPKTyeHDJ2+d996b/OAHwgCcXKIwhO3bT/4r94MHk02bBqetAE4WUTiOAweS++9vs+7Dh5OtW9usG5hOonAcTz6ZPPdcm3UvLAze4LZ/f5v1A9NHFI7j859vu/4nnkgef7ztDMD0EAUAiii8jl/+MnnttdZTDE5hHT3aegpgGojC6/jWt5K//KX1FMk3v5n89a+tpwCmgSgAUEQBgCIKABRRAKCIwuu44Ybk7LNbTzH4cxerV7eeApgGovA63ve+ZNWq1lMk69cnK1e2ngKYBqIAQBGF4/j619uu/6qrkg98oO0MwPQQheNYvz658MI2656dTd7//uRtb2uzfmD6iMJxrFkz+BecLczNJV/5Spt1A9NJFIbwsY8lH/pQ0umcvHWuXJl84xvJMs8QcBLZ5QzhLW8Z/Oe1M888eev8wheS6647uSECEIUhzc4OdtQnwzvekXz4w4IAnHyzrQc4XSxblnzpS8mb3pR89avJ3/62NOu54ILkpz9N3vrWpfn6AK/HkcIJOOOM5ItfTO68c2m+/rp1yWOPCQLQjii8AVu2JD/72fguPi9fnmzfnnz728m73jX61wN4o5w+egPOOGPwJzDe855k8+Zk9+5k794T/zozM8m11yaf+Uxy/fWuIQDticIIzjgjeeSRZM+eZOfOwYXoXm+4z73mmsGtrps3Dy5iA5wK7I7G4N3vHlwPuO66pN9PXn45ufHG/1zu6quT228fPD7rrMGtrgCnElEYk2XLkre/ffD4gguSP/zhvy/nFBFwKhOFJWLnD5yO3H0EQBEFAIooAFBEAYAiCgAUUQCgTP0tqQ899FDOO++81mOM7KWXXkqSHDt2LPfcc086E3BP7BNPPJEk+eMf/5itW7c2nmY8XnjhhSTJzp07s7i42Hia0S0uLmZ+fj5J8vDDD2fHjh2NJxrd/v37W4/QVKff7/eHWnACdjIA02yY3f3UHylw6jvnnHNy+eWXtx5jLJ566qm88sorrceA/0kUOOVddtll+f73v996jLHYsGFDfvSjH7UeA/4nF5oBKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKJ1+v98fasFOZ6lngf/q7LPPzqWXXtp6jLHYvXt3XnnlldZjMKWG2d2LAsCUGGZ3P3sS5jilbdmyJWvWrGk9xshefPHF3HfffVmxYkXuvPPOLFt2+p8Z3LlzZx5//PG8853vzObNm1uPMxYPPvhgfv/732fjxo1Zv35963FGtri4mDvuuCPz8/P5vyTntx5oDP6cZFvrIVrqDynJRH785je/GfZbcErbtWtXP0l/9erV/YWFhdbjjMVdd93VT9L/4Ac/2HqUsbn22mv7Sfp3331361HG4u9//3t/1apV/ST9XyX9/gR87D4F9ktL9TGM0//lJABjIwoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQZlsP0Fq3283CwkLrMUbW7Xb/7XG/3284zXgsLi4mSfr9/kQ8R0nqeVlcXJyIbfq3n7skp/8WDbZjmnX6Q+49Op3OUs/SxMqVKzMzM9N6jJH1er0cOXIkSbJq1arG04zH/Px85ufnMzs7mze/+c2txxmLo0ePptvtZvny5Vm+fHnrccbi0KFDSZKVSU7/36Skl+RI6yGWyDC7+6mPAsC0GGZ3P/Wnj7Zv354LL7yw9Rgje/bZZ/PpT386c3Nz+clPfjIRRz8PP/xw7rvvvlx55ZXZtm1b63HG4uabb86TTz6ZW265JZ/61KdajzOybreba665JkeOHMkjjzyStWvXth5pZPv27cumTZtaj9HM1Edh7dq1WbduXesxRtbr9ZIkMzMzueSSSzI7e/o/tTt27EgyOB126aWXNp5mPP5xau/888+fiG1aWFioFyBr166diG2a9rMi7j4CoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAlNnWA7S2d+/e9Hq91mOM7Pnnn0+S9Hq97NmzJzMzM40nGt3+/fuTJIcOHcqvf/3rxtOMx6FDh5IMtm0Stqnb7dbvz759+9LpdBpPNLp9+/a1HqGpTr/f7w+14AQ82QDTbJjdvdNHABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAmR12wX6/v5RzAHAKcKQAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQPl/XZzCldSyLccAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Reward: 0\n",
      "Observation: [2 1]\n",
      "Info: {}\n",
      "Pixels shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check GridWorldEnv\n",
    "env = gym.make(\"GridWorldEnv-v0\", render_mode=\"rgb_array\", grid_file='custom_envs/grid_envs/grid_world2.txt')\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "print(\"Initial Observation:\", observation)\n",
    "print(\"Initial Info:\", info)\n",
    "\n",
    "# Step through the environment\n",
    "\n",
    "for _ in range(10):\n",
    "    # Random action\n",
    "    action = np.random.randint(env.action_space.n)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Observation: {observation}\")\n",
    "    print(f\"Info: {info}\")\n",
    "\n",
    "    # Render the environment\n",
    "    pixels = env.render()\n",
    "    print(f\"Pixels shape: {pixels.shape}\")\n",
    "    plt.imshow(pixels)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Action 0: Down\n",
    "# Action 1: Right\n",
    "# Action 2: Up\n",
    "# Action 3: Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import GymWrapper\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"GridWorldEnv-v0\", render_mode = \"rgb_array\", grid_file='custom_envs/grid_envs/grid_world2.txt')\n",
    "env = GymWrapper(env, from_pixels=True, pixels_only=False, device=\"cpu\")\n",
    "print(env.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = env.rollout(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 512, 512, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 512, 512, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: tensor([4, 2], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([0, 0, 0, 1])\n",
      "Agent: tensor([4, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([1, 0, 0, 0])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([1, 0, 0, 0])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([0, 1, 0, 0])\n",
      "Agent: tensor([5, 2], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAANhElEQVR4nO3dXYyUhb3H8d90N2FFQWNBIZY0tpHIDSS+pb6kNjVVFE3rVdOEqDWmF76htVfe6J0ofQnRXjSxJcS2Wi80jVGicLFeqFUO6oUKmlSbwCG2JUQQ7K7M7pyLSf9HztF26czwLDufT7JxSYZ9fg/Lznee2VlpdTqdTgAgyZeaHgDA7CEKABRRAKCIAgBFFAAoogBAEQUAiigAUEZnesNWqzXIHQAM2Ex+VtmVAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEabHtC0J598MsuXL296Rs/eeuutrF27NgsWLMj4+HhGRkaantSzTZs2ZePGjbnsssvyyCOPND2nL2699da8/PLLueuuu3LTTTc1Padn7XY7l19+eQ4fPpzHk6xoelAf7Ezyg6ZHNGjoo7B8+fKsWrWq6Rk9m5ycTJKMjIxk5cqVGR098T+1S5YsSZIsWLBgTnyOku65JMnSpUvnxDkdOXKkHoCck+TEP6NkqukBDfP0EQBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCMNj2gaW+99VYmJyebntGzd955J0nSbrezffv2jIyMNLyod3v27EmSHDhwIK+99lrDa/rjwIEDSZLdu3fPiXNqt9uZmppKkrydZKrZOX3xbtMDGtbqdDqdGd2w1Rr0FgAGaCZ390N/pbBgwYI58ai63W7n0KFDabVaOfXUU5ue0xcTExOZmJjI6OhoTjnllKbn9MWhQ4fSbrczNjaWsbGxpuf0xUcffZRk7nwtTU1N5eOPP256RmOG/kphx44dWblyZdMzerZ9+/ZccsklOe200/Lhhx/OiS/ODRs25N57783q1avzzDPPND2nL9asWZMXXngh69evzz333NP0nJ612+2ceeaZOXjwYF599dWcd955TU/q2RtvvJGLLrqo6RkD4UphBkZGRjI6euL/MXw2AnPlnP75QKTVas2J80nm3jl99k5mrvy9mwsPqHrh1UcAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAMtr0gKZt2rQpS5YsaXpGz/bs2ZMkmZiYyIYNG9JqtRpe1LsXX3wxSfL+++9n/fr1Da/pjw8++CBJMj4+nunp6YbX9G56ejqTk5NJks2bN2fr1q0NL+rd3r17m57QqFan0+nM6IZz4E4GYJjN5O5+6K8UmP0WL16cCy+8sOkZffHaa69l3759Tc+ALyQKzHoXXHBBnn322aZn9MXq1avz/PPPNz0DvpBvNANQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQWp1OpzOjG7Zag94Cn2vRokU5//zzm57RFzt27Mi+ffuansGQmsndvSgADImZ3N2PHocds9rdd9+dpUuXNj2jZ7t3787DDz+csbGx3H///fnSl078ZwbHx8fz3HPP5dxzz83NN9/c9Jy+ePTRR/Pee+9lzZo1ufzyy5ue07Pp6encd999mZyczLokZzU9qA/+O8nGpkc0qTNDSebk25tvvjnTP4JZ7dVXX+0k6Zx22mmdI0eOND2nLx544IFOks7VV1/d9JS+ueqqqzpJOg8++GDTU/ri008/7SxcuLCTpPNfSaczB952zIL7pUG9zcSJ/3ASgL4RBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACiiAEARBQCKKABQRAGAIgoAFFEAoIgCAEUUACijTQ9oWrvdzpEjR5qe0bN2u33U+51Op8E1/TE9PZ0k6XQ6c+JzlKQ+L9PT03PinI76e5fkxD+j7nkMs1ZnhvcerVZr0FsacfLJJ2dkZKTpGT2bmprK4cOHkyQLFy5seE1/TE5OZnJyMqOjo5k/f37Tc/rik08+Sbvdzrx58zJv3rym5/TFwYMHkyQnJznxv5KSqSSHmx4xIDO5ux/6KAAMi5nc3Q/900ePP/54zjnnnKZn9Oztt9/OjTfemAULFmTbtm1z4upn8+bNefjhh3PppZdm48aNTc/pizvuuCOvvPJK7rzzztxwww1Nz+lZu93OFVdckcOHD+exxx7LihUrmp7Us127dmXt2rVNz2jM0EdhxYoVWbVqVdMzejY1NZUkGRkZyXnnnZfR0RP/U7t169Yk3afDzj///IbX9Mc/n9o766yz5sQ5HTlypB6ArFixYk6c07A/K+LVRwAUUQCgiAIARRQAKKIAQBEFAMqJ/7pFBmZiIvnb37rvv/BC8tBDSauVdDr/+9+77kquvbZ7m8WLk5NOamwu0AeiwOfasiXZti35+c//9e1uu637liS3355ceWVy3XWD3wcMhihwlD17knXrkpdeSv7612P7vY88kjzxRPLNbyYbNiRnn929ogBOHKJAkmT//uS995K1a5M///k//zj79iVPPZVs35784Q/JOeckixb1bycwWL7RTCYmkltvTS6+uLcgfNbu3ckllyQ/+lFy6FB/PiYweKIw5P7xj+SWW7qP6gfh6aeTG25IPvlkMB8f6C9RGGL79yc//GHyu98N9jhPP919Wurvfx/scYDeicKQ6nSS998f3BXC//X008nOnd3jArOXKAypPXuS73//+B5z7drkL385vscEjo0oDKmf/KR7pXA87d6d3Hnn8T0mcGxEYQg9/3zy4ovNHPtPf0r++Mdmjg38e6IwZCYmuj+pfKw/mNYv+/YlW7d6NRLMVqIwZPbvT37602Y3/PKXyYcfNrsB+HyiAEARhSGzZUvTC7q2bPHyVJiNRGHI/OxnTS/omi07gKOJAgBFFAAoogBAEQUAiigMmR//uOkFXbNlB3A0URgy11zT9IKua67xT3XCbCQKABRRGDKnn57cc0+zG267LVmypNkNwOcThSEzNpZ85zvJGWc0c/xFi7rHnz+/meMD/5ooDKGrrkq+9a1mjv2NbyTf/W4zxwb+PVEYUhs2JF/72vE95rJlycaNx/eYwLERhSG1bFnyxBPH95i//W1y9tnH95jAsRGFIdVqJV//+vH7d5qvvz5ZscLLUGG2G216AM05/fTkN79JRkaS3/9+cMf53ve6Vwm+uQyznyuFITd/fvLrXw/uiuH665PHHhMEOFGIAhkb6/4TmS+/3L9vPn/lK8lLLyW/+lVyyin9+ZjA4IkCSZIvfzm5+OJkfLz76P7MM/+zj7NoUff3j493P97ixf1cCQya7ylwlGXLkqeeSp57Ltm2LfnFL2b+e2+/PbnyyuS66wa3DxgsUeBzXXNN8u1vJ+vWdX+9dWvy0EP//3br1iXXXtt9/4wzkpNOOn4bgf4TBb7Q2Fjy1a9237/llu4bMLf5ngIARRQAKKIAQBEFAIooAFBEAYAy9C9J3blzZ6amppqe0bN33303STI1NZU33ngjIyMjDS/q3d69e5MkBw8ezOuvv97wmv44ePBgku65zYVzarfb9fWza9eutObA/wZ3165dTU9oVKvT6XRmdMM58MkGGGYzubv39BEARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBGZ3rDTqczyB0AzAKuFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAo/wNeRa6fUl2b2wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([0, 0, 0, 1])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([1, 0, 0, 0])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([1, 0, 0, 0])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([0, 0, 1, 0])\n",
      "Agent: tensor([4, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN4klEQVR4nO3da4xcBf3H4e8wW7sCxSoIRUyILyCtaJuUFo2aYOKthGAiLwxoQ4nERDFeSF9ZYiTxBZgSYwMvTNRgUw3xipfUqGisbzDQf20TixTjJaGl8VID1LZ02Zk9/xcjP/FGp53ZPe3u8ySTTLOTOb+z0z2fOWfOzHSapmkCAEnOansAAE4fogBAEQUAiigAUEQBgCIKABRRAKCIAgBlYtgbdjqd2ZwDgFk2zHuV7SkAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBlou0B2vaNb3wjl19+edtjjGzv3r1Zv359lixZkh07dqTb7bY90sjuu+++bNmyJW95y1ty7733tj3OWNx666156KGH8olPfCI333xz2+OMrNfr5eqrr87Ro0dzf5IVbQ80Bo8lubHtIVq04KNw+eWXZ9WqVW2PMbKpqakkSbfbzcqVKzMxceY/tMuWLUuSLFmyZF48RslgXZLk4osvnhfrND09XU9ALkty5q9R0m97gJY5fARAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoE20P0La9e/dmamqq7TFG9pvf/CZJ0uv1snPnznS73ZYnGt2BAweSJM8880weeeSRlqcZj2eeeSZJsn///nmxTr1eL/1+P0nyaJJ+u+OMxeNtD9CyTtM0zVA37HRmexYAZtEwm/sFv6ewZMmSefGsutfr5ciRI+l0OnnZy17W9jhjcfz48Rw/fjwTExM599xz2x5nLI4cOZJer5fJyclMTk62Pc5YPP3000nmz99Sv9/P3//+97bHaM2C31PYtWtXVq5c2fYYI9u5c2fe9KY3ZenSpfnTn/40L/44N2/enE2bNmXdunX5wQ9+0PY4Y3HttdfmJz/5Se66665s3Lix7XFG1uv1ctFFF+Xw4cN5+OGHs3r16rZHGtnu3btz1VVXtT3GrLCnMIRut5uJiTP/1/DCCMyXdXr+iUin05kX65PMv3V64UZmvvy/mw9PqEbh7CMAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAmWh7gLbdd999WbZsWdtjjOzAgQNJkuPHj2fz5s3pdDotTzS6X/ziF0mSP/zhD7nrrrtanmY8/vjHPyZJduzYkZmZmZanGd3MzEympqaSJFu3bs2DDz7Y8kSjO3jwYNsjtKrTNE0z1A3nwUYGYCEbZnO/4PcUOP298pWvzNq1a9seYyweeeSRHDp0qO0x4H8SBU57a9asyfbt29seYyzWrVuXH//4x22PAf+TF5oBKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKJ2maZqhbtjpzPYs8F9dcMEFufLKK9seYyx27dqVQ4cOtT0GC9Qwm3tRAFgghtncT8zBHKe12267LRdffHHbY4xs//79ueeeezI5OZk77rgjZ5115h8Z3LFjR374wx9m+fLl+cAHPtD2OGPxpS99Kb/97W9z7bXX5uqrr257nJHNzMzk05/+dKampvLxJJe0PdAYPJlkS9tDtKkZUpJ5edmzZ8+wv4LT2sMPP9wkaZYuXdpMT0+3Pc5Y3HnnnU2S5pprrml7lLF517ve1SRpPvvZz7Y9ylg899xzzXnnndckaf4vaZp5cNl1GmyXZusyjDP/6SQAYyMKABRRAKCIAgBFFAAoogBAWfDvUxin598X8tRTyaZN//nzNWuSW27557+9HxA43YjCGDz1VHLgQPKe9yTT00m/nzz55H/ebtu25DOfGVx/73uTD30oec1rknnwPjNgnrA5GkHTJF/+cnLzzcnKlcnvf5888cR/D0KSHDs2+PkTTyR3352sWJF8/vPJnj1zODTAi7CncAqaJnnuuWTLluRTnxpcPxXT08nGjYM4bN+evPrVyaJF450V4GTYUzgFP/95csEFye23n3oQXuixx5IrrkhuuCH5619Hvz+AU2VP4ST96EfJhg3JkSPjvd9nn02+853BXsjXvpa89KXjvX+AYdhTGFLTJD/7WXLTTclf/jJ7y3nggeT97x9/dACGIQpDmp5Orr9+bg7vPPBA8rnPzf5yAP6dKAyhaQZnCR07NnfL/Na3kt/97p/vfQCYC6IwhK1bB2cZ9Xpzt8xf/zp5xzuSmZm5WyaAKJzA008PDueM4yyjk/XnPyff/e7cLxdYuEThBJ58Mvn+99tZ9rPPJl/9ajtBAhYmUTiB669vd/nf+94gDABzQRROYC5fR/hvmsbrCsDcEYUXcejQ4FTUth061H6cgIVBFF7EHXck+/e3PUXyyU8OXnQGmG2iAEARBQCKKABQRAGAIgovYu3a5Nxz254ieeMbfZQ2MDdE4UVs2JBceGHbUyS33JK84hVtTwEsBKIAQBGFE7jhhnaXv3x5smpVuzMAC4conMAHP5gsXtze8teuHVwA5oIonMCllyZ3393Osl/+8uSLX2xn2cDCJAon0Okkb31r8vrXz/2yP/KRZNGiuV8usHCJwhBe97pk+/bknHPmbpkf/3hy++3JWR4hYA7Z5AzpVa9KrrtubpZ14YWDr+KcnJyb5QE8b6LtAc4U3W5y772D7zf4+tdnbzmLFiVf+UpyzTWztwyA/8Wewkk4//xk69bkfe+bnfu/6KLBV3+uWzc79w9wIvYUTtLixYMzgq64IvnmN5M9e8Zzv7fdlrz97YIAtEsUTsHZZyebNg32GN75zuTgweTo0VO7r/PPTz784cGLyl5DANomCiO49NJk377BmUnbtiXf/vbw36e8fPngg+6+8IXkJS8ZnPoK0DZRGEGnM7hcd93gsM+73z2Iwt/+lmzcOHhR+oXe8Ibk1lsH11/72mTNmrmfGeDFiMKYLFqUrF8/uN7vJzfe+J+3mZxMli6d07EATooozIJuN1m2rO0pAE6eU1IBKKIAQBEFAIooAFBEAYCy4M8+6vV6mZ6ebnuMkfV6vX+53vz7myTOQDP/eCdg0zTz4jFKUo/LzMzMvFinf/l/l+TMX6PBeixknWbIrUdnnr7l9pxzzkm32217jJH1+/0c/cdnbZx33nktTzMeU1NTmZqaysTERM4+++y2xxmLY8eOpdfrZfHixVnc5ve8jtHhw4eTJOckOfP/kpJ+klP81JrT3jCb+wUfBYCFYpjN/YI/fHT//ffnsssua3uMkT366KPZsGFDlixZkp/+9KfzYu9n69atueeee/LmN785W7ZsaXucsfjoRz+aX/7yl/nYxz6Wm266qe1xRtbr9fK2t70tR48ezbZt27JixYq2RxrZvn37sv75jydYgBZ8FFasWJFVq1a1PcbI+v1+kqTb7Wb16tWZmDjzH9oHH3wwyeBw2JVXXtnyNOPx/KG9Sy65ZF6s0/T0dD0BWbFixbxYp4V+VMTZRwAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAiCgAUUQCgiAIARRQAKKIAQBEFAIooAFBEAYAy0fYAbXvsscfS7/fbHmNkjz/+eJKk3+9n9+7d6Xa7LU80uoMHDyZJDh8+nF/96lctTzMehw8fTjJYt/mwTr1er/5+9u3bl06n0/JEo9u3b1/bI7Sq0zRNM9QN58GDDbCQDbO5d/gIgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAiigAUEQBgCIKABRRAKCIAgBFFAAoE8PesGma2ZwDgNOAPQUAiigAUEQBgCIKABRRAKCIAgBFFAAoogBAEQUAyv8Dj03XoROaRUcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([1, 0, 0, 0])\n",
      "Agent: tensor([5, 1], dtype=torch.int32)\n",
      "Done main: tensor([False])\n",
      "Done info: tensor([False])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Plot the first pixels image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Agent: {data['observation'][i]}\")\n",
    "    print(f\"Done main: {data['done'][i]}\")\n",
    "    print(f\"Done info: {data['next', 'done'][i]}\")\n",
    "    plt.imshow(data[\"pixels\"][i])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(f\"Action: {data['action'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data['next','pixels'][-1].numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# convert a numpy to a gray scale image\n",
    "# with PIL\n",
    "from PIL import Image\n",
    "import numpy\n",
    "\n",
    "# Create a numpy array\n",
    "\n",
    "# Create a numpy array\n",
    "numpy_array = data['next','pixels'][-1].numpy()\n",
    "\n",
    "# Create a PIL Image from the numpy array\n",
    "image = Image.fromarray(numpy_array)\n",
    "\n",
    "# Convert the PIL image to a gray scale image\n",
    "gray_image = image.convert('L')\n",
    "\n",
    "# Display the gray scale image\n",
    "plt.imshow(gray_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        agent: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                agent: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 512, 512, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                target: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 512, 512, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "        target: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'key \"observation\" not found in TensorDict with keys [\\'action\\', \\'agent\\', \\'done\\', \\'next\\', \\'pixels\\', \\'target\\', \\'terminated\\', \\'truncated\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/tensordict/base.py:323\u001b[0m, in \u001b[0;36mTensorDictBase.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    321\u001b[0m idx_unravel \u001b[38;5;241m=\u001b[39m _unravel_key_to_tuple(index)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx_unravel:\n\u001b[0;32m--> 323\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_unravel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNO_DEFAULT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_non_tensor(result):\n\u001b[1;32m    325\u001b[0m         result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, NO_DEFAULT)\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/tensordict/_td.py:2078\u001b[0m, in \u001b[0;36mTensorDict._get_tuple\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default):\n\u001b[0;32m-> 2078\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m default:\n\u001b[1;32m   2080\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m first\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/tensordict/_td.py:2074\u001b[0m, in \u001b[0;36mTensorDict._get_str\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   2072\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensordict\u001b[38;5;241m.\u001b[39mget(first_key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/final-project/lib/python3.11/site-packages/tensordict/base.py:2915\u001b[0m, in \u001b[0;36mTensorDictBase._default_get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   2912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2914\u001b[0m     \u001b[38;5;66;03m# raise KeyError\u001b[39;00m\n\u001b[0;32m-> 2915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   2916\u001b[0m         _KEY_ERROR\u001b[38;5;241m.\u001b[39mformat(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m   2917\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key \"observation\" not found in TensorDict with keys [\\'action\\', \\'agent\\', \\'done\\', \\'next\\', \\'pixels\\', \\'target\\', \\'terminated\\', \\'truncated\\']'"
     ]
    }
   ],
   "source": [
    "data[\"observation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([4]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs.utils import RandomPolicy\n",
    "    \n",
    "policy = RandomPolicy(env.action_spec)\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=policy,\n",
    "    frames_per_batch=4,\n",
    "    total_frames=16000,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1,\n",
    "    init_random_frames=1000,\n",
    ")\n",
    "\n",
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([4]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([4]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(torch.nn.Module):\n",
    "    \"\"\"The convolutional network used to compute the agent's Q-values.\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 num_outputs,\n",
    "                 num_cells_cnn, \n",
    "                 kernel_sizes, \n",
    "                 strides, \n",
    "                 num_cells_mlp,\n",
    "                 activation_class,\n",
    "                 use_batch_norm=False):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        self.activation_class = activation_class()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "      \n",
    "        # Input shape example: (10, 4, 84, 84)\n",
    "        _, channels, width, height = input_shape\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Xavier (Glorot) uniform initialization\n",
    "        self.initializer = torch.nn.init.xavier_uniform_\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.batch_norm_layers = torch.nn.ModuleList()\n",
    "        in_channels = channels\n",
    "        for out_channels, kernel_size, stride in zip(num_cells_cnn, kernel_sizes, strides):\n",
    "            conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            if self.use_batch_norm:\n",
    "                batch_norm_layer = torch.nn.BatchNorm2d(out_channels)\n",
    "                self.batch_norm_layers.append(batch_norm_layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride  + 1\n",
    "        \n",
    "        # Compute the output shape of the conv layers\n",
    "        width_output = width\n",
    "        height_output = height\n",
    "        for kernel_size, stride in zip(kernel_sizes, strides):\n",
    "            width_output = conv2d_size_out(width_output, kernel_size, stride)\n",
    "            height_output = conv2d_size_out(height_output, kernel_size, stride)\n",
    "\n",
    "        cnn_output = width_output * height_output * num_cells_cnn[-1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        input_size = cnn_output\n",
    "\n",
    "        if len(num_cells_mlp) != 0:\n",
    "            self.fc_layers = torch.nn.ModuleList()\n",
    "            for units in num_cells_mlp:\n",
    "                fc_layer = torch.nn.Linear(input_size, units)\n",
    "                self.fc_layers.append(fc_layer)\n",
    "                input_size = units\n",
    "        else:\n",
    "            self.fc_layers = None\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = torch.nn.Linear(input_size, self.num_outputs)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.conv_layers:\n",
    "            self.initializer(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        if self.fc_layers is not None:\n",
    "            for layer in self.fc_layers:\n",
    "                self.initializer(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.zeros_(layer.bias)\n",
    "        self.initializer(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            torch.nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.float() / 255.0 # Already normalized by VecNorm\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            if self.use_batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "            x = self.activation_class(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        if self.fc_layers is not None:\n",
    "            for fc_layer in self.fc_layers:\n",
    "                x = self.activation_class(fc_layer(x))\n",
    "        q_values = self.output_layer(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICODQNNetwork(torch.nn.Module):\n",
    "    \"\"\"The convolutional network used to compute the agent's Q-values.\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 num_outputs,\n",
    "                 num_cells_cnn, \n",
    "                 kernel_sizes, \n",
    "                 strides, \n",
    "                 num_cells_mlp,\n",
    "                 activation_class,\n",
    "                 use_batch_norm=False):\n",
    "        super(MICODQNNetwork, self).__init__()\n",
    "\n",
    "        self.activation_class = activation_class()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "      \n",
    "        # Input shape example: (4, 84, 84)\n",
    "        channels, width, height = input_shape\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Xavier (Glorot) uniform initialization\n",
    "        self.initializer = torch.nn.init.xavier_uniform_\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.batch_norm_layers = torch.nn.ModuleList()\n",
    "        in_channels = channels\n",
    "        for out_channels, kernel_size, stride in zip(num_cells_cnn, kernel_sizes, strides):\n",
    "            conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            if self.use_batch_norm:\n",
    "                batch_norm_layer = torch.nn.BatchNorm2d(out_channels)\n",
    "                self.batch_norm_layers.append(batch_norm_layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride  + 1\n",
    "        \n",
    "        # Compute the output shape of the conv layers\n",
    "        width_output = width\n",
    "        height_output = height\n",
    "        for kernel_size, stride in zip(kernel_sizes, strides):\n",
    "            width_output = conv2d_size_out(width_output, kernel_size, stride)\n",
    "            height_output = conv2d_size_out(height_output, kernel_size, stride)\n",
    "\n",
    "        cnn_output = width_output * height_output * num_cells_cnn[-1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        input_size = cnn_output\n",
    "\n",
    "        if len(num_cells_mlp) != 0:\n",
    "            self.fc_layers = torch.nn.ModuleList()\n",
    "            for units in num_cells_mlp:\n",
    "                fc_layer = torch.nn.Linear(input_size, units)\n",
    "                self.fc_layers.append(fc_layer)\n",
    "                input_size = units\n",
    "        else:\n",
    "            self.fc_layers = None\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = torch.nn.Linear(input_size, self.num_outputs)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.conv_layers:\n",
    "            self.initializer(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "        if self.fc_layers is not None:\n",
    "            for layer in self.fc_layers:\n",
    "                self.initializer(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.zeros_(layer.bias)\n",
    "        self.initializer(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            torch.nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "\n",
    "            # NOTE: The collector uses a tensor for checking something\n",
    "            # but this tensor is not in batch format, so we need to\n",
    "            # check if the tensor is in batch format to apply batch norm\n",
    "            if self.use_batch_norm and len(input.shape) == 4:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "            x = self.activation_class(x)\n",
    "\n",
    "        if len(input.shape) == 4:\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        else:\n",
    "            x = x.view(-1)\n",
    "        \n",
    "        representation = x\n",
    "\n",
    "        if self.fc_layers is not None:\n",
    "            for fc_layer in self.fc_layers:\n",
    "                x = self.activation_class(fc_layer(x))\n",
    "        q_values = self.output_layer(x)\n",
    "        return q_values, representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the network with a random input\n",
    "tensor = torch.rand(10, 4, 84, 84)\n",
    "num_actions = 2\n",
    "num_cells_cnn = [32, 64, 64]\n",
    "kernel_sizes = [8, 4, 3]\n",
    "strides = [4, 2, 1]\n",
    "num_cells_mlp = [512]\n",
    "activation_class = torch.nn.ReLU\n",
    "\n",
    "network = MICODQNNetwork(\n",
    "    input_shape=tensor.shape[-3:],\n",
    "    num_outputs=num_actions,\n",
    "    num_cells_cnn=num_cells_cnn,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    strides=strides,\n",
    "    num_cells_mlp=num_cells_mlp,\n",
    "    activation_class=activation_class,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "q_values, representation = network(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MICODQNNetwork(\n",
       "  (activation_class): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (batch_norm_layers): ModuleList(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1-2): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dqn_modules(proof_environment, policy_cfg):\n",
    "\n",
    "    # Define input shape\n",
    "    input_shape = proof_environment.observation_spec[\"pixels\"].shape\n",
    "    env_specs = proof_environment.specs\n",
    "\n",
    "    # NOTE: I think I can change the next two lines by\n",
    "    # num_outputs = proof_environment.action_spec.shape[-1]\n",
    "    # action_spec = proof_environment.action_spec.space\n",
    "    num_outputs = env_specs[\"input_spec\", \"full_action_spec\", \"action\"].space.n\n",
    "    action_spec = env_specs[\"input_spec\", \"full_action_spec\", \"action\"]\n",
    "\n",
    "    # Define Q-Value Module and Representations (for MICO)\n",
    "\n",
    "    activation_class = torch.nn.ReLU\n",
    "\n",
    "    module = MICODQNNetwork(\n",
    "        input_shape=input_shape,\n",
    "        num_outputs=num_outputs,\n",
    "        num_cells_cnn=policy_cfg.cnn_net.num_cells,\n",
    "        kernel_sizes=policy_cfg.cnn_net.kernel_sizes,\n",
    "        strides=policy_cfg.cnn_net.strides,\n",
    "        num_cells_mlp=policy_cfg.mlp_net.num_cells,\n",
    "        activation_class=activation_class,\n",
    "        use_batch_norm=policy_cfg.use_batch_norm\n",
    "    )\n",
    "\n",
    "    module = TensorDictModule(module,\n",
    "            in_keys=[\"pixels\"], \n",
    "            out_keys=[\"action_value\", \"representation\"])\n",
    "\n",
    "    # NOTE: Do I need CompositeSpec here?\n",
    "    # I think I only need proof_environment.action_spec\n",
    "    qvalue_module = QValueActor(\n",
    "        module=module,\n",
    "        spec=CompositeSpec(action=action_spec),\n",
    "        in_keys=[\"pixels\"],\n",
    "    )\n",
    "\n",
    "    return qvalue_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': None, 'exp_name': 'DQN_pixels', 'env': {'env_name': 'CartPole-v1', 'seed': 118398}, 'collector': {'total_frames': 5000016, 'frames_per_batch': 16, 'eps_start': 1.0, 'eps_end': 0.05, 'annealing_frames': 200000, 'init_random_frames': 50000, 'frame_skip': 2}, 'policy': {'type': 'CNN_MLP', 'cnn_net': {'num_cells': [32, 64, 64], 'kernel_sizes': [8, 4, 3], 'strides': [4, 2, 1]}, 'mlp_net': {'num_cells': [512]}, 'activation': 'ReLU', 'use_batch_norm': False}, 'buffer': {'buffer_size': 50000, 'batch_size': 128, 'scratch_dir': None, 'prioritized_replay': False, 'alpha': 0.7, 'beta': 0.4}, 'logger': {'backend': 'wandb', 'project_name': 'dqn_pixels_cartpole', 'group_name': None, 'test_interval': 50000, 'num_test_episodes': 3, 'video': False}, 'optim': {'lr': 0.001, 'max_grad_norm': 10}, 'loss': {'gamma': 0.99, 'hard_update_freq': 50, 'num_updates': 1}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dqn_mico_er/config_cartpole.yaml\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with open(\"dqn_pixels/config_cartpole.yaml\") as f:\n",
    "    cfg = OmegaConf.create(yaml.safe_load(f))\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([4]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([4, 3136]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([4]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs.utils import RandomPolicy\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "\n",
    "module = make_dqn_modules(env, cfg.policy)\n",
    "\n",
    "greedy_module = EGreedyModule(\n",
    "    annealing_num_steps=1000,\n",
    "    eps_init=0.9,\n",
    "    eps_end=0.1,\n",
    "    spec=module.spec,\n",
    ")\n",
    "model_explore = TensorDictSequential(\n",
    "    module,\n",
    "    greedy_module,\n",
    ")\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=model_explore,\n",
    "    frames_per_batch=4,\n",
    "    total_frames=16000,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1,\n",
    ")\n",
    "\n",
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([4]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([4]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([4, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([4, 3136]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','pixels'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]],\n",
       "\n",
       "        [[1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05],\n",
       "         [1.7881e-05, 1.7881e-05, 1.7881e-05,  ..., 1.7881e-05,\n",
       "          1.1921e-05, 1.7881e-05],\n",
       "         ...,\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [5.9605e-06, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.1921e-05, 1.1921e-05],\n",
       "         [1.1921e-05, 1.1921e-05, 1.1921e-05,  ..., 1.1921e-05,\n",
       "          1.7881e-05, 1.1921e-05]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','pixels'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0704, 0.0711, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0824, 0.0610, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0814, 0.0059, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0010,  ..., 0.0142, 0.0028, 0.0000]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10, 3136]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape, representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the network with a random input\n",
    "tensor = torch.rand(10, 4, 84, 84)\n",
    "num_actions = 2\n",
    "num_cells_cnn = [64, 64, 32]\n",
    "kernel_sizes = [5, 5, 5]\n",
    "strides = [2, 2, 2]\n",
    "num_cells_mlp = []\n",
    "activation_class = torch.nn.ReLU\n",
    "\n",
    "network = DQNNetwork(\n",
    "    input_shape=tensor.shape,\n",
    "    num_outputs=num_actions,\n",
    "    num_cells_cnn=num_cells_cnn,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    strides=strides,\n",
    "    num_cells_mlp=num_cells_mlp,\n",
    "    activation_class=activation_class,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "q_values = network(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNNetwork(\n",
       "  (activation_class): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(4, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (2): Conv2d(64, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  )\n",
       "  (batch_norm_layers): ModuleList(\n",
       "    (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=1568, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10, 3136]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape, representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (0): LazyConv2d(0, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): SquashDims()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrl.modules import ConvNet\n",
    "\n",
    "cnn = ConvNet(\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    num_cells=[32, 64, 64],\n",
    "    kernel_sizes=[8, 4, 3],\n",
    "    strides=[4, 2, 1])\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the network with a toy input\n",
    "x = torch.randn(1, 4, 84, 84)\n",
    "y = cnn(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28224"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "84 * 84 * 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 2 3 4 5 6 7 8 9 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([3, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([3]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([3, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([3]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: a rollout will be take a trajectory of frame of 10 steps and will concat them by N = 4\n",
    "# so the output will be 7x4x84x84\n",
    "env.rollout(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedEnv(\n",
       "    env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu),\n",
       "    transform=StepCounter(keys=[]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TransformedEnv(GymEnv(\"CartPole-v1\"), StepCounter())\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # considering rollout['pixels'].shape equal torch.Size([10, 400, 600, 3]), plot the 10 images\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "# for i, ax in enumerate(axs):\n",
    "#     ax.imshow(rollout['pixels'][i])\n",
    "#     ax.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICOMLPNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 activation_class, \n",
    "                 encoder_out_features,\n",
    "                 mlp_out_features,\n",
    "                 encoder_num_cells = None,\n",
    "                 mlp_num_cells = None):\n",
    "        super(MICOMLPNetwork, self).__init__()\n",
    "\n",
    "        self.activation = activation_class()\n",
    "\n",
    "        if encoder_num_cells is None:\n",
    "            encoder_num_cells = []\n",
    "        layers_sizes = [in_features] + encoder_num_cells + [encoder_out_features]\n",
    "\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.encoder.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "\n",
    "        if mlp_num_cells is None:\n",
    "            mlp_num_cells = []\n",
    "\n",
    "        layers_sizes = [encoder_out_features] + mlp_num_cells + [mlp_out_features.item()]\n",
    "\n",
    "        self.q_net = torch.nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.q_net.append(torch.nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.encoder)):\n",
    "            x = self.activation(self.encoder[i](x))\n",
    "\n",
    "        representation = x\n",
    "\n",
    "        for i in range(len(self.q_net)-1):\n",
    "            x = self.activation(self.q_net[i](x))\n",
    "\n",
    "        return self.q_net[-1](x), representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MICOMLPNetwork(\n",
       "  (activation): ReLU()\n",
       "  (encoder): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       "  (q_net): ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_mlp = MICOMLPNetwork(\n",
    "    in_features=4,\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    encoder_out_features=3,\n",
    "    mlp_out_features=env.action_spec.shape[-1],\n",
    "    encoder_num_cells=[64],\n",
    "    mlp_num_cells=[64]\n",
    ")\n",
    "\n",
    "value_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4385e-01,  1.6361e+01,  1.6819e-01,  1.6592e+01],\n",
      "        [-2.4529e+00,  6.2587e+00,  2.1316e-01, -1.7105e+01],\n",
      "        [ 1.0673e+00,  6.0321e-01,  4.1905e-02, -6.5497e+00],\n",
      "        [-2.2201e+00, -5.3833e+00, -5.6525e-02, -2.0494e+01],\n",
      "        [ 2.4468e+00,  1.3530e-01,  8.5826e-02, -7.4412e+00],\n",
      "        [-1.0956e+00, -1.7736e+01, -2.9005e-01,  5.1246e+00],\n",
      "        [-8.3562e-02,  5.9217e+00, -3.8831e-01, -1.0923e+01],\n",
      "        [-9.6649e-02, -1.7004e+01,  1.8255e-01,  5.0339e+00],\n",
      "        [-1.9215e+00, -6.0012e+00, -3.3985e-01, -1.9400e+00],\n",
      "        [-7.9192e-01, -8.4512e+00, -1.1339e-01, -2.4514e+00],\n",
      "        [-3.8455e+00, -4.1582e+00, -6.5327e-02, -6.5401e+00],\n",
      "        [ 2.5188e+00, -2.6075e-01, -4.0146e-01, -4.9766e+00],\n",
      "        [ 3.4100e+00, -5.7687e+00, -1.3475e-01, -4.9081e+00],\n",
      "        [-3.0499e+00,  1.7682e+01,  2.6956e-01,  6.2869e+00],\n",
      "        [ 3.5047e+00, -1.3430e+01,  3.0075e-01, -9.5426e+00],\n",
      "        [ 3.7697e+00,  1.1834e+01,  1.7286e-01,  1.1398e+01],\n",
      "        [ 4.7634e+00, -1.6998e+01, -2.9006e-01,  9.7743e+00],\n",
      "        [ 1.2804e+00, -1.6293e+01, -1.0586e-01,  1.2830e+01],\n",
      "        [-1.3542e+00,  1.2150e+00,  3.0854e-01, -6.3442e-01],\n",
      "        [ 1.3793e+00, -1.2606e+00, -2.7460e-01, -7.9568e+00],\n",
      "        [-3.4166e+00,  4.0322e+00,  3.5936e-01,  7.5585e-01],\n",
      "        [ 1.6275e+00, -1.2598e+01, -2.7653e-01,  8.0609e+00],\n",
      "        [-2.8654e+00, -1.2266e+01,  3.2490e-01,  3.5293e+00],\n",
      "        [ 2.2782e+00, -1.9660e+01,  1.3434e-01,  6.5428e+00],\n",
      "        [-3.0847e+00, -5.8907e+00,  9.9763e-02, -1.0073e+01],\n",
      "        [-4.4755e+00, -6.6018e+00,  2.9518e-01, -7.1580e+00],\n",
      "        [-7.7387e-01, -5.6805e+00, -3.8991e-01,  2.2664e+01],\n",
      "        [ 4.1145e+00,  3.5917e+00,  1.1061e-01,  3.8655e+00],\n",
      "        [ 3.6731e+00, -3.3140e+00, -2.8392e-01, -7.2439e+00],\n",
      "        [ 3.3574e+00,  1.0757e+01,  1.1015e-01,  1.0531e+01],\n",
      "        [ 9.3583e-01,  3.1470e+00, -9.9956e-02, -3.7179e+00],\n",
      "        [ 2.6877e+00,  4.0512e+00,  1.3872e-01, -1.1658e+00],\n",
      "        [-3.6225e+00, -1.5832e+01,  1.2115e-01,  9.4262e-01],\n",
      "        [-3.4231e+00, -5.0136e-01, -1.6577e-01,  4.5416e+00],\n",
      "        [ 3.6540e-01,  2.4872e+01,  3.1235e-01, -7.4388e+00],\n",
      "        [ 4.1537e+00, -5.6400e+00,  2.7341e-01,  1.0245e+01],\n",
      "        [ 2.9621e+00,  4.7071e+00, -2.7539e-01, -3.1666e+00],\n",
      "        [ 3.9641e-01, -1.6041e+01, -2.6614e-01,  8.3124e+00],\n",
      "        [-2.6162e+00, -1.0861e+01, -4.0242e-01, -1.0668e+00],\n",
      "        [-3.5202e-01, -1.6549e+01,  2.0079e-01,  3.8322e-01],\n",
      "        [ 6.5740e-01, -9.0891e+00,  3.0693e-01,  1.8954e+01],\n",
      "        [ 1.9418e+00, -7.5173e+00, -2.6882e-01,  4.5016e+00],\n",
      "        [-2.6346e+00, -1.1080e+01, -1.6767e-01, -5.9871e-01],\n",
      "        [-4.6178e+00, -4.3420e+00,  2.9479e-01,  1.5252e+00],\n",
      "        [ 2.5610e+00, -1.9293e+00, -1.5271e-01,  6.9512e+00],\n",
      "        [ 3.6868e+00,  1.1232e+01, -4.0983e-01, -1.1306e+01],\n",
      "        [-1.9918e+00,  1.6648e+01, -2.2109e-01,  6.8823e-02],\n",
      "        [-2.2255e+00, -2.6932e+00,  1.3542e-01,  1.6596e+01],\n",
      "        [ 2.1815e+00,  2.1407e+01, -2.7794e-01, -2.0183e+01],\n",
      "        [ 1.2551e+00,  9.4587e+00, -5.1251e-02,  9.4529e+00],\n",
      "        [-3.3348e+00,  8.8511e+00, -3.1012e-01, -5.5034e+00],\n",
      "        [ 1.5573e+00,  6.4087e-01,  2.1983e-01,  7.8851e-01],\n",
      "        [ 1.6549e+00,  4.2646e+00, -2.0338e-01,  2.1678e+00],\n",
      "        [ 1.4192e+00, -3.4515e+00, -5.2174e-02, -7.3042e-02],\n",
      "        [ 3.0520e+00,  1.3845e-01,  3.0907e-01,  9.0296e+00],\n",
      "        [-3.1411e+00, -5.8337e+00,  1.2755e-01,  7.2418e+00],\n",
      "        [-2.3204e+00,  9.7580e+00,  2.0980e-03, -8.5886e-01],\n",
      "        [ 1.8602e+00,  1.3757e+01,  9.6058e-02, -1.1331e+00],\n",
      "        [ 4.4212e+00,  4.5136e+00,  2.0828e-01,  4.6422e+00],\n",
      "        [ 4.1317e+00,  9.3499e+00,  3.6456e-01, -1.4507e+01],\n",
      "        [ 2.1573e+00,  2.4650e+00, -2.9339e-01, -1.0865e+01],\n",
      "        [-3.8495e+00, -1.3594e+01,  1.2703e-02,  1.2632e+01],\n",
      "        [-6.8884e-01,  2.1425e+00,  1.0611e-01, -4.7899e-01],\n",
      "        [-3.2792e+00, -8.4699e+00,  2.5777e-01, -2.2777e+01],\n",
      "        [-3.2588e-02,  1.3808e+00, -4.0830e-02,  2.8451e+00],\n",
      "        [-5.5408e-01,  7.8820e+00, -2.5214e-01, -5.9751e+00],\n",
      "        [ 4.3313e+00,  1.9255e+01, -1.0267e-01,  4.5766e+00],\n",
      "        [-2.8965e+00, -1.7932e+01,  1.1841e-01, -3.2316e-01],\n",
      "        [ 2.2868e+00,  1.1169e+01,  8.2241e-02,  2.0722e+01],\n",
      "        [-3.8481e+00,  6.2149e-01, -2.4441e-01,  1.4349e+01],\n",
      "        [ 3.9882e+00, -2.4354e+01,  2.7695e-01, -4.6339e-02],\n",
      "        [-1.6930e+00, -5.0639e+00, -1.8364e-02, -5.8502e+00],\n",
      "        [ 3.6190e+00,  1.2486e+01,  2.2415e-01, -1.0536e+01],\n",
      "        [-2.6681e+00, -2.9938e+00, -1.7717e-01, -3.6205e-01],\n",
      "        [ 1.2115e+00, -8.4943e+00,  4.0222e-01, -5.2072e+00],\n",
      "        [-3.7875e+00, -6.7369e+00, -3.9171e-01,  4.1235e+00],\n",
      "        [-4.2319e+00,  7.2685e+00,  1.9693e-01, -5.7202e+00],\n",
      "        [-4.2699e+00,  1.5016e+01, -5.4718e-02,  4.1169e+00],\n",
      "        [ 1.6500e+00, -1.3882e+01,  1.9879e-01, -9.4679e+00],\n",
      "        [-4.7901e+00, -1.9096e-01, -1.1157e-01, -8.3775e+00],\n",
      "        [-2.5481e+00, -1.3673e+01, -7.5641e-02,  3.8938e+00],\n",
      "        [-4.2958e+00, -1.1290e+01, -8.5735e-02,  5.4854e+00],\n",
      "        [-4.6203e+00, -2.3623e+01, -2.8453e-01,  4.9888e+00],\n",
      "        [ 1.5685e+00,  1.0291e+01, -3.7911e-01, -1.9327e+01],\n",
      "        [-2.0667e+00, -6.1631e+00, -1.7566e-01,  6.6134e+00],\n",
      "        [-4.5113e+00,  1.1305e+01,  7.1680e-02, -1.4278e+01],\n",
      "        [-2.0119e+00,  1.3298e+01, -5.8542e-02, -2.6829e+00],\n",
      "        [-4.0401e-01,  1.4928e+01,  1.6699e-01,  4.1037e+00],\n",
      "        [ 2.1938e+00, -1.9031e+00, -4.9020e-02,  5.3712e+00],\n",
      "        [-4.2902e+00, -3.9736e+00,  1.4393e-02, -2.5382e+00],\n",
      "        [-3.6693e+00,  8.4035e+00, -2.2838e-01,  2.7123e+00],\n",
      "        [ 7.9573e-01, -1.5453e+01,  3.8860e-01, -2.3633e+00],\n",
      "        [-1.8259e+00,  2.4629e+00, -1.8037e-02, -6.0044e+00],\n",
      "        [ 2.9725e+00,  9.7075e+00, -1.9743e-01, -3.5357e+00],\n",
      "        [-4.5811e-01,  5.0699e+00,  3.8326e-01, -4.4813e+00],\n",
      "        [ 3.7240e+00, -2.7199e+00,  1.9372e-01, -1.5675e+01],\n",
      "        [-3.8979e+00, -1.8111e+01, -2.1873e-01, -1.5584e+01],\n",
      "        [ 1.1599e+00,  1.8423e+01, -5.4413e-02, -1.0687e+01],\n",
      "        [-4.0736e+00,  1.2573e+01, -2.3828e-01, -2.8207e-01],\n",
      "        [-4.9072e-01,  3.8657e+00,  2.7469e-01, -2.0025e-01]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specifications\n",
    "num_observations = 4\n",
    "cart_position_min = -4.8\n",
    "cart_position_max = 4.8\n",
    "cart_velocity_min = -np.inf\n",
    "cart_velocity_max = np.inf\n",
    "pole_angle_min = -0.418\n",
    "pole_angle_max = 0.418\n",
    "pole_angular_velocity_min = -np.inf\n",
    "pole_angular_velocity_max = np.inf\n",
    "\n",
    "def create_batched_random_tensor(n):\n",
    "    # Creating the batched random tensor\n",
    "    cart_position = np.random.uniform(cart_position_min, cart_position_max, size=n)\n",
    "    cart_velocity = np.random.normal(loc=0.0, scale=10.0, size=n)  # Assuming normal distribution with large std deviation\n",
    "    pole_angle = np.random.uniform(pole_angle_min, pole_angle_max, size=n)\n",
    "    pole_angular_velocity = np.random.normal(loc=0.0, scale=10.0, size=n)  # Assuming normal distribution with large std deviation\n",
    "\n",
    "    # Combining into a single tensor of shape (n, 4)\n",
    "    batched_tensor = np.stack((cart_position, cart_velocity, pole_angle, pole_angular_velocity), axis=-1)\n",
    "    \n",
    "    return batched_tensor\n",
    "\n",
    "# Example usage with batch size n = 5\n",
    "n = 100\n",
    "batched_tensor = torch.tensor(create_batched_random_tensor(n), dtype=torch.float32)\n",
    "print(batched_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the network\n",
    "# network = MICOMLPNetwork(in_features=4,\n",
    "#                          activation_class=torch.nn.ReLU, \n",
    "#                          encoder_out_features=8,\n",
    "#                          mlp_out_features=1,\n",
    "#                          encoder_num_cells=[16],\n",
    "#                          mlp_num_cells=[8])\n",
    "\n",
    "network = MICOMLPNetwork(\n",
    "    in_features=4,\n",
    "    activation_class=torch.nn.ReLU,\n",
    "    encoder_out_features=3,\n",
    "    mlp_out_features=env.action_spec.shape[-1],\n",
    "    encoder_num_cells=[128],\n",
    "    mlp_num_cells=[64]\n",
    ")\n",
    "\n",
    "# Define dummy target tensors for losses\n",
    "target_representation = torch.randn(100, 2)  # Assuming the representation has 8 features\n",
    "target_q_values = torch.randn(100, 2)  # Assuming the Q-values have 1 feature\n",
    "\n",
    "# Define loss functions\n",
    "criterion_representation = nn.MSELoss()\n",
    "criterion_q_values = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Forward pass\n",
    "q_values, representation = network(batched_tensor)\n",
    "\n",
    "# Compute the losses\n",
    "# loss_representation = criterion_representation(representation, target_representation)\n",
    "# loss_q_values = criterion_q_values(q_values, target_q_values)\n",
    "\n",
    "# # Sum the losses\n",
    "# total_loss = loss_representation + loss_q_values\n",
    "\n",
    "# # Backward pass and optimization\n",
    "# optimizer.zero_grad()\n",
    "# total_loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# # Print losses\n",
    "# print(f\"Total Loss: {total_loss.item()}, Loss Representation: {loss_representation.item()}, Loss Q-Values: {loss_q_values.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the gradients of the total_loss\n",
    "# for name, param in network.named_parameters():\n",
    "#     print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_example = torch.tensor(batched_tensor, dtype=torch.float32)\n",
    "# q_values, representation = value_mlp(toy_example)\n",
    "# print(q_values)\n",
    "# print(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=MICOMLPNetwork(\n",
       "      (activation): ReLU()\n",
       "      (encoder): ModuleList(\n",
       "        (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=3, bias=True)\n",
       "      )\n",
       "      (q_net): ModuleList(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['action_value', 'representation'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net = Mod(network, \n",
    "                in_keys=[\"observation\"], \n",
    "                out_keys=[\"action_value\", \"representation\"])\n",
    "value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QValueActor(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MICOMLPNetwork(\n",
       "            (activation): ReLU()\n",
       "            (encoder): ModuleList(\n",
       "              (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "              (1): Linear(in_features=128, out_features=3, bias=True)\n",
       "            )\n",
       "            (q_net): ModuleList(\n",
       "              (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "            )\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['action_value', 'representation'])\n",
       "      (1): QValueModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy = Seq(value_net, \n",
    "#              QValueModule(spec=env.action_spec))\n",
    "# policy\n",
    "\n",
    "policy = QValueActor(\n",
    "    module=value_net,\n",
    "    spec=CompositeSpec(action= env.specs[\"input_spec\", \"full_action_spec\", \"action\"]),\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): QValueActor(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=MICOMLPNetwork(\n",
       "                  (activation): ReLU()\n",
       "                  (encoder): ModuleList(\n",
       "                    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "                    (1): Linear(in_features=128, out_features=3, bias=True)\n",
       "                  )\n",
       "                  (q_net): ModuleList(\n",
       "                    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "                    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "                  )\n",
       "                ),\n",
       "                device=cpu,\n",
       "                in_keys=['observation'],\n",
       "                out_keys=['action_value', 'representation'])\n",
       "            (1): QValueModule()\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['representation', 'action', 'action_value', 'chosen_action_value'])\n",
       "      (1): EGreedyModule()\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['representation', 'action_value', 'chosen_action_value', 'action'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the exploration step (e-greedy policy)\n",
    "exploration_module = EGreedyModule(\n",
    "    env.action_spec, \n",
    "    annealing_num_steps=100_000, \n",
    "    eps_init=0.1,\n",
    ")\n",
    "policy_explore = Seq(policy, \n",
    "                     exploration_module)\n",
    "policy_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to collect the data (experiences)\n",
    "init_rand_steps = 5000 # warm-up steps\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "replay_capacity = 100_000\n",
    "\n",
    "# NOTE: collector will gather rollouts continously\n",
    "# If the current trajectory ends, it will start a new one\n",
    "# NOTE: the rollout gotten from the collector is a dictionary\n",
    "# that defines the sate and next state as a tensor with a batch dimension in the begining\n",
    "# for example a rollout of 10 steps will have a tensor of observation of 10 in the batch dimension\n",
    "# and the next will also have 10 which are all the tensors of the next state\n",
    "# Practically, next is as you will shift the tensor of observation by one step\n",
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=frames_per_batch,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=init_rand_steps,\n",
    "# )\n",
    "# rb = ReplayBuffer(storage=LazyTensorStorage(replay_capacity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the recording and logging\n",
    "path = \"./training_loop\"\n",
    "logger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\n",
    "video_recorder = VideoRecorder(logger, tag=\"video\")\n",
    "record_env = TransformedEnv(\n",
    "    GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0873, 0.2517],\n",
      "        [0.0000, 0.0238, 0.1848],\n",
      "        [0.0000, 0.0000, 0.3682],\n",
      "        [0.0000, 0.0874, 0.2489],\n",
      "        [0.0000, 0.0206, 0.1842],\n",
      "        [0.0000, 0.0000, 0.3667],\n",
      "        [0.0000, 0.0905, 0.2513],\n",
      "        [0.0000, 0.0148, 0.1964],\n",
      "        [0.0000, 0.0970, 0.2523],\n",
      "        [0.0000, 0.0000, 0.2323]])\n"
     ]
    }
   ],
   "source": [
    "# collector = SyncDataCollector(\n",
    "#     env,\n",
    "#     policy_explore,\n",
    "#     frames_per_batch=10,\n",
    "#     total_frames=500_100,\n",
    "#     init_random_frames=10000,\n",
    "# )\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=env,\n",
    "    policy=policy_explore,\n",
    "    frames_per_batch=10,\n",
    "    total_frames=100,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1\n",
    ")\n",
    "# NOTE: IMPORTANTISIMO en las primeras iteraciones no se usa la policy, entonces representation se configura\n",
    "# a zero, por lo que el primer batch de datos no tiene representation\n",
    "# Tengo que hacer el warm-up de otra manera (ojo con esto)\n",
    "\n",
    "for data in collector:\n",
    "    print(data['representation'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_next_rewards(rewards, dones):\n",
    "    next_next_rewards = rewards.clone().roll(-1, dims=0)\n",
    "    next_next_rewards[dones] = 0\n",
    "    return next_next_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_next_rewards = get_next_next_rewards(data['next','reward'], data['next','done'])\n",
    "\n",
    "next_next_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                next_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data.device is not None:\n",
    "    next_next_rewards = next_next_rewards.to(data.device)\n",
    "\n",
    "data.set(\n",
    "    (\"next\", \"next_reward\"),\n",
    "    next_next_rewards,\n",
    "    inplace=True,\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.tensor([1, 2, 3, 4, 5])\n",
    "arr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data['next','done'][-1]:\n",
    "    # next_next_rewards[-1] = 0 # Set according the reward of the env \n",
    "                              # in cartpole will be one so I don't need to do anything\n",
    "    # NOTE: Better remove that last step to avoid problems\n",
    "    data = data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tensor_dict_next_next_reward(tensordict):\n",
    "\n",
    "    def get_next_next_rewards(rewards, dones):\n",
    "        next_next_rewards = rewards.clone().roll(-1, dims=0)\n",
    "        next_next_rewards[dones] = 0\n",
    "        return next_next_rewards\n",
    "\n",
    "    next_next_rewards = get_next_next_rewards(tensordict['next','reward'], tensordict['next','done'])\n",
    "\n",
    "    if tensordict.device is not None:\n",
    "        next_next_rewards = next_next_rewards.to(tensordict.device)\n",
    "\n",
    "    tensordict.set(\n",
    "        (\"next\", \"next_reward\"),\n",
    "        next_next_rewards,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Side effect of the last step. As we cannot get the next_next_reward of the last step\n",
    "    # we need to remove it or assign it to a value that we know for sure like in cartpole\n",
    "    # will be always be one, and zero if it is done True.\n",
    "\n",
    "    # NOTE: We comment this part, because we are using the cartpole environment, but \n",
    "    # uncomment and update properly depending the case\n",
    "    # if not tensordict['next','done'][-1]:\n",
    "    #     # Two options\n",
    "    #     # 1. Assign a value\n",
    "    #     # next_next_rewards[-1] = 1 # Set according the reward of the env \n",
    "    #                                # in cartpole will be one so I don't need to do anything\n",
    "    #                                # in cartpole I don't need even to assign because all are 1\n",
    "    #     # 2. Remove that last step to avoid problems\n",
    "    #     data = data[:-1] \n",
    "\n",
    "    return tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                next_reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([10, 4, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = update_tensor_dict_next_next_reward(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_next_next_reward = data['next','reward'].clone()\n",
    "tmp_next_next_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_next_mask = data['next','done'].clone().roll(1, dims=0)\n",
    "next_next_mask[0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,  10,  20, 100, 200])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_temp = torch.tensor([1, 2, 3, 10, 20, 100, 200])\n",
    "reward_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True, False,  True])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([False, False, True, False, True, False, True])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   3,  10,  20, 100, 200,   1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_temp.roll(-1, dims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 100,   1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_temp.roll(-1, dims=0)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,   10,   20,  100,  200, 1000])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_temp = torch.tensor([1, 2, 3, 10, 20, 100, 200, 1000])\n",
    "reward_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True, False,  True, False])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([False, False, True, False, True, False, True, False])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   3,   0,  20,   0, 200,   0,   1])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_next_rewards(reward_temp, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   3,   10,   20,  100,  200, 1000])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_temp = torch.tensor([3, 10, 20, 100, 200, 1000])\n",
    "reward_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False,  True, False])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([ True, False, True, False, True, False])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  20,   0, 200,   0,   3])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_next_rewards(reward_temp, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1196, -1.7470,  0.2361,  2.8161])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','observation'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0037, -0.7668,  0.0538,  1.1900],\n",
       "        [-0.0885, -1.5509,  0.1867,  2.4725],\n",
       "        [-0.1196, -1.7470,  0.2361,  2.8161],\n",
       "        [-0.0581, -0.7588,  0.0363,  1.1615],\n",
       "        [-0.0955,  0.0178,  0.0960,  0.0764],\n",
       "        [-0.1176, -0.7678,  0.1408,  1.3712],\n",
       "        [-0.1755, -1.3575,  0.2432,  2.3923],\n",
       "        [-0.0218, -0.7441,  0.0790,  1.2167],\n",
       "        [-0.1049, -1.5290,  0.2149,  2.5275],\n",
       "        [-0.0177, -0.7991,  0.0436,  1.1731]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['next','observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 1, 2, 2, 3])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['collector','traj_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8554e-02,  1.5046e-02,  1.8845e-02, -1.1108e-02],\n",
       "        [-3.6897e-03, -7.6677e-01,  5.3815e-02,  1.1900e+00],\n",
       "        [-8.8540e-02, -1.5509e+00,  1.8668e-01,  2.4725e+00],\n",
       "        [-3.6490e-02,  2.2149e-02,  2.5354e-03, -1.8976e-02],\n",
       "        [-5.8139e-02, -7.5877e-01,  3.6262e-02,  1.1615e+00],\n",
       "        [-9.5512e-02,  1.7841e-02,  9.6036e-02,  7.6353e-02],\n",
       "        [-1.1765e-01, -7.6778e-01,  1.4078e-01,  1.3712e+00],\n",
       "        [-1.4165e-03,  3.8949e-02,  4.3404e-02, -1.4220e-02],\n",
       "        [-2.1787e-02, -7.4414e-01,  7.9022e-02,  1.2167e+00],\n",
       "        [ 7.1530e-03, -1.7813e-02,  9.4300e-03, -1.6209e-02]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8554e-02,  1.5046e-02,  1.8845e-02, -1.1108e-02],\n",
       "        [-3.6897e-03, -7.6677e-01,  5.3815e-02,  1.1900e+00],\n",
       "        [-8.8540e-02, -1.5509e+00,  1.8668e-01,  2.4725e+00],\n",
       "        [-3.6490e-02,  2.2149e-02,  2.5354e-03, -1.8976e-02],\n",
       "        [-5.8139e-02, -7.5877e-01,  3.6262e-02,  1.1615e+00],\n",
       "        [-9.5512e-02,  1.7841e-02,  9.6036e-02,  7.6353e-02],\n",
       "        [-1.1765e-01, -7.6778e-01,  1.4078e-01,  1.3712e+00],\n",
       "        [-1.4165e-03,  3.8949e-02,  4.3404e-02, -1.4220e-02],\n",
       "        [-2.1787e-02, -7.4414e-01,  7.9022e-02,  1.2167e+00],\n",
       "        [ 7.1530e-03, -1.7813e-02,  9.4300e-03, -1.6209e-02]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['observation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyTensorStorage(\n",
       "        data=<empty>, \n",
       "        shape=None, \n",
       "        len=0, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=None, slice_len=2, end_key=('next', 'done'), traj_key=('collector', 'traj_ids'), truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=0, full_storage=False), \n",
       "    batch_size=10, \n",
       "    collate_fn=<function _collate_id at 0x7fe4f157fba0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "\n",
    "size = 100\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(size),\n",
    "    sampler=SliceSampler(traj_key=(\"collector\",\"traj_ids\"), slice_len=2),\n",
    "    batch_size=10,\n",
    ")\n",
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['collector','traj_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rb.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8],\n",
       "        [2],\n",
       "        [3],\n",
       "        [5],\n",
       "        [6],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['step_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0251e-02,  1.3341e+00, -9.1547e-02, -2.0319e+00],\n",
       "        [ 6.6933e-02,  1.5300e+00, -1.3218e-01, -2.3514e+00],\n",
       "        [-3.4538e-02,  3.5789e-01,  2.1562e-02, -5.5068e-01],\n",
       "        [-2.7381e-02,  5.5270e-01,  1.0548e-02, -8.3650e-01],\n",
       "        [-1.3732e-03,  9.4288e-01, -2.8698e-02, -1.4205e+00],\n",
       "        [ 1.7484e-02,  1.1383e+00, -5.7108e-02, -1.7220e+00],\n",
       "        [-3.7802e-02,  1.6316e-01,  2.6894e-02, -2.6660e-01],\n",
       "        [-3.4538e-02,  3.5789e-01,  2.1562e-02, -5.5068e-01],\n",
       "        [-3.7170e-02, -3.1572e-02,  2.6542e-02,  1.7587e-02],\n",
       "        [-3.7802e-02,  1.6316e-01,  2.6894e-02, -2.6660e-01]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0669,  1.5300, -0.1322, -2.3514],\n",
       "        [ 0.0975,  1.7260, -0.1792, -2.6817],\n",
       "        [-0.0274,  0.5527,  0.0105, -0.8365],\n",
       "        [-0.0163,  0.7477, -0.0062, -1.1258],\n",
       "        [ 0.0175,  1.1383, -0.0571, -1.7220],\n",
       "        [ 0.0403,  1.3341, -0.0915, -2.0319],\n",
       "        [-0.0345,  0.3579,  0.0216, -0.5507],\n",
       "        [-0.0274,  0.5527,  0.0105, -0.8365],\n",
       "        [-0.0378,  0.1632,  0.0269, -0.2666],\n",
       "        [-0.0345,  0.3579,  0.0216, -0.5507]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"next\", \"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5099, 0.0000],\n",
       "        [0.0000, 0.5961, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.2237, 0.0000],\n",
       "        [0.0000, 0.3417, 0.0000],\n",
       "        [0.0000, 0.4248, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.0813, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "sample['representation']\n",
    "\n",
    "first_states = sample[0::2] # even rows\n",
    "second_states = sample[1::2] # odd rows (or next states)\n",
    "\n",
    "print(first_states)\n",
    "print(second_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5099, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.3417, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000],\n",
       "        [0.0000, 0.0813, 0.0000]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5961, 0.0000],\n",
       "        [0.0000, 0.2237, 0.0000],\n",
       "        [0.0000, 0.4248, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n",
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "repeated_a = torch.Tensor.repeat(a, (2, 3))\n",
    "\n",
    "print(repeated_a)\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "tiled_a = torch.tile(a, (2, 3))\n",
    "print(tiled_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5961, 0.0000],\n",
       "        [0.0000, 0.2237, 0.0000],\n",
       "        [0.0000, 0.4248, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation'].shape # batch, rep_dim\n",
    "\n",
    "repeated_rep = torch.tile(second_states['representation'], (1,1,5)).view(5,5,3)\n",
    "repeated_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squarify(x):\n",
    "    # Squarify will take the input and adds a new dimension between the batch and the representation\n",
    "    # so that the representation is repeated along the new dimension\n",
    "    # To visualize thing of x as a matrix of batch_size x representation_dim\n",
    "    # and squarify will place that matrix in a lateral way and repeat it along the new dimension j\n",
    "\n",
    "    # NOTE: after squarify if you pick a i-th row all the elements (j-th index) in that row will be the same\n",
    "    batch_size = x.shape[0]\n",
    "    if len(x.shape) > 1:\n",
    "        representation_dim = x.shape[-1]\n",
    "        return x.tile((batch_size,)).view(batch_size, batch_size, representation_dim)\n",
    "    return x.tile((batch_size,)).view(batch_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarify(second_states['next','reward']).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000],\n",
       "         [0.0000, 0.5961, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000],\n",
       "         [0.0000, 0.2237, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000],\n",
       "         [0.0000, 0.4248, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000],\n",
       "         [0.0000, 0.1776, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000],\n",
       "         [0.0000, 0.1231, 0.0000]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarify(second_states['representation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3077, 0.1550, 0.2202, 0.1458, 0.1376, 0.1935, 0.0408, 0.1060, 0.0316,\n",
       "        0.0234, 0.2360, 0.0834, 0.1486, 0.0741, 0.0659, 0.1853, 0.0326, 0.0978,\n",
       "        0.0234, 0.0152, 0.1810, 0.0284, 0.0935, 0.0191, 0.0109])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representation_distances(first_representations, second_representations,\n",
    "                             distance_fn, beta=0.1,\n",
    "                             return_distance_components=False):\n",
    "  \"\"\"Compute distances between representations.\n",
    "     In the paper, it corresponds to the calculation of the U term\n",
    "     for each pair of representations in the batch (all-vs-all).\n",
    "\n",
    "  This will compute the distances between two representations.\n",
    "\n",
    "  Args:\n",
    "    first_representations: first set of representations to use.\n",
    "    second_representations: second set of representations to use.\n",
    "    distance_fn: function to use for computing representation distances.\n",
    "    beta: float, weight given to cosine distance between representations.\n",
    "    return_distance_components: bool, whether to return the components used for\n",
    "      computing the distance.\n",
    "\n",
    "  Returns:\n",
    "    The distances between representations, combining the average of the norm of\n",
    "    the representations and the distance given by distance_fn.\n",
    "  \"\"\"\n",
    "  batch_size = first_representations.shape[0]\n",
    "  representation_dim = first_representations.shape[-1]\n",
    "\n",
    "  # Squarify the representations and reshape them to make a pair-waise comparison with vmap\n",
    "  first_squared_reps = squarify(first_representations)\n",
    "  first_squared_reps = torch.reshape(first_squared_reps,\n",
    "                                   [batch_size**2, representation_dim])\n",
    "  \n",
    "  # Squarify the representations and reshape them to make a pair-waise comparison with vmap\n",
    "  # However, we now need to permute (transpose) the dimension 0, 1 to alternate the values\n",
    "  # so that we have the pair-wise comparisons of all-vs-all\n",
    "  second_squared_reps = squarify(second_representations)\n",
    "  second_squared_reps = torch.permute(second_squared_reps, dims=(1, 0, 2))\n",
    "  second_squared_reps = torch.reshape(second_squared_reps,\n",
    "                                    [batch_size**2, representation_dim])\n",
    "  \n",
    "  # vmap will calculate the pairwise distance_fn along the dimension specified\n",
    "  # in in_axes. In this case, will take the dim 0 of the first_squared_reps and\n",
    "  # the dim 0 of the second_squared_reps and apply the distance\n",
    "  # It vertorize the process of calculating the distance between all the pairs\n",
    "\n",
    "  # NOTE: base distance corresponds to the second term in the U calculation in the paper\n",
    "  # It calculates the angle between the representations in the paper\n",
    "  # Check what function is using\n",
    "  base_distances = torch.vmap(distance_fn, in_dims=(0, 0))(first_squared_reps,\n",
    "                                                         second_squared_reps)\n",
    "  base_distances = base_distances\n",
    "  print(base_distances.shape)\n",
    "  # Sum along the second dimension and normalize the distance\n",
    "  # NOTE: this is practically the first term of U in the paper\n",
    "  norm_average = 0.5 * (torch.sum(torch.square(first_squared_reps), -1) +\n",
    "                        torch.sum(torch.square(second_squared_reps), -1))\n",
    "  \n",
    "  print(norm_average.shape)\n",
    "  if return_distance_components:\n",
    "    return norm_average + beta * base_distances, norm_average, base_distances\n",
    "  return norm_average + beta * base_distances\n",
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def _sqrt(x):\n",
    "  # zeros like instead of zeros\n",
    "  # It is because vmap works with a weird way of broadcasting\n",
    "  # and a weird structure based on tensors\n",
    "  tol = torch.zeros_like(x)\n",
    "  return torch.sqrt(torch.maximum(x, tol))\n",
    "\n",
    "\n",
    "def cosine_distance(x, y):\n",
    "  # NOTE: the cosine similarity is not calculate directly for \n",
    "  # instabilities observed when using `jnp.arccos`, but I'm using torch\n",
    "  # so I don't know if I will need to do this\n",
    "  numerator = torch.sum(x * y)\n",
    "  denominator = torch.sqrt(torch.sum(x**2)) * torch.sqrt(torch.sum(y**2))\n",
    "  cos_similarity = numerator / (denominator + EPSILON)\n",
    "\n",
    "  # cos_similarity = cos(theta)\n",
    "\n",
    "  # NOTE: From, the Pythagorean trigometric identity\n",
    "  # sin^2(theta) + cos^2(theta) = 1\n",
    "  # you can get sin(theta) = sqrt(1 - cos^2(theta))\n",
    "  # and the arctan2(sin(theta), cos(theta)) = theta\n",
    "  return torch.arctan2(_sqrt(1. - cos_similarity**2), cos_similarity)\n",
    "\n",
    "distances = representation_distances(first_states['representation'], \n",
    "                                     second_states['representation'], \n",
    "                                     cosine_distance)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: check in the main code if the output of this requires grad and the \n",
    "# other output must require grad\n",
    "# @torch.no_grad()\n",
    "def target_distances(representations, rewards, distance_fn, cumulative_gamma):\n",
    "  \"\"\"Target distance using the metric operator. This is the T in the paper :D\"\"\"\n",
    "  next_state_similarities = representation_distances(\n",
    "      representations, representations, distance_fn)\n",
    "  squared_rews = squarify(rewards).squeeze(-1)\n",
    "  squared_rews_transp = squared_rews.T\n",
    "  squared_rews = squared_rews.reshape((squared_rews.shape[0]**2))\n",
    "  squared_rews_transp = squared_rews_transp.reshape(\n",
    "      (squared_rews_transp.shape[0]**2))\n",
    "  reward_diffs = torch.abs(squared_rews - squared_rews_transp)\n",
    "  return reward_diffs + cumulative_gamma * next_state_similarities\n",
    "\n",
    "t_distances = target_distances(first_states['representation'], first_states['next','reward'], cosine_distance, cumulative_gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([5]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([5]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3077, 0.1550, 0.2202, 0.1458, 0.1376, 0.1935, 0.0408, 0.1060, 0.0316,\n",
       "        0.0234, 0.2360, 0.0834, 0.1486, 0.0741, 0.0659, 0.1853, 0.0326, 0.0978,\n",
       "        0.0234, 0.0152, 0.1810, 0.0284, 0.0935, 0.0191, 0.0109])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mico_distance = representation_distances(\n",
    "    first_states['representation'], second_states['representation'], cosine_distance)\n",
    "mico_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mico_distance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = first_states['representation'].shape[0]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_vs_all_mico_priorities(batch_online_representation, batch_target_representation, mico_beta):\n",
    "\n",
    "    all_vs_all_mico_distances = representation_distances(\n",
    "        batch_online_representation, batch_target_representation, cosine_distance)\n",
    "    \n",
    "    batch_size = batch_online_representation.shape[0]\n",
    "\n",
    "    # NOTE: Mico distance is a unidimensional tensor with the distances of all the pairs\n",
    "    # Apply the reshape to get the distances of all the pairs, and get the mean\n",
    "    # of the distances of the current states\n",
    "    all_vs_all_mico_distances = all_vs_all_mico_distances.reshape((batch_size,batch_size))\n",
    "    return all_vs_all_mico_distances.mean(-1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "mico_priorities = all_vs_all_mico_priorities(\n",
    "        batch_online_representation = first_states['representation'],\n",
    "        batch_target_representation = second_states['representation'],\n",
    "        mico_beta = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1932, 0.0790, 0.1216, 0.0709, 0.0666])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mico_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3077, 0.1550, 0.2202, 0.1458, 0.1376],\n",
      "        [0.1935, 0.0408, 0.1060, 0.0316, 0.0234],\n",
      "        [0.2360, 0.0834, 0.1486, 0.0741, 0.0659],\n",
      "        [0.1853, 0.0326, 0.0978, 0.0234, 0.0152],\n",
      "        [0.1810, 0.0284, 0.0935, 0.0191, 0.0109]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1932, 0.0790, 0.1216, 0.0709, 0.0666])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Mico distance is a unidimensional tensor with the distances of all the pairs\n",
    "# Apply the reshape to get the distances of all the pairs, and get the mean\n",
    "# of the distances of the current states\n",
    "mico_priority = mico_distance.reshape((batch_size,batch_size))\n",
    "print(mico_priority)\n",
    "mico_priority = mico_priority.mean(-1)\n",
    "mico_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0790)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mico_distance.reshape((batch_size,batch_size))[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_distances.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_vs_next_mico_priorities(\n",
    "        current_state_representations,\n",
    "        next_state_representations,\n",
    "        mico_beta,\n",
    "        return_distance_components = False):\n",
    "\n",
    "  base_distances = torch.vmap(cosine_distance, in_dims=(0, 0))(current_state_representations,\n",
    "                                                         next_state_representations)\n",
    "\n",
    "  # Sum along the second dimension and normalize the distance\n",
    "  # NOTE: this is practically the first term of U in the paper\n",
    "  norm_average = 0.5 * (torch.sum(torch.square(current_state_representations), -1) +\n",
    "                        torch.sum(torch.square(next_state_representations), -1))\n",
    "  \n",
    "  mico_distance = norm_average + mico_beta * base_distances\n",
    "  \n",
    "  if return_distance_components:\n",
    "    return mico_distance, norm_average, base_distances\n",
    "  \n",
    "  # Repeat the priority to assign the same priority to both\n",
    "  # the current and next state\n",
    "  return mico_distance.repeat_interleave(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3077, 0.1550, 0.2202, 0.1458, 0.1376, 0.1935, 0.0408, 0.1060, 0.0316,\n",
       "        0.0234, 0.2360, 0.0834, 0.1486, 0.0741, 0.0659, 0.1853, 0.0326, 0.0978,\n",
       "        0.0234, 0.0152, 0.1810, 0.0284, 0.0935, 0.0191, 0.0109])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mico_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5099, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.3417, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000],\n",
       "        [0.0000, 0.0813, 0.0000]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5961, 0.0000],\n",
       "        [0.0000, 0.2237, 0.0000],\n",
       "        [0.0000, 0.4248, 0.0000],\n",
       "        [0.0000, 0.1776, 0.0000],\n",
       "        [0.0000, 0.1231, 0.0000]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_states['representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3077, 0.3077, 0.0408, 0.0408, 0.1486, 0.1486, 0.0234, 0.0234, 0.0109,\n",
       "        0.0109])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_vs_next_mico_priorities(\n",
    "        current_state_representations = first_states['representation'],\n",
    "        next_state_representations = second_states['representation'],\n",
    "        mico_beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1717, 0.1063, 0.0000],\n",
       "         [0.1832, 0.0750, 0.4068],\n",
       "         [0.0490, 0.0752, 0.0896],\n",
       "         [0.0857, 0.0904, 0.1998],\n",
       "         [0.0490, 0.0752, 0.0896]]),\n",
       " tensor([[0.0836, 0.2269],\n",
       "         [0.0057, 0.2379],\n",
       "         [0.0791, 0.2383],\n",
       "         [0.0543, 0.2416],\n",
       "         [0.0791, 0.2383]]),\n",
       " tensor([[0.2269],\n",
       "         [0.2379],\n",
       "         [0.2383],\n",
       "         [0.2416],\n",
       "         [0.2383]]),\n",
       " tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Checar la distancia euclidean entre la representacion target y la representacion con\n",
    "# la politica actual\n",
    "\n",
    "# NOTE: en el repositorio de MICO, la distancia target es calculada con\n",
    "# la target network (que es una copia de la politica actual) osea mis representaciones guardadas\n",
    "# la distancia online por otra parte es calculada con una representacion con red actual\n",
    "# y una representacion target\n",
    "\n",
    "\n",
    "collector.policy(first_states['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make the components\n",
    "# Policy\n",
    "\n",
    "from dqn_mico_er.utils_cartpole import make_dqn_model, make_env\n",
    "from dqn_mico_er.custom_modules import MICODQNLoss\n",
    "\n",
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.data.replay_buffers.samplers import RandomSampler, PrioritizedSampler, PrioritizedSliceSampler\n",
    "from torchrl.objectives import DQNLoss, HardUpdate\n",
    "\n",
    "# load condig_cartpole.yaml\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with open(\"dqn_mico_er/config_cartpole.yaml\") as f:\n",
    "    cfg = OmegaConf.create(yaml.safe_load(f))\n",
    "\n",
    "model = make_dqn_model(\"CartPole-v1\", cfg.policy)\n",
    "\n",
    "greedy_module = EGreedyModule(\n",
    "    annealing_num_steps=cfg.collector.annealing_frames,\n",
    "    eps_init=cfg.collector.eps_start,\n",
    "    eps_end=cfg.collector.eps_end,\n",
    "    spec=model.spec,\n",
    ")\n",
    "model_explore = TensorDictSequential(\n",
    "    model,\n",
    "    greedy_module,\n",
    ") #.to(device)\n",
    "\n",
    "# Create the collector\n",
    "# NOTE: init_random_frames: Number of frames \n",
    "# for which the policy is ignored before it is called.\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=make_env(cfg.env.env_name, \"cpu\", cfg.env.seed),\n",
    "    policy=model_explore,\n",
    "    frames_per_batch=cfg.collector.frames_per_batch,\n",
    "    total_frames=cfg.collector.total_frames,\n",
    "    device=\"cpu\",\n",
    "    storing_device=\"cpu\",\n",
    "    max_frames_per_traj=-1,\n",
    "    init_random_frames=cfg.collector.init_random_frames,\n",
    ")\n",
    "\n",
    "# Create the replay buffer\n",
    "if cfg.buffer.prioritized_replay:\n",
    "    print(\"Using Prioritized Replay Buffer\")\n",
    "    sampler = PrioritizedSliceSampler(\n",
    "        max_capacity=cfg.buffer.buffer_size, \n",
    "        alpha=cfg.buffer.alpha, \n",
    "        beta=cfg.buffer.beta, \n",
    "        traj_key=(\"collector\",\"traj_ids\"), \n",
    "        slice_len=2)\n",
    "else:\n",
    "    sampler = SliceSampler(\n",
    "        traj_key=(\"collector\",\"traj_ids\"), \n",
    "        slice_len=2)\n",
    "    \n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    pin_memory=False,\n",
    "    prefetch=10,\n",
    "    storage=LazyTensorStorage(\n",
    "        max_size=cfg.buffer.buffer_size,\n",
    "        device=\"cpu\",\n",
    "    ),\n",
    "    batch_size=cfg.buffer.batch_size,\n",
    "    sampler = sampler\n",
    ")\n",
    "\n",
    "# Create the loss module\n",
    "loss_module = MICODQNLoss(\n",
    "    value_network=model,\n",
    "    loss_function=\"l2\", \n",
    "    delay_value=True, # delay_value=True means we will use a target network\n",
    "    mico_gamma=cfg.loss.mico_gamma,\n",
    "    mico_beta=cfg.loss.mico_beta,\n",
    "    mico_weight=cfg.loss.mico_weight,\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(gamma=cfg.loss.gamma) # only to change the gamma value\n",
    "loss_module = loss_module #.to(device)\n",
    "target_net_updater = HardUpdate(\n",
    "    loss_module, value_network_update_interval=cfg.loss.hard_update_freq\n",
    ")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(loss_module.parameters(), lr=cfg.optim.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of the model parameters per layer\n",
      "tensor(0.4998)\n",
      "tensor(0.4991)\n",
      "tensor(0.0913)\n",
      "tensor(0.0879)\n",
      "tensor(0.1083)\n",
      "tensor(-0.0046)\n",
      "tensor(0.5714)\n",
      "tensor(0.5667)\n",
      "tensor(0.1090)\n",
      "tensor(0.0080)\n",
      "Maximum of the model parameters per layer\n",
      "tensor(0.4998, grad_fn=<MaxBackward1>)\n",
      "tensor(0.4991, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0913, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0879, grad_fn=<MaxBackward1>)\n",
      "tensor(0.1083, grad_fn=<MaxBackward1>)\n",
      "tensor(-0.0046, grad_fn=<MaxBackward1>)\n",
      "tensor(0.5714, grad_fn=<MaxBackward1>)\n",
      "tensor(0.5667, grad_fn=<MaxBackward1>)\n",
      "tensor(0.1090, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0080, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.4998, grad_fn=<MaxBackward1>),\n",
       " tensor(0.4991, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0913, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0879, grad_fn=<MaxBackward1>),\n",
       " tensor(0.1083, grad_fn=<MaxBackward1>),\n",
       " tensor(-0.0046, grad_fn=<MaxBackward1>),\n",
       " tensor(0.5714, grad_fn=<MaxBackward1>),\n",
       " tensor(0.5667, grad_fn=<MaxBackward1>),\n",
       " tensor(0.1090, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0080, grad_fn=<MaxBackward1>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the maximum of the model parameters\n",
    "def print_maximum_weights(model):\n",
    "    print(\"Maximum of the model parameters per layer\")#\n",
    "    weights = []\n",
    "    for p in model.parameters():\n",
    "        print(torch.max(p))\n",
    "        weights.append(torch.max(p))\n",
    "    return weights\n",
    "\n",
    "def print_maximum_grads(model):\n",
    "    print(\"Maximum of the model gradients per layer\")#\n",
    "    max_grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            print(torch.max(p.grad))\n",
    "            max_grads.append(torch.max(p.grad))\n",
    "    return max_grads\n",
    "\n",
    "def print_target_value_weights(loss_module):\n",
    "    with loss_module.target_value_network_params.to_module(loss_module.value_network):\n",
    "        return print_maximum_weights(loss_module.value_network)    \n",
    "\n",
    "def print_value_weights(loss_module):\n",
    "    with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "        return print_maximum_weights(loss_module.value_network)\n",
    "        # print_maximum_grads(loss_module.value_network)    \n",
    "\n",
    "def print_value_grads(loss_module):\n",
    "    with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "        # print_maximum_weights(loss_module.value_network)\n",
    "        return print_maximum_grads(loss_module.value_network)   \n",
    "\n",
    "# Print the maximum of the model parameters\n",
    "print_target_value_weights(loss_module)\n",
    "\n",
    "print_value_weights(loss_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchviz/dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"340pt\" height=\"468pt\"\n",
       " viewBox=\"0.00 0.00 340.00 468.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 464)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-464 336,-464 336,4 -4,4\"/>\n",
       "<!-- 139726566625584 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139726566625584</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"193.5,-31 139.5,-31 139.5,0 193.5,0 193.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 139726561649728 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139726561649728</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223,-86 110,-86 110,-67 223,-67 223,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MseLossBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649728&#45;&gt;139726566625584 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>139726561649728&#45;&gt;139726566625584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-66.79C166.5,-60.07 166.5,-50.4 166.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-41.19 166.5,-31.19 163,-41.19 170,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139726561649920 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139726561649920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-141 116,-141 116,-122 217,-122 217,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649920&#45;&gt;139726561649728 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139726561649920&#45;&gt;139726561649728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-121.75C166.5,-114.8 166.5,-104.85 166.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-96.09 166.5,-86.09 163,-96.09 170,-96.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649680 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139726561649680</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649680&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139726561649680&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.14,-176.98C87.8,-168.46 116.75,-155.23 138.24,-145.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.88,-148.51 147.52,-141.17 136.97,-142.14 139.88,-148.51\"/>\n",
       "</g>\n",
       "<!-- 139726566625200 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139726566625200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"83,-262 18,-262 18,-232 83,-232 83,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">fc2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 139726566625200&#45;&gt;139726561649680 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139726566625200&#45;&gt;139726561649680</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n",
       "</g>\n",
       "<!-- 139726561649824 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139726561649824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-196 119,-196 119,-177 214,-177 214,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649824&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139726561649824&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-176.75C166.5,-169.8 166.5,-159.85 166.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-151.09 166.5,-141.09 163,-151.09 170,-151.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649632 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139726561649632</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-256.5 112,-256.5 112,-237.5 213,-237.5 213,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649632&#45;&gt;139726561649824 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139726561649632&#45;&gt;139726561649824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.09,-237.37C163.65,-229.25 164.5,-216.81 165.21,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.72,-206.38 165.91,-196.17 161.73,-205.91 168.72,-206.38\"/>\n",
       "</g>\n",
       "<!-- 139726561649296 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139726561649296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"118,-322.5 17,-322.5 17,-303.5 118,-303.5 118,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649296&#45;&gt;139726561649632 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139726561649296&#45;&gt;139726561649632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.31,-303.37C95.89,-292.87 122.43,-275 141.12,-262.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.25,-265.19 149.59,-256.7 139.34,-259.38 143.25,-265.19\"/>\n",
       "</g>\n",
       "<!-- 139726566625008 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139726566625008</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"100,-394 35,-394 35,-364 100,-364 100,-394\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 139726566625008&#45;&gt;139726561649296 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139726566625008&#45;&gt;139726561649296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.5,-363.8C67.5,-354.7 67.5,-342.79 67.5,-332.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71,-332.84 67.5,-322.84 64,-332.84 71,-332.84\"/>\n",
       "</g>\n",
       "<!-- 139726561649344 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139726561649344</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-322.5 136,-322.5 136,-303.5 213,-303.5 213,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649344&#45;&gt;139726561649632 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139726561649344&#45;&gt;139726561649632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.88,-303.37C171.14,-294.07 168.31,-278.98 166.04,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.45,-266.09 164.17,-256.91 162.57,-267.38 169.45,-266.09\"/>\n",
       "</g>\n",
       "<!-- 139726561649248 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139726561649248</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"225,-388.5 124,-388.5 124,-369.5 225,-369.5 225,-388.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649248&#45;&gt;139726561649344 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139726561649248&#45;&gt;139726561649344</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.5,-369.37C174.5,-360.16 174.5,-345.29 174.5,-333.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178,-332.91 174.5,-322.91 171,-332.91 178,-332.91\"/>\n",
       "</g>\n",
       "<!-- 139726566624912 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139726566624912</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"213,-460 136,-460 136,-430 213,-430 213,-460\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"174.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (5, 10)</text>\n",
       "</g>\n",
       "<!-- 139726566624912&#45;&gt;139726561649248 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139726566624912&#45;&gt;139726561649248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.5,-429.8C174.5,-420.7 174.5,-408.79 174.5,-398.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178,-398.84 174.5,-388.84 171,-398.84 178,-398.84\"/>\n",
       "</g>\n",
       "<!-- 139726561649968 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>139726561649968</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"313,-196 236,-196 236,-177 313,-177 313,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139726561649968&#45;&gt;139726561649920 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139726561649968&#45;&gt;139726561649920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.15,-176.98C239.93,-168.54 213.3,-155.47 193.35,-145.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.69,-142.43 184.17,-141.17 191.6,-148.72 194.69,-142.43\"/>\n",
       "</g>\n",
       "<!-- 139726561649200 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>139726561649200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"332,-256.5 231,-256.5 231,-237.5 332,-237.5 332,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139726561649200&#45;&gt;139726561649968 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139726561649200&#45;&gt;139726561649968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M280.47,-237.37C279.5,-229.25 278.01,-216.81 276.76,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.2,-205.68 275.54,-196.17 273.25,-206.51 280.2,-205.68\"/>\n",
       "</g>\n",
       "<!-- 139726566625104 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>139726566625104</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"320,-328 243,-328 243,-298 320,-298 320,-328\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">fc2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (2, 5)</text>\n",
       "</g>\n",
       "<!-- 139726566625104&#45;&gt;139726561649200 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>139726566625104&#45;&gt;139726561649200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.5,-297.8C281.5,-288.7 281.5,-276.79 281.5,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"285,-266.84 281.5,-256.84 278,-266.84 285,-266.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f14a0127d50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define a simple neural network with two outputs\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)  # Two outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the network and some input data\n",
    "net = SimpleNet()\n",
    "x = torch.randn(1, 10)\n",
    "target = torch.randn(1, 2)  # Dummy target for the loss function\n",
    "\n",
    "# Perform the forward pass\n",
    "outputs = net(x)\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(outputs, target)\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Visualize the computational graph using torchviz\n",
    "make_dot(loss, params=dict(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "Maximum of the model gradients per layer\n",
      "tensor(0.0024)\n",
      "tensor(0.0056)\n",
      "tensor(0.0090)\n",
      "tensor(0.0161)\n",
      "tensor(0.0317)\n",
      "tensor(0.0813)\n",
      "tensor(0.0212)\n",
      "tensor(0.1943)\n",
      "tensor(0.)\n",
      "tensor(-1.0666)\n",
      "Maximum of the model gradients per layer\n",
      "tensor(0.0024)\n",
      "tensor(0.0056)\n",
      "tensor(0.0090)\n",
      "tensor(0.0161)\n",
      "tensor(0.0317)\n",
      "tensor(0.0813)\n",
      "tensor(0.0212)\n",
      "tensor(0.1943)\n",
      "tensor(0.)\n",
      "tensor(-1.0666)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main loop\n",
    "collected_frames = 0\n",
    "total_episodes = 0\n",
    "start_time = time.time()\n",
    "num_updates = cfg.loss.num_updates\n",
    "batch_size = cfg.buffer.batch_size\n",
    "test_interval = cfg.logger.test_interval\n",
    "num_test_episodes = cfg.logger.num_test_episodes\n",
    "frames_per_batch = cfg.collector.frames_per_batch\n",
    "# pbar = tqdm.tqdm(total=cfg.collector.total_frames)\n",
    "init_random_frames = cfg.collector.init_random_frames\n",
    "sampling_start = time.time()\n",
    "q_losses = torch.zeros(num_updates) #, device=device)\n",
    "\n",
    "for i, data in enumerate(collector):\n",
    "\n",
    "        # NOTE: This reshape must be for frame data (maybe)\n",
    "        data = data.reshape(-1)\n",
    "        current_frames = data.numel()\n",
    "        replay_buffer.extend(data)\n",
    "        collected_frames += current_frames\n",
    "        greedy_module.step(current_frames)\n",
    "\n",
    "        # Get the number of episodes\n",
    "        total_episodes += data[\"next\", \"done\"].sum()\n",
    "\n",
    "        # Get and log training rewards and episode lengths\n",
    "        # Collect the episode rewards and lengths in average over the\n",
    "        # transitions in the current data batch\n",
    "        episode_rewards = data[\"next\", \"episode_reward\"][data[\"next\", \"done\"]]\n",
    "\n",
    "\n",
    "        # Warmup phase (due to the continue statement)\n",
    "        # Additionally This help us to keep a track of the collected_frames\n",
    "        # after the init_random_frames\n",
    "        if collected_frames < init_random_frames:\n",
    "            continue\n",
    "\n",
    "        # optimization steps\n",
    "        training_start = time.time()\n",
    "        for j in range(num_updates):\n",
    "            sampled_tensordict = replay_buffer.sample(batch_size)\n",
    "            # TODO: check if the sample is already in the device\n",
    "            sampled_tensordict = sampled_tensordict #.to(device)\n",
    "\n",
    "            # Also the loss module will use the current and target model to get the q-values\n",
    "            loss_td = loss_module(sampled_tensordict)\n",
    "            q_loss = loss_td[\"loss\"]\n",
    "\n",
    "            # with loss_module.value_network_params.to_module(loss_module.value_network):\n",
    "            #     dot = make_dot(q_loss, params=dict(loss_module.value_network.named_parameters()), show_attrs=True, show_saved=True)\n",
    "            #     dot.render(\"computational_graph_with_loss\", format=\"png\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the priorities\n",
    "            if cfg.buffer.prioritized_replay:\n",
    "                replay_buffer.update_priority(index=sampled_tensordict['index'], priority = sampled_tensordict['td_error'])\n",
    "\n",
    "            # NOTE: This is only one step (after n-updated steps defined before)\n",
    "            # the target will update\n",
    "            target_net_updater.step()\n",
    "            q_losses[j].copy_(q_loss.detach())\n",
    "        training_time = time.time() - training_start\n",
    "\n",
    "        # Get and log evaluation rewards and eval time\n",
    "        # NOTE: As I'm using only the model and not the model_explore that will deterministic I think\n",
    "        # with torch.no_grad(): #, set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "\n",
    "        #     # NOTE: Check how we are using the frames here because it seems that I am dividing \n",
    "        #     # 10 for 50000\n",
    "        #     prev_test_frame = ((i - 1) * frames_per_batch) // test_interval\n",
    "        #     cur_test_frame = (i * frames_per_batch) // test_interval\n",
    "        #     final = current_frames >= collector.total_frames\n",
    "\n",
    "        #     # compara prev_test_frame < cur_test_frame is the same as current_frames % test_interval == 0\n",
    "        #     if (i >= 1 and (prev_test_frame < cur_test_frame)) or final:\n",
    "        #         model.eval()\n",
    "        #         eval_start = time.time()\n",
    "        #         test_rewards = eval_model(model, test_env, num_test_episodes)\n",
    "        #         eval_time = time.time() - eval_start\n",
    "        #         model.train()\n",
    "        #         log_info.update(\n",
    "        #             {\n",
    "        #                 \"eval/reward\": test_rewards,\n",
    "        #                 \"eval/eval_time\": eval_time,\n",
    "        #             }\n",
    "        #         )\n",
    "\n",
    "        # Log all the information\n",
    "\n",
    "        # update weights of the inference policy\n",
    "        # NOTE: Updates the policy weights if the policy of the data \n",
    "        # collector and the trained policy live on different devices.\n",
    "        collector.update_policy_weights_()\n",
    "        sampling_start = time.time()\n",
    "\n",
    "collector.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = DQNLoss(value_network=policy, \n",
    "               action_space=env.action_spec, \n",
    "               delay_value=True) # delay_value=True means we will use a target network\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "\n",
    "# eps: will be used to update the target network as \n",
    "# \\theta_t = \\theta_{t-1} * \\epsilon + \\theta_t * (1-\\epsilon)\n",
    "# where eps = 1 is hard update\n",
    "updater = SoftUpdate(loss, eps=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictParams(params=TensorDict(\n",
       "    fields={\n",
       "        module: TensorDict(\n",
       "            fields={\n",
       "                0: TensorDict(\n",
       "                    fields={\n",
       "                        module: TensorDict(\n",
       "                            fields={\n",
       "                                activation: TensorDict(\n",
       "                                    fields={\n",
       "                                    },\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                encoder: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([3, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                q_net: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False)},\n",
       "                            batch_size=torch.Size([]),\n",
       "                            device=None,\n",
       "                            is_shared=False)},\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False),\n",
       "                1: TensorDict(\n",
       "                    fields={\n",
       "                    },\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=None,\n",
       "            is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.value_network_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictParams(params=TensorDict(\n",
       "    fields={\n",
       "        module: TensorDict(\n",
       "            fields={\n",
       "                0: TensorDict(\n",
       "                    fields={\n",
       "                        module: TensorDict(\n",
       "                            fields={\n",
       "                                activation: TensorDict(\n",
       "                                    fields={\n",
       "                                    },\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                encoder: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([3, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False),\n",
       "                                q_net: TensorDict(\n",
       "                                    fields={\n",
       "                                        0: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([64, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False),\n",
       "                                        1: TensorDict(\n",
       "                                            fields={\n",
       "                                                bias: Parameter(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                                weight: Parameter(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([]),\n",
       "                                            device=None,\n",
       "                                            is_shared=False)},\n",
       "                                    batch_size=torch.Size([]),\n",
       "                                    device=None,\n",
       "                                    is_shared=False)},\n",
       "                            batch_size=torch.Size([]),\n",
       "                            device=None,\n",
       "                            is_shared=False)},\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False),\n",
       "                1: TensorDict(\n",
       "                    fields={\n",
       "                    },\n",
       "                    batch_size=torch.Size([]),\n",
       "                    device=None,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=None,\n",
       "            is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.target_value_network_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode are grouped tensor([3, 3, 4, 4, 2, 2, 1, 1], dtype=torch.int32)\n",
      "steps are successive tensor([0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from torchrl.data import SliceSampler\n",
    "from torchrl.data import LazyMemmapStorage\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(size),\n",
    "    sampler=SliceSampler(traj_key=\"episode\", num_slices=4),\n",
    "    batch_size=8,\n",
    ")\n",
    "episode = torch.zeros(10, dtype=torch.int)\n",
    "episode[:3] = 1\n",
    "episode[3:5] = 2\n",
    "episode[5:7] = 3\n",
    "episode[7:] = 4\n",
    "steps = torch.cat([torch.arange(3), torch.arange(2), torch.arange(2), torch.arange(3)])\n",
    "obs = torch.randn((3, 4, 5)).expand(10, 3, 4, 5)\n",
    "data = TensorDict(\n",
    "    {\n",
    "        \"episode\": episode,\n",
    "        \"obs\": obs,\n",
    "        \"act\": torch.randn((20,)).expand(10, 20),\n",
    "        \"other\": torch.randn((20, 50)).expand(10, 20, 50),\n",
    "        \"steps\": steps,\n",
    "    },\n",
    "    [10],\n",
    ")\n",
    "rb.extend(data)\n",
    "sample = rb.sample()\n",
    "print(\"episode are grouped\", sample[\"episode\"])\n",
    "print(\"steps are successive\", sample[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 3, 3, 4, 4, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictReplayBuffer(\n",
       "    storage=LazyMemmapStorage(\n",
       "        data=TensorDict(\n",
       "            fields={\n",
       "                act: MemoryMappedTensor(shape=torch.Size([10, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                episode: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                index: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                obs: MemoryMappedTensor(shape=torch.Size([10, 3, 4, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                other: MemoryMappedTensor(shape=torch.Size([10, 20, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                steps: MemoryMappedTensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cpu,\n",
       "            is_shared=False), \n",
       "        shape=torch.Size([10]), \n",
       "        len=10, \n",
       "        max_size=100), \n",
       "    sampler=SliceSampler(num_slices=4, slice_len=None, end_key=('next', 'done'), traj_key=episode, truncated_key=('next', 'truncated'), strict_length=True), \n",
       "    writer=TensorDictRoundRobinWriter(cursor=10, full_storage=False), \n",
       "    batch_size=8, \n",
       "    collate_fn=<function _collate_id at 0x7f7435503c40>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_07_23-17_34_50'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "date_str = current_date.strftime(\"%Y_%m_%d-%H_%M_%S\")  # Includes date and time\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/final-project/lib/python3.11/site-packages/torchrl/data/replay_buffers/replay_buffers.py:585: UserWarning: Got conflicting batch_sizes in constructor (32) and `sample` (128). Refer to the ReplayBuffer documentation for a proper usage of the batch-size arguments. The batch-size provided to the sample method will prevail.\n",
      "  warnings.warn(\n",
      "2024-07-23 17:34:53,433 [torchrl][INFO] solved after 0 steps, 0 episodes and in 2.57519268989563s.\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in replay buffer\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:][\"next\", \"step_count\"].max() # From all the next steps get the max step count\n",
    "    if len(rb) > init_rand_steps: # wam-up steps\n",
    "        # Optim loop (we do several optim steps\n",
    "        # per batch collected for efficiency)\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128) # sample a batch of 128 (repetition is allowed)\n",
    "            # print(sample)\n",
    "            break\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update exploration factor\n",
    "            # NOTE: Why I am updating the exploration factor here? \n",
    "            # I'm considering practically that I did 100 (or n) iteractions in the environment time optim_steps\n",
    "            exploration_module.step(data.numel()) # data.numel() returns the number of elements in the data\n",
    "            # Update target params each optimisation step\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum() # sum the number of done episodes\n",
    "    \n",
    "    if max_length > 200:\n",
    "        break\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        _weight: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        action: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([128, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([128]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([128, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        representation: Tensor(shape=torch.Size([128, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([128]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_env.rollout(max_steps=1000, policy=policy)\n",
    "video_recorder.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118398, 676190, 786456, 171936, 887739, 919409, 711872, 442081, 189061, 117840]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate and print 10 random seeds\n",
    "random_seeds = [random.randint(0, 1000000) for _ in range(10)]\n",
    "print(random_seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
