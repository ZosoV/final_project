{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Basic Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math, random\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sett all the libraries with a random seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    gym.utils.seeding.np_random(seed)\n",
    "\n",
    "# Set a seed for all libraries\n",
    "SEED = 42\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Exploration\n",
    "\n",
    "Explore the environment state, action and reward dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space:  Discrete(2)\n",
      "Observation space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Next State:  [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward:  1.0\n",
      "Done:  False\n",
      "Truncation:  False\n",
      "Info:  {}\n",
      "Next State:  [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward:  1.0\n",
      "Done:  False\n",
      "Truncation:  False\n",
      "Info:  {}\n",
      "Next State:  [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward:  1.0\n",
      "Done:  False\n",
      "Truncation:  False\n",
      "Info:  {}\n"
     ]
    }
   ],
   "source": [
    "# Reset environment and get initial state\n",
    "# Initialize environment\n",
    "set_seed(SEED)\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Print state, action, and observation spaces\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "\n",
    "num_iterations = 3\n",
    "\n",
    "\n",
    "state, info = env.reset(seed=SEED)\n",
    "for t in range(num_iterations):\n",
    "\n",
    "    # Take a random action\n",
    "    action = np.random.randint(0,env.action_space.n)\n",
    "\n",
    "    # Take a step in the environment\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Print the next_state, reward, and done\n",
    "    print(\"Next State: \", next_state)\n",
    "    print(\"Reward: \", reward)\n",
    "    print(\"Done: \", done)\n",
    "    print(\"Truncation: \", truncated)\n",
    "    print(\"Info: \", info)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "We define our neural networks of interest:\n",
    "\n",
    "1. DQNNetwork: maps from the state space to Q-values space (one per each action)\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for Deep Q-Learning\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1497,  0.0344],\n",
       "        [-0.0549, -0.1398],\n",
       "        [ 0.0337, -0.0223]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the network with a toy state\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "network = DQNNetwork(state_dim, action_dim)\n",
    "\n",
    "# Create a toy batch of states\n",
    "state = torch.randn(3, state_dim)\n",
    "\n",
    "# Get the Q-values from the network\n",
    "# TODO: The state should be unsqueezed to add a batch dimension\n",
    "q_values = network(state)\n",
    "\n",
    "# Print the Q-values\n",
    "print(q_values.shape)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Define a class for our Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple replay buffer\n",
    "\n",
    "class ExperienceReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    # Add a new experience to the buffer\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    # Function to transform to tensors\n",
    "    def toTensor(self, states, actions, rewards, next_states, dones):\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).view(-1, 1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, requires_grad=False)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        # TODO: Add the conversion to Pytorch tensor here\n",
    "\n",
    "        return self.toTensor(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:  tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]]) torch.Size([2, 4])\n",
      "Actions:  tensor([[0],\n",
      "        [1]]) torch.Size([2, 1])\n",
      "Rewards:  tensor([[1.],\n",
      "        [2.]]) torch.Size([2, 1])\n",
      "Next States:  tensor([[ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]]) torch.Size([2, 4])\n",
      "Dones:  tensor([[0.],\n",
      "        [1.]]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Test the Experience Replay with a toy example\n",
    "# Initialize the buffer\n",
    "buffer = ExperienceReplayBuffer(1000)\n",
    "\n",
    "# Add a new experience\n",
    "state = np.array([1, 2, 3, 4])\n",
    "action = 0\n",
    "reward = 1\n",
    "next_state = np.array([5, 6, 7, 8])\n",
    "done = False\n",
    "buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "# Add another experience\n",
    "state = np.array([5, 6, 7, 8])\n",
    "action = 1\n",
    "reward = 2\n",
    "next_state = np.array([9, 10, 11, 12])\n",
    "done = True\n",
    "buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "# Sample a batch from the buffer\n",
    "batch_size = 2\n",
    "states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "# The experience replay buffer allow us to sample by batches\n",
    "print(\"States: \", states, states.shape)\n",
    "print(\"Actions: \", actions, actions.shape)\n",
    "print(\"Rewards: \", rewards, rewards.shape)\n",
    "print(\"Next States: \", next_states, next_states.shape)\n",
    "print(\"Dones: \", dones, dones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Define our Agent class with a train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Learning agent\n",
    "class DQN_Agent:\n",
    "    def __init__(self, state_dim, action_dim, gamma_td=0.99, lr=0.001):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma_td = gamma_td\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Define the q-network and target q-network\n",
    "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
    "        self.target_q_network = DQNNetwork(state_dim, action_dim)\n",
    "\n",
    "        # Define the optimizer and loss function\n",
    "        self.optimizer = optim.Adam(list(self.q_network.parameters()), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # Load the weights of the Q-Network\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def select_action(self, state, epsilon):\n",
    "        # TODO: Check if the random are a proper way to sample \n",
    "        # or I should use random from pytorch\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1) # NOTE: -1 because randint is inclusive\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0) # Add batch dimension with unsqueeze(0)\n",
    "            q_values = self.q_network(state)\n",
    "            return q_values.argmax().item()\n",
    "        \n",
    "    def train(self, replay_buffer, batch_size):                \n",
    "        # NOTE: At the begining, it's needed some iterations to fill the replay buffer\n",
    "        if len(replay_buffer.buffer) < batch_size:\n",
    "            return 0., 0., 0.\n",
    "        \n",
    "        # Gather a sample from the replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Calculating the TD-Loss\n",
    "        # q_values shape: (batch_size, 1) before gathering\n",
    "        q_values = self.q_network(state).gather(dim = 1, index = action) \n",
    "\n",
    "        # next_q_values shape: (batch_size, 1) before max\n",
    "        next_q_values = self.target_q_network(next_state).max(dim = 1)[0].unsqueeze(1)\n",
    "\n",
    "        # target_q_values shape: (batch_size, 1)\n",
    "        target_q_values = reward + (1 - done) * self.gamma_td * next_q_values\n",
    "        \n",
    "        # Calculate the TD-Loss\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon schedule\n",
    "# epsilon_start = 1.0\n",
    "# epsilon_final = 0.01\n",
    "# epsilon_decay = 500\n",
    "\n",
    "# epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe798048110>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_iterations = 2000\n",
    "num_episodes = 3\n",
    "num_total_steps = num_iterations * num_episodes\n",
    "plt.plot([epsilon_by_frame(i) for i in range(num_total_steps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/rl-project/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/zosov/anaconda3/envs/rl-project/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize environment\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Parameters\n",
    "gamma_td = 0.99\n",
    "lr = 0.001\n",
    "# epsilon = 0.1 # I'm using an schedule now\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "batch_size = 32\n",
    "capacity = 1000\n",
    "num_iterations = 2000  # iterations of the game per episode\n",
    "num_episodes = 3\n",
    "target_update_frequency = num_iterations\n",
    "log_interval = 1000  # Log metrics every 1000 iterations\n",
    "window_size = 100  # For running average\n",
    "\n",
    "# Initialize agent and replay buffer\n",
    "agent = DQN_Agent(state_dim, action_dim, gamma_td, lr)\n",
    "replay_buffer = ExperienceReplayBuffer(capacity)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(f\"models/DQN_CartPole-v1_lr_{lr}_gamma_{gamma_td}\")\n",
    "\n",
    "# Metrics storage\n",
    "episode_rewards = []\n",
    "iteration_losses = []\n",
    "running_avg_rewards = []\n",
    "running_avg_losses = []\n",
    "\n",
    "steps_done = 0\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    iteration = 0\n",
    "\n",
    "    while not done:\n",
    "        epsilon = epsilon_by_frame(episode * num_iterations + iteration)\n",
    "\n",
    "        # Choose what action to take with e-greedy policy\n",
    "        action = agent.select_action(state, epsilon)\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        steps_done += 1\n",
    "\n",
    "        # Add the experience to the replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Train the agent if there are enough experiences\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            loss = agent.train(replay_buffer, batch_size)\n",
    "            iteration_losses.append(loss)\n",
    "            episode_loss += loss\n",
    "\n",
    "            # Track gradient norm\n",
    "            # for param in agent.q_network.parameters():\n",
    "            #     writer.add_histogram('Gradients/norm', param.grad.norm(), steps_done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        iteration += 1\n",
    "\n",
    "        # Update target network periodically\n",
    "        if steps_done % target_update_frequency == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        # Log periodic metrics\n",
    "        if steps_done % log_interval == 0:\n",
    "            avg_loss = np.mean(iteration_losses[-log_interval:])\n",
    "            writer.add_scalar(f'Loss/average_loss_per_{log_interval}_iterations', avg_loss, steps_done)\n",
    "            # writer.add_scalar('Epsilon', epsilon, steps_done)\n",
    "            # writer.add_scalar('Max Q-Value', np.max(agent.q_network(torch.FloatTensor(state).unsqueeze(0))), steps_done)\n",
    "            # writer.add_scalar('Average Q-Value', np.mean(agent.q_network(torch.FloatTensor(state).unsqueeze(0))), steps_done)\n",
    "            # writer.add_scalar('Replay Buffer Utilization', len(replay_buffer) / capacity, steps_done)\n",
    "\n",
    "        if done or iteration >= num_iterations:\n",
    "            break\n",
    "\n",
    "    # Metrics per episode\n",
    "    episode_rewards.append(episode_reward)\n",
    "    avg_episode_loss = episode_loss / episode_length if episode_length > 0 else 0\n",
    "    running_avg_rewards.append(np.mean(episode_rewards[-window_size:]))\n",
    "    running_avg_losses.append(np.mean(iteration_losses[-window_size:]))\n",
    "\n",
    "    # Log metrics\n",
    "    writer.add_scalar('Reward/episode_reward', episode_reward, episode)\n",
    "    writer.add_scalar('Loss/average_loss_per_episode', avg_episode_loss, episode)\n",
    "    # writer.add_scalar('Episode Length', episode_length, episode)\n",
    "    if len(episode_rewards) >= window_size:\n",
    "        writer.add_scalar('Reward/running_average_reward', running_avg_rewards[-1], episode)\n",
    "        writer.add_scalar('Loss/running_average_loss', running_avg_losses[-1], episode)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
